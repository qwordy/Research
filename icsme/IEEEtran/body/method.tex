\section{Methodology}
\label{sec:method}
This section presents our research questions (Section~\ref{sec:method:rq}), our data set (Section~\ref{sec:method:data}), and our support tool (Section~\ref{sec:method:tool}).
\subsection{Research questions}
\label{sec:method:rq}
To understand how concurrency code is maintained, in this study, we focus on the following research questions:

\textbf{RQ1.} What patterns are followed when programmers maintain concurrency code?

In each day, programmers can make numerous commits. Based on their analysis on commits, Kim and Notkin~\cite{conf/icse/KimN09} find that code changes can be repetitive, and Martinez \emph{et al.}~\cite{conf/icsm/MartinezDM13} further extract change patterns to denote such repetitive changes. However, the change patterns of concurrency programming is rarely studied. The recent study~\cite{conf/sigsoft/GuJSZL15} mainly focuses on changes only on critical sections. As a result, this research question is still largely open. To explore this research question, we carefully put our selected concurrent-related commits into \zhong{??} categories (see Section~\ref{sec:result:rq1} for details).


\textbf{RQ2.} How useful are our extracted change patterns, when programmers maintain concurrency code?

To assess the usefulness of our extracted change patterns, we search open source search code in open-source projects with our change patterns. In particular, \zhong{Please explain how you build queries for the search, and how to write your pull requests.} Our changes have been accepted by the owner of some project\footnote{https://github.com/derekmu/Schmince-2}. In Section~\ref{sec:result:sample}, ...



\textbf{RQ3.} What are the change trends of using parallel APIs?

J2SE provides standard APIs\footnote{\url{https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/package-summary.html}} for developing concurrency code. Alternatively, programmers can use third-party libraries such as Apache Commons\footnote{\url{https://commons.apache.org/}} and Guava\footnote{\url{https://github.com/google/guava}}, since they provide similar and extensive functions. In practice, programmers can choose different APIs to implement their concurrency code. Their different choices can lead to different change trends that can be examined through their revision histories. For example, a parallel API can be difficult to use, so programmers have to constantly modify corresponding code, during software maintenance. In Section~\ref{sec:result:trend}, we count commits that involve different parallel APIs over time. We find 3 types of change trends. They are ascending trends (24\%), descending trends (8\%) and hybrid trends (68\%).%\zhong{Please add details.}

\textbf{RQ4.} Are there any correlations between code changes and changes on concurrency code?

Gu \emph{et al.}~\cite{conf/sigsoft/GuJSZL15} compare the code changes and changes on concurrency code, and they find strong correlation between the two types of code changes. With a different data set, we explore whether their finding still holds on other projects. Our results show that \zhong{Please add details.}

\subsection{Data set}
\label{sec:method:data}
In this study, we collected commits from seven open-source projects such as Hadoop\footnote{\url{http://hadoop.apache.org/}}, Tomcat\footnote{\url{http://tomcat.apache.org/}}, Cassandra\footnote{\url{http://cassandra.apache.org/}}, Solr\footnote{\url{http://lucene.apache.org/solr/}}, Netty\footnote{\url{http://netty.io/}}, Flink\footnote{\url{https://flink.apache.org/}} and Mahout\footnote{\url{http://mahout.apache.org/}}. Table~\ref{table:dataset} shows the details of our data set. We selected these projects, since they are popular and active. These projects cover various types of projects such as distributed computing, web server, database, information retrieval, I/O and machine learning. In particular, Hadoop is one of the most popular distributed computing frameworks in Java. Tomcat is a popular server. Cassandra is a database system that manages massive data. Solr is an enterprise search platform. Netty is an asynchronous network application framework. Flink is a stream processing framework. Mahout is a machine learning library. Column ``LOC'' lists the lines of code. Column ``\#Files'' lists number of source files. Column ``\#Commits'' lists number of commits. Column ``\#Selected Commits'' lists our manually investigated commits. Section~\ref{sec:method:steps} explains how we selected these commits. We checked out all the commits in December 2016.

\begin{table}
	\centering
	\caption{Projects Information (LOC and \#Files are both of Java files)}
    \label{table:dataset}
	\begin{tabular}{|c|r|r|r|r|}\hline
		Project&\multicolumn{1}{|c|}{LOC}&\#Files&\#Commits&\#Selected Commits\\\hline
		Hadoop&1,202,764&7,701&14,930&49\\
		Tomcat&301,173&2,192&17,731&159\\
		Cassandra&387,980&2,143&21,982&48\\
		Lucene-solr&918,398&6,310&26,152&74\\
		Netty&218,131&2,054&7,759&202\\
		Flink&414,264&4,068&9,771&28\\\hline
		%Guava&251,205&1,672&3,850\\\hline
		%Mahout&109,584&1,215&3,703&0\\\hline
		Total&3,442,710&24,468&98,352&?\\\hline
	\end{tabular}
%\zhong{Add a total row. Remove Mahout, since you did not analyze its commits.}
\end{table}

\subsection{Study mechanism}
\label{sec:method:tool}
As introduced in Section~\ref{sec:intro}, it is quite difficult to implement a single tool to automate our analysis. Instead, we employ and implement a set of tools to reduce the analysis effort. Inevitably, we have to introduce manual analysis in several steps. Our study mechanism has the following steps:

\subsubsection{Step 1. Collecting commits} All the projects in our study use Git\footnote{\url{https://git-scm.com/}} as their version control system. We implement a tool to check out all their commits. Our tool is based on JGit\footnote{\url{https://eclipse.org/jgit/}}, a lightweight library of manipulating Git repositories. A typical commit log contains a commit id, an author name, the commit date, and a message. Based on the Once we get a commit id, our tool uses the \texttt{git show} command to list details, and then uses the textual \texttt{diff} command to produce its change hunks.

\subsubsection{Step 2. Identifying commits for the follow-up analysis} From collected commits, the second step is to extract commits that are related to concurrency code. Tian \emph{et al.}~\cite{conf/icse/TianLL12} gave a successful example of identifying bug fixing patches using machine learning. We use machine learning to train and predict whether a commit is concurrent-related. We adopt both text analysis and code analysis to extract features. A commit log uses natural language to present what was changed and why the change was made in most cases. We treat each commit log as a bag of words then match the words to a set of concurrent keywords which we have defined as the Java concurrent keywords like \texttt{synchronized}, \texttt{volatile} and names of common classes or interfaces in Java libraries which are related to concurrency. We also do a code analysis based on the \texttt{diff} result. 12 features are extracted for each commit, which is shown in Table II. The first column shows the feature names and the second column shows the explanations.

We use the SVM \cite{journals/ml/CortesV95} algorithm to train and classify commits as concurrent-related or not. SVM is a supervised classification algorithm which needs both positive and negative labeled data for training. In our tool, we use an implementation of SVM, LIBSVM \cite{libsvm}. We manually label some data as a training data set first then train a model. The trained classifier selects 135 positive instances from all the commits which we have collected.

Finally we have 561 potential concurrent-related commits selected by keyword matching of commit message and 135 commits by using machine learning.

\begin{table}
	\centering
	\caption{Features of Data}
	\begin{tabular}{|c|c|}\hline
		Feature&Explanation\\\hline
		msgKey&Number of keywords in commit message\\\hline
		file&Number of files in a commit\\\hline
		hunk&Number of hunks in a commit\\\hline
		lineAdd&Number of added lines in a commit\\\hline
		lineRemove&Number of removed lines in a commit\\\hline
		lineSub&lineAdd - lineRemove\\\hline
		lineSum&lineAdd + lineRemove\\\hline
		keyAdd&Number of added keywords in a commit\\\hline
		keyRemove&Number of removed keywords in a commit\\\hline
		keySub&keyAdd - keyRemove\\\hline
		keySum&keyAdd + keyRemove\\\hline
		contextKey&Number of keywords in context code\\\hline
	\end{tabular}
\end{table}

\subsection{Workflow}
\label{sec:method:steps}
\textbf{RQ1.} We first need to select concurrent-related commits from repositories. A concurrent-related commit here is defined as a commit which performs code changes about concurrent programming. It hard to give a concise definition. But a concurrent-related commit is usually correlated with synchronization, thread, concurrent class, etc. We use our tool to do this job. We use textual analysis and machine learning.

Once we have set prepared set of concurrent-related commits, we manually analyze these potential concurrent-related commits to understand the changes made to code. Here is an example. We list part of lines. The first several lines describe the metadata of this commit. The rest describes differences between two versions of code.

\begin{lstlisting}
commit 563e546236217dace58a8031d56d08a27e08160b
Author: zentol <s.motsu@web.de>
Date:   Mon Jan 26 11:07:53 2015 +0100
[FLINK-1419] [runtime] Fix: distributed cache properly synchronized
This closes #339

public FutureTask<Path> createTmpFile(String name, DistributedCacheEntry entry, JobID jobID) {
-    synchronized (count) {
-      Pair<JobID, String> key = new ImmutablePair<JobID, String>(jobID, name);
-      if (count.containsKey(key)) {
-        count.put(key, count.get(key) + 1);
+    synchronized (lock) {
+      if (!jobCounts.containsKey(jobID)) {
+        jobCounts.put(jobID, new HashMap<String, Integer>());
+      }
+      Map<String, Integer> count = jobCounts.get(jobID);
+      if (count.containsKey(name)) {
+        count.put(name, count.get(name) + 1);
       } else {
-        count.put(key, 1);
+        count.put(name, 1);
       }
     }
\end{lstlisting}

We will show how we examine the changes. We first read the commit message to understand what this commit does and why. If the message explicitly involves concurrency, it will enhance our confidence that this one deserves our attention. Then we scan the differences and try to find concurrency keywords. If we find occurrence of concurrency keywords, we will read the nearby code. The ``+'' symbol marks the line as added line while the ``-'' symbol marks the line as removed line. But differences are usually insufficient for us to understand the changes because some important contexts are not shown in this kind of file. So if we find something confusing, for example, we do not know the type and definition of a variable, we will consult the original and modified versions of full files for help. For example, we cannot know the type and definition of variable \texttt{count} and \texttt{lock}. With help of full files, we know the types of them are \texttt{Map} and \texttt{Object} respectively.

\textbf{RQ2.} We first decide whether a commit is concurrent-related or not through the method described before. For each project's each month, we count the total number of commits and the number concurrent-related commits. We also calculate the percentage of concurrent-related commits. We draw figures to show them visually.

\textbf{RQ3.} We take all commits of the 7 projects into consideration. We count the occurrence times of each concurrent-related class in the added lines or the deleted lines of each month during history. We use tables and figures to show them clearly.

\textbf{RQ4.} It is hard to read tremendous amount of source code of open-source projects line by line to find which piece of concurrent-related code can be applied with the patterns. We search the keywords of pattern in repositories.

%\begin{figure}
%	\centering
%	\includegraphics[width=2in]{workflow}
%	\caption{Workflow}
%\end{figure}

%Figure 1 shows the basic workflow of our study. We first collect all the commits from the 7 projects using our tool. Then we use textual analysis and machine learning to select concurrent-related commits using our tool. Finally we manually analyze the potential concurrent-related commits to understand them.

%The first two steps have been described in Section Tool Support. Now we have got potential concurrent-related commits.
