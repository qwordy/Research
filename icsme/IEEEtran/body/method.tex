\section{Methodology}
\label{sec:method}
This section presents our research questions (Section~\ref{sec:method:rq}), our data set (Section~\ref{sec:method:data}), and our support tool (Section~\ref{sec:method:tool}).
\subsection{Research questions}
\label{sec:method:rq}
To understand how concurrency code is maintained, in this study, we focus on the following research questions:

\textbf{RQ1.} What patterns are followed when programmers maintain concurrency code?

In each day, programmers can make numerous commits. Based on their analysis on commits, Kim and Notkin~\cite{conf/icse/KimN09} find that code changes can be repetitive, and Martinez \emph{et al.}~\cite{conf/icsm/MartinezDM13} further extract change patterns to denote such repetitive changes. However, the change patterns of concurrency programming is rarely studied. The recent study~\cite{conf/sigsoft/GuJSZL15} mainly focuses on changes only on critical sections. As a result, this research question is still largely open. To explore this research question, we carefully put our selected concurrent-related commits into \zhong{??} categories (see Section~\ref{sec:result:rq1} for details).


\textbf{RQ2.} How useful are our extracted change patterns, when programmers maintain concurrency code?

To assess the usefulness of our extracted change patterns, we search open source search code in open-source projects with our change patterns. In particular, \zhong{Please explain how you build queries for the search, and how to write your pull requests.} Our changes have been accepted by the owner of some project\footnote{\url{https://github.com/derekmu/Schmince-2}}. In Section~\ref{sec:result:sample}, ...



\textbf{RQ3.} What are the change trends of using parallel APIs?

J2SE provides standard APIs\footnote{\url{https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/package-summary.html}} for developing concurrency code. Alternatively, programmers can use third-party libraries such as Apache Commons\footnote{\url{https://commons.apache.org/}} and Guava\footnote{\url{https://github.com/google/guava}}, since they provide similar and extensive functions. In practice, programmers can choose different APIs to implement their concurrency code. Their different choices can lead to different change trends that can be examined through their revision histories. For example, a parallel API can be difficult to use, so programmers have to constantly modify corresponding code, during software maintenance. In Section~\ref{sec:result:trend}, we count commits that involve different parallel APIs over time. We find 3 types of change trends. They are ascending trends (24\%), descending trends (8\%) and hybrid trends (68\%).%\zhong{Please add details.}

\textbf{RQ4.} Are there any correlations between code changes and changes on concurrency code?

Gu \emph{et al.}~\cite{conf/sigsoft/GuJSZL15} compare the code changes and changes on concurrency code, and they find strong correlation between the two types of code changes. With a different data set, we explore whether their finding still holds on other projects. Our results show that \zhong{Please add details.}

\subsection{Data set}
\label{sec:method:data}
In this study, we collected commits from seven open-source projects such as Hadoop\footnote{\url{http://hadoop.apache.org/}}, Tomcat\footnote{\url{http://tomcat.apache.org/}}, Cassandra\footnote{\url{http://cassandra.apache.org/}}, Solr\footnote{\url{http://lucene.apache.org/solr/}}, Netty\footnote{\url{http://netty.io/}}, Flink\footnote{\url{https://flink.apache.org/}} and Mahout\footnote{\url{http://mahout.apache.org/}}. Table~\ref{table:dataset} shows the details of our data set. We selected these projects, since they are popular and active. These projects cover various types of projects such as distributed computing, web server, database, information retrieval, I/O and machine learning. In particular, Hadoop is one of the most popular distributed computing frameworks in Java. Tomcat is a popular server. Cassandra is a database system that manages massive data. Solr is an enterprise search platform. Netty is an asynchronous network application framework. Flink is a stream processing framework. Mahout is a machine learning library. Column ``LOC'' lists the lines of code. Column ``\#Files'' lists number of source files. Column ``\#Commits'' lists number of commits. Column ``\#Selected Commits'' lists our manually investigated commits. Section~\ref{sec:method:steps} explains how we selected these commits. We checked out all the commits in December 2016.

\begin{table}
	\centering
	\caption{Projects Information (LOC and \#Files are both of Java files)}
    \label{table:dataset}
	\begin{tabular}{|c|r|r|r|r|}\hline
		Project&\multicolumn{1}{|c|}{LOC}&\#Files&\#Commits&\#Selected Commits\\\hline
		Hadoop&1,202,764&7,701&14,930&49\\
		Tomcat&301,173&2,192&17,731&159\\
		Cassandra&387,980&2,143&21,982&48\\
		Lucene-solr&918,398&6,310&26,152&74\\
		Netty&218,131&2,054&7,759&202\\
		Flink&414,264&4,068&9,771&28\\\hline
		%Guava&251,205&1,672&3,850\\\hline
		%Mahout&109,584&1,215&3,703&0\\\hline
		Total&3,442,710&24,468&98,352&?\\\hline
	\end{tabular}
%\zhong{Add a total row. Remove Mahout, since you did not analyze its commits.}
\end{table}

\subsection{Study mechanism}
\label{sec:method:tool}
As introduced in Section~\ref{sec:intro}, it is quite difficult to implement a single tool to automate our analysis. Instead, we employ and implement a set of tools to reduce the analysis effort. Inevitably, we have to introduce manual analysis in several steps. Our study mechanism has the following steps:

\subsubsection{Step 1. Collecting commits} All the projects in our study use Git\footnote{\url{https://git-scm.com/}} as their version control system. We implement a tool to check out all their commits. Our tool is based on JGit\footnote{\url{https://eclipse.org/jgit/}}, a lightweight library of manipulating Git repositories. A typical commit log contains a commit id, an author name, the commit date, and a message. Based on the Once we get a commit id, our tool uses the \texttt{git show} command to list details, and then uses the textual \texttt{diff} command to produce its change hunks.

\subsubsection{Step 2. Identifying commits for the follow-up analysis} From collected commits, the second step is to extract commits that are related to concurrency code. Here, we consider that a commit is related to concurrency programming, if the commit involves synchronization, thread, or concurrent API classes. In this paper, we call such commits as concurrency commits. A commit has a commit log that is written in natural language. The commit log often explains which files are modified and why programmers make such modifications. Our tool builds queries to search for commits that are related concurrency programming. The built queries contain keywords that are related to concurrency programming (\emph{e.g.}, \texttt{synchronized}, \texttt{volatile}, and related API class names). The search returns 561 commits.

The search alone can lose some useful commits. Researchers have explored related problems. For example, Tian \emph{et al.}~\cite{conf/icse/TianLL12} propose an approach that identifies bug fixing patches with classification techniques. Motivated by their approach, we train a classifier to predict whether a commit is related to concurrency code. When training the classifier, our tool analyzes change hunks that are produced by the \texttt{diff} command, and uses the results as our code features. As shown in Table~\ref{table:feature}, in total, our tool extracts 12 features from each commit. The first column shows the feature names, and the second column shows the explanations.
Our tool employs the SVM \cite{journals/ml/CortesV95} algorithm to identify commits that are related to concurrency programming. In particular, our tool is implemented based on the popular SVM library, LIBSVM \cite{libsvm}. As SVM is a supervised classification algorithm, it needs both labeled positive and negative data for training. We manually label some data as a training data set first then train a model. The trained classifier selects 135 positive instances from all the commits.

\zhong{Do you have overlaps between the two sets of commits? How many commits do you analysis in total?}

Finally we have 561 potential concurrent-related commits selected by keyword matching of commit message and 135 commits by using machine learning.

\begin{table}
	\centering
	\caption{Features of Data}
\label{table:feature}
	\begin{tabular}{|c|c|}\hline
		Feature&Explanation\\\hline
		msgKey&Number of keywords in commit message\\
		file&Number of files in a commit\\
		hunk&Number of hunks in a commit\\
		lineAdd&Number of added lines in a commit\\
		lineRemove&Number of removed lines in a commit\\
		lineSub&lineAdd - lineRemove\\
		lineSum&lineAdd + lineRemove\\
		keyAdd&Number of added keywords in a commit\\
		keyRemove&Number of removed keywords in a commit\\
		keySub&keyAdd - keyRemove\\
		keySum&keyAdd + keyRemove\\
		contextKey&Number of keywords in context code\\\hline
	\end{tabular}
\end{table}

\subsubsection{Step 3. Analyzing commits according to different research questions} We then conduct detailed analysis according to our research questions.



\textbf{RQ1. Determining change patterns.} To explore this research question, we manually analyze concurrency commits. For example, the below is a concurrency commit. The top five lines describe the metadata of the commit. The other lines describe the differences between two versions of code.

\begin{lstlisting}
commit 563e546236217dace58a8031d56d08a27e08160b
Author: zentol <s.motsu@web.de>
Date:   Mon Jan 26 11:07:53 2015 +0100
[FLINK-1419] [runtime] Fix: distributed cache properly synchronized
This closes #339

public FutureTask<Path> createTmpFile(String name, DistributedCacheEntry entry, JobID jobID) {
-    synchronized (count) {
-      Pair<JobID, String> key = new ImmutablePair<JobID, String>(jobID, name);
-      if (count.containsKey(key)) {
-        count.put(key, count.get(key) + 1);
+    synchronized (lock) {
+      if (!jobCounts.containsKey(jobID)) {
+        jobCounts.put(jobID, new HashMap<String, Integer>());
+      }
+      Map<String, Integer> count = jobCounts.get(jobID);
+      if (count.containsKey(name)) {
+        count.put(name, count.get(name) + 1);
       } else {
-        count.put(key, 1);
+        count.put(name, 1);
       }
     }
\end{lstlisting}

For each commit, we first read the metadata and the corresponding issue to understand why programmers make the commit. After that, we scan change hunks to understand the details. In a change hunk, the ``+'' symbol denotes added lines, and the ``-'' symbol denotes removed lines. In some cases, it is infeasible to determine the category of a commit based on only its change hunks. For example, as change hunks are limited, it can be infeasible to determine the type of a variable. In such cases, we check out the original and modified versions of all files to analyze. In this example, we cannot determine the type of the \texttt{count} and \texttt{lock} variables. After we check out all the files, we understand that the types of them are \texttt{Map} and \texttt{Object}, respectively.


We classify concurrency commits into different categories, mainly according to our observed code changes. During classification, we consider various issues such as modified code elements, employed parallel libraries, . \zhong{Please explain what issues do you consider!}

\textbf{RQ2.} We first decide whether a commit is concurrent-related or not through the method described before. For each project's each month, we count the total number of commits and the number concurrent-related commits. We also calculate the percentage of concurrent-related commits. We draw figures to show them visually.

\textbf{RQ3.} We take all commits of the 7 projects into consideration. We count the occurrence times of each concurrent-related class in the added lines or the deleted lines of each month during history. We use tables and figures to show them clearly.

\textbf{RQ4.} It is hard to read tremendous amount of source code of open-source projects line by line to find which piece of concurrent-related code can be applied with the patterns. We search the keywords of pattern in repositories.

%\begin{figure}
%	\centering
%	\includegraphics[width=2in]{workflow}
%	\caption{Workflow}
%\end{figure}

%Figure 1 shows the basic workflow of our study. We first collect all the commits from the 7 projects using our tool. Then we use textual analysis and machine learning to select concurrent-related commits using our tool. Finally we manually analyze the potential concurrent-related commits to understand them.

%The first two steps have been described in Section Tool Support. Now we have got potential concurrent-related commits.
