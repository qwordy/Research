commit 4809f5367b08a9734fc1bd4875be51a9f3bb65aa
Author: Aljoscha Krettek <aljoscha.krettek@gmail.com>
Date:   Wed Aug 10 18:44:50 2016 +0200

    [FLINK-3761] Refactor State Backends/Make Keyed State Key-Group Aware
    
    The biggest change in this is that functionality that used to be in
    AbstractStateBackend is now moved to CheckpointStreamFactory and
    KeyedStateBackend. The former is responsible for providing streams that
    can be used to checkpoint data while the latter is responsible for
    keeping keyed state. A keyed backend can checkpoint the state that it
    keeps by using a CheckpointStreamFactory.
    
    This also refactors how asynchronous keyed state snapshots work. They
    are not implemented using a Future/RunnableFuture.
    
    Also, this changes the keyed state backends to be key-group aware and to
    snapshot the state in key-groups with an index for restoring.

diff --git a/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/AbstractRocksDBState.java b/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/AbstractRocksDBState.java
index 3c4a209..710f506 100644
--- a/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/AbstractRocksDBState.java
+++ b/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/AbstractRocksDBState.java
@@ -23,7 +23,6 @@ import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.core.memory.DataOutputView;
 import org.apache.flink.core.memory.DataOutputViewStreamWrapper;
 import org.apache.flink.runtime.state.KvState;
-import org.apache.flink.runtime.state.KvStateSnapshot;
 import org.apache.flink.util.Preconditions;
 import org.rocksdb.ColumnFamilyHandle;
 import org.rocksdb.RocksDBException;
@@ -46,7 +45,7 @@ import java.io.IOException;
  * @param <SD> The type of {@link StateDescriptor}.
  */
 public abstract class AbstractRocksDBState<K, N, S extends State, SD extends StateDescriptor<S, V>, V>
-		implements KvState<K, N, S, SD, RocksDBStateBackend>, State {
+		implements KvState<N>, State {
 
 	private static final Logger LOG = LoggerFactory.getLogger(AbstractRocksDBState.class);
 
@@ -57,7 +56,7 @@ public abstract class AbstractRocksDBState<K, N, S extends State, SD extends Sta
 	private N currentNamespace;
 
 	/** Backend that holds the actual RocksDB instance where we store state */
-	protected RocksDBStateBackend backend;
+	protected RocksDBKeyedStateBackend backend;
 
 	/** The column family of this particular instance of state */
 	protected ColumnFamilyHandle columnFamily;
@@ -77,7 +76,7 @@ public abstract class AbstractRocksDBState<K, N, S extends State, SD extends Sta
 	protected AbstractRocksDBState(ColumnFamilyHandle columnFamily,
 			TypeSerializer<N> namespaceSerializer,
 			SD stateDesc,
-			RocksDBStateBackend backend) {
+			RocksDBKeyedStateBackend backend) {
 
 		this.namespaceSerializer = namespaceSerializer;
 		this.backend = backend;
@@ -106,7 +105,7 @@ public abstract class AbstractRocksDBState<K, N, S extends State, SD extends Sta
 	}
 
 	protected void writeKeyAndNamespace(DataOutputView out) throws IOException {
-		backend.keySerializer().serialize(backend.currentKey(), out);
+		backend.getKeySerializer().serialize(backend.getCurrentKey(), out);
 		out.writeByte(42);
 		namespaceSerializer.serialize(currentNamespace, out);
 	}
@@ -117,27 +116,6 @@ public abstract class AbstractRocksDBState<K, N, S extends State, SD extends Sta
 	}
 
 	@Override
-	public void dispose() {
-		// ignore because we don't hold any state ourselves
-	}
-
-	@Override
-	public SD getStateDescriptor() {
-		return stateDesc;
-	}
-
-	@Override
-	public void setCurrentKey(K key) {
-		// ignore because we don't hold any state ourselves
-	}
-
-	@Override
-	public KvStateSnapshot<K, N, S, SD, RocksDBStateBackend> snapshot(long checkpointId,
-			long timestamp) throws Exception {
-		throw new RuntimeException("Should not be called. Backups happen in RocksDBStateBackend.");
-	}
-
-	@Override
 	@SuppressWarnings("unchecked")
 	public byte[] getSerializedValue(byte[] serializedKeyAndNamespace) throws Exception {
 		// Serialized key and namespace is expected to be of the same format
diff --git a/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBFoldingState.java b/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBFoldingState.java
index f1cf409..8c0799b 100644
--- a/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBFoldingState.java
+++ b/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBFoldingState.java
@@ -66,7 +66,7 @@ public class RocksDBFoldingState<K, N, T, ACC>
 	public RocksDBFoldingState(ColumnFamilyHandle columnFamily,
 			TypeSerializer<N> namespaceSerializer,
 			FoldingStateDescriptor<T, ACC> stateDesc,
-			RocksDBStateBackend backend) {
+			RocksDBKeyedStateBackend backend) {
 
 		super(columnFamily, namespaceSerializer, stateDesc, backend);
 
diff --git a/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBKeyedStateBackend.java b/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBKeyedStateBackend.java
new file mode 100644
index 0000000..63f1fa2
--- /dev/null
+++ b/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBKeyedStateBackend.java
@@ -0,0 +1,251 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.contrib.streaming.state;
+
+import org.apache.commons.io.FileUtils;
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.api.common.state.FoldingState;
+import org.apache.flink.api.common.state.FoldingStateDescriptor;
+import org.apache.flink.api.common.state.ListState;
+import org.apache.flink.api.common.state.ListStateDescriptor;
+import org.apache.flink.api.common.state.ReducingState;
+import org.apache.flink.api.common.state.ReducingStateDescriptor;
+import org.apache.flink.api.common.state.StateDescriptor;
+import org.apache.flink.api.common.state.ValueState;
+import org.apache.flink.api.common.state.ValueStateDescriptor;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.api.java.tuple.Tuple2;
+import org.apache.flink.runtime.query.TaskKvStateRegistry;
+import org.apache.flink.runtime.state.CheckpointStreamFactory;
+import org.apache.flink.runtime.state.KeyGroupRange;
+import org.apache.flink.runtime.state.KeyGroupRangeOffsets;
+import org.apache.flink.runtime.state.KeyGroupsStateHandle;
+import org.apache.flink.runtime.state.KeyedStateBackend;
+import org.apache.flink.runtime.util.SerializableObject;
+import org.rocksdb.ColumnFamilyDescriptor;
+import org.rocksdb.ColumnFamilyHandle;
+import org.rocksdb.ColumnFamilyOptions;
+import org.rocksdb.DBOptions;
+import org.rocksdb.RocksDB;
+import org.rocksdb.RocksDBException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.Future;
+
+/**
+ * A {@link KeyedStateBackend} that stores its state in {@code RocksDB} and will serialize state to
+ * streams provided by a {@link org.apache.flink.runtime.state.CheckpointStreamFactory} upon
+ * checkpointing. This state backend can store very large state that exceeds memory and spills
+ * to disk.
+ */
+public class RocksDBKeyedStateBackend<K> extends KeyedStateBackend<K> {
+
+	private static final Logger LOG = LoggerFactory.getLogger(RocksDBKeyedStateBackend.class);
+
+	/** Operator identifier that is used to uniqueify the RocksDB storage path. */
+	private final String operatorIdentifier;
+
+	/** JobID for uniquifying backup paths. */
+	private final JobID jobId;
+
+	/** The options from the options factory, cached */
+	private final ColumnFamilyOptions columnOptions;
+
+	/** Path where this configured instance stores its data directory */
+	private final File instanceBasePath;
+
+	/** Path where this configured instance stores its RocksDB data base */
+	private final File instanceRocksDBPath;
+
+	/**
+	 * Our RocksDB data base, this is used by the actual subclasses of {@link AbstractRocksDBState}
+	 * to store state. The different k/v states that we have don't each have their own RocksDB
+	 * instance. They all write to this instance but to their own column family.
+	 */
+	protected volatile RocksDB db;
+
+	/**
+	 * Lock for protecting cleanup of the RocksDB db. We acquire this when doing asynchronous
+	 * checkpoints and when disposing the db. Otherwise, the asynchronous snapshot might try
+	 * iterating over a disposed db.
+	 */
+	private final SerializableObject dbCleanupLock = new SerializableObject();
+
+	/**
+	 * Information about the k/v states as we create them. This is used to retrieve the
+	 * column family that is used for a state and also for sanity checks when restoring.
+	 */
+	private Map<String, Tuple2<ColumnFamilyHandle, StateDescriptor>> kvStateInformation;
+
+	public RocksDBKeyedStateBackend(
+			JobID jobId,
+			String operatorIdentifier,
+			File instanceBasePath,
+			DBOptions dbOptions,
+			ColumnFamilyOptions columnFamilyOptions,
+			TaskKvStateRegistry kvStateRegistry,
+			TypeSerializer<K> keySerializer,
+			KeyGroupAssigner<K> keyGroupAssigner,
+	        KeyGroupRange keyGroupRange
+	) throws Exception {
+
+		super(kvStateRegistry, keySerializer, keyGroupAssigner, keyGroupRange);
+
+		this.operatorIdentifier = operatorIdentifier;
+		this.jobId = jobId;
+		this.columnOptions = columnFamilyOptions;
+
+		this.instanceBasePath = instanceBasePath;
+		this.instanceRocksDBPath = new File(instanceBasePath, "db");
+
+		RocksDB.loadLibrary();
+
+		if (!instanceBasePath.exists()) {
+			if (!instanceBasePath.mkdirs()) {
+				throw new RuntimeException("Could not create RocksDB data directory.");
+			}
+		}
+
+		// clean it, this will remove the last part of the path but RocksDB will recreate it
+		try {
+			if (instanceRocksDBPath.exists()) {
+				LOG.warn("Deleting already existing db directory {}.", instanceRocksDBPath);
+				FileUtils.deleteDirectory(instanceRocksDBPath);
+			}
+		} catch (IOException e) {
+			throw new RuntimeException("Error cleaning RocksDB data directory.", e);
+		}
+
+		List<ColumnFamilyDescriptor> columnFamilyDescriptors = new ArrayList<>(1);
+		// RocksDB seems to need this...
+		columnFamilyDescriptors.add(new ColumnFamilyDescriptor("default".getBytes()));
+		List<ColumnFamilyHandle> columnFamilyHandles = new ArrayList<>(1);
+		try {
+			db = RocksDB.open(dbOptions, instanceRocksDBPath.getAbsolutePath(), columnFamilyDescriptors, columnFamilyHandles);
+		} catch (RocksDBException e) {
+			throw new RuntimeException("Error while opening RocksDB instance.", e);
+		}
+
+		kvStateInformation = new HashMap<>();
+	}
+
+	@Override
+	public void close() throws Exception {
+		super.close();
+
+		// we have to lock because we might have an asynchronous checkpoint going on
+		synchronized (dbCleanupLock) {
+			if (db != null) {
+				for (Tuple2<ColumnFamilyHandle, StateDescriptor> column : kvStateInformation.values()) {
+					column.f0.dispose();
+				}
+
+				db.dispose();
+				db = null;
+			}
+		}
+
+		FileUtils.deleteDirectory(instanceBasePath);
+	}
+
+	@Override
+	public Future<KeyGroupsStateHandle> snapshot(
+			long checkpointId,
+			long timestamp,
+			CheckpointStreamFactory streamFactory) throws Exception {
+		throw new RuntimeException("Not implemented.");
+	}
+
+	// ------------------------------------------------------------------------
+	//  State factories
+	// ------------------------------------------------------------------------
+
+	/**
+	 * Creates a column family handle for use with a k/v state. When restoring from a snapshot
+	 * we don't restore the individual k/v states, just the global RocksDB data base and the
+	 * list of column families. When a k/v state is first requested we check here whether we
+	 * already have a column family for that and return it or create a new one if it doesn't exist.
+	 *
+	 * <p>This also checks whether the {@link StateDescriptor} for a state matches the one
+	 * that we checkpointed, i.e. is already in the map of column families.
+	 */
+	protected ColumnFamilyHandle getColumnFamily(StateDescriptor descriptor) {
+
+		Tuple2<ColumnFamilyHandle, StateDescriptor> stateInfo = kvStateInformation.get(descriptor.getName());
+
+		if (stateInfo != null) {
+			if (!stateInfo.f1.equals(descriptor)) {
+				throw new RuntimeException("Trying to access state using wrong StateDescriptor, was " + stateInfo.f1 + " trying access with " + descriptor);
+			}
+			return stateInfo.f0;
+		}
+
+		ColumnFamilyDescriptor columnDescriptor = new ColumnFamilyDescriptor(descriptor.getName().getBytes(), columnOptions);
+
+		try {
+			ColumnFamilyHandle columnFamily = db.createColumnFamily(columnDescriptor);
+			kvStateInformation.put(descriptor.getName(), new Tuple2<>(columnFamily, descriptor));
+			return columnFamily;
+		} catch (RocksDBException e) {
+			throw new RuntimeException("Error creating ColumnFamilyHandle.", e);
+		}
+	}
+
+	@Override
+	protected <N, T> ValueState<T> createValueState(TypeSerializer<N> namespaceSerializer,
+			ValueStateDescriptor<T> stateDesc) throws Exception {
+
+		ColumnFamilyHandle columnFamily = getColumnFamily(stateDesc);
+
+		return new RocksDBValueState<>(columnFamily, namespaceSerializer,  stateDesc, this);
+	}
+
+	@Override
+	protected <N, T> ListState<T> createListState(TypeSerializer<N> namespaceSerializer,
+			ListStateDescriptor<T> stateDesc) throws Exception {
+
+		ColumnFamilyHandle columnFamily = getColumnFamily(stateDesc);
+
+		return new RocksDBListState<>(columnFamily, namespaceSerializer, stateDesc, this);
+	}
+
+	@Override
+	protected <N, T> ReducingState<T> createReducingState(TypeSerializer<N> namespaceSerializer,
+			ReducingStateDescriptor<T> stateDesc) throws Exception {
+
+		ColumnFamilyHandle columnFamily = getColumnFamily(stateDesc);
+
+		return new RocksDBReducingState<>(columnFamily, namespaceSerializer,  stateDesc, this);
+	}
+
+	@Override
+	protected <N, T, ACC> FoldingState<T, ACC> createFoldingState(TypeSerializer<N> namespaceSerializer,
+			FoldingStateDescriptor<T, ACC> stateDesc) throws Exception {
+
+		ColumnFamilyHandle columnFamily = getColumnFamily(stateDesc);
+
+		return new RocksDBFoldingState<>(columnFamily, namespaceSerializer, stateDesc, this);
+	}
+}
diff --git a/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBListState.java b/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBListState.java
index ff1038e..d8f937b 100644
--- a/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBListState.java
+++ b/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBListState.java
@@ -67,8 +67,8 @@ public class RocksDBListState<K, N, V>
 	public RocksDBListState(ColumnFamilyHandle columnFamily,
 			TypeSerializer<N> namespaceSerializer,
 			ListStateDescriptor<V> stateDesc,
-			RocksDBStateBackend backend) {
-		
+			RocksDBKeyedStateBackend backend) {
+
 		super(columnFamily, namespaceSerializer, stateDesc, backend);
 		this.valueSerializer = stateDesc.getSerializer();
 
diff --git a/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBReducingState.java b/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBReducingState.java
index efa2931..15ae493 100644
--- a/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBReducingState.java
+++ b/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBReducingState.java
@@ -65,8 +65,8 @@ public class RocksDBReducingState<K, N, V>
 	public RocksDBReducingState(ColumnFamilyHandle columnFamily,
 			TypeSerializer<N> namespaceSerializer,
 			ReducingStateDescriptor<V> stateDesc,
-			RocksDBStateBackend backend) {
-		
+			RocksDBKeyedStateBackend backend) {
+
 		super(columnFamily, namespaceSerializer, stateDesc, backend);
 		this.valueSerializer = stateDesc.getSerializer();
 		this.reduceFunction = stateDesc.getReduceFunction();
diff --git a/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBStateBackend.java b/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBStateBackend.java
index 74276c0..62b71d9 100644
--- a/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBStateBackend.java
+++ b/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBStateBackend.java
@@ -17,64 +17,31 @@
 
 package org.apache.flink.contrib.streaming.state;
 
-import org.apache.commons.io.FileUtils;
 import org.apache.flink.api.common.JobID;
-import org.apache.flink.api.common.state.FoldingState;
-import org.apache.flink.api.common.state.FoldingStateDescriptor;
-import org.apache.flink.api.common.state.ListState;
-import org.apache.flink.api.common.state.ListStateDescriptor;
-import org.apache.flink.api.common.state.ReducingState;
-import org.apache.flink.api.common.state.ReducingStateDescriptor;
+import org.apache.flink.api.common.state.KeyGroupAssigner;
 import org.apache.flink.api.common.state.StateBackend;
-import org.apache.flink.api.common.state.StateDescriptor;
-import org.apache.flink.api.common.state.ValueState;
-import org.apache.flink.api.common.state.ValueStateDescriptor;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;
-import org.apache.flink.api.java.tuple.Tuple2;
-import org.apache.flink.api.java.typeutils.runtime.DataInputViewStream;
 import org.apache.flink.core.fs.Path;
-import org.apache.flink.core.memory.DataInputView;
-import org.apache.flink.core.memory.DataInputViewStreamWrapper;
-import org.apache.flink.core.memory.DataOutputView;
-import org.apache.flink.core.memory.DataOutputViewStreamWrapper;
 import org.apache.flink.runtime.execution.Environment;
-import org.apache.flink.runtime.fs.hdfs.HadoopFileSystem;
+import org.apache.flink.runtime.query.TaskKvStateRegistry;
 import org.apache.flink.runtime.state.AbstractStateBackend;
-import org.apache.flink.runtime.state.AsynchronousKvStateSnapshot;
-import org.apache.flink.runtime.state.KvState;
-import org.apache.flink.runtime.state.KvStateSnapshot;
-import org.apache.flink.runtime.state.StreamStateHandle;
+import org.apache.flink.runtime.state.CheckpointStreamFactory;
+import org.apache.flink.runtime.state.KeyGroupRange;
+import org.apache.flink.runtime.state.KeyGroupsStateHandle;
+import org.apache.flink.runtime.state.KeyedStateBackend;
 import org.apache.flink.runtime.state.filesystem.FsStateBackend;
-import org.apache.flink.runtime.util.SerializableObject;
-import org.apache.flink.streaming.util.HDFSCopyFromLocal;
-import org.apache.flink.streaming.util.HDFSCopyToLocal;
-import org.apache.hadoop.fs.FileSystem;
-import org.rocksdb.BackupEngine;
-import org.rocksdb.BackupableDBOptions;
-import org.rocksdb.ColumnFamilyDescriptor;
-import org.rocksdb.ColumnFamilyHandle;
 import org.rocksdb.ColumnFamilyOptions;
 import org.rocksdb.DBOptions;
-import org.rocksdb.Env;
-import org.rocksdb.ReadOptions;
-import org.rocksdb.RestoreOptions;
-import org.rocksdb.RocksDB;
-import org.rocksdb.RocksDBException;
-import org.rocksdb.RocksIterator;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.io.EOFException;
 import java.io.File;
 import java.io.IOException;
 import java.io.ObjectInputStream;
 import java.io.ObjectOutputStream;
 import java.net.URI;
 import java.util.ArrayList;
-import java.util.HashMap;
 import java.util.List;
-import java.util.Map;
 import java.util.Random;
 import java.util.UUID;
 
@@ -83,12 +50,12 @@ import static java.util.Objects.requireNonNull;
 /**
  * A {@link StateBackend} that stores its state in {@code RocksDB}. This state backend can
  * store very large state that exceeds memory and spills to disk.
- * 
+ *
  * <p>All key/value state (including windows) is stored in the key/value index of RocksDB.
  * For persistence against loss of machines, checkpoints take a snapshot of the
  * RocksDB database, and persist that snapshot in a file system (by default) or
  * another configurable state backend.
- * 
+ *
  * <p>The behavior of the RocksDB instances can be parametrized by setting RocksDB Options
  * using the methods {@link #setPredefinedOptions(PredefinedOptions)} and
  * {@link #setOptions(OptionsFactory)}.
@@ -101,15 +68,9 @@ public class RocksDBStateBackend extends AbstractStateBackend {
 	// ------------------------------------------------------------------------
 	//  Static configuration values
 	// ------------------------------------------------------------------------
-	
-	/** The checkpoint directory that we copy the RocksDB backups to. */
-	private final Path checkpointDirectory;
-
-	/** The state backend that stores the non-partitioned state */
-	private final AbstractStateBackend nonPartitionedStateBackend;
 
-	/** Whether we do snapshots fully asynchronous */
-	private boolean fullyAsyncBackup = false;
+	/** The state backend that we use for creating checkpoint streams. */
+	private final AbstractStateBackend checkpointStreamBackend;
 
 	/** Operator identifier that is used to uniqueify the RocksDB storage path. */
 	private String operatorIdentifier;
@@ -118,66 +79,35 @@ public class RocksDBStateBackend extends AbstractStateBackend {
 	private JobID jobId;
 
 	// DB storage directories
-	
+
 	/** Base paths for RocksDB directory, as configured. May be null. */
 	private Path[] configuredDbBasePaths;
 
 	/** Base paths for RocksDB directory, as initialized */
 	private File[] initializedDbBasePaths;
-	
+
 	private int nextDirectory;
-	
+
 	// RocksDB options
-	
+
 	/** The pre-configured option settings */
 	private PredefinedOptions predefinedOptions = PredefinedOptions.DEFAULT;
-	
+
 	/** The options factory to create the RocksDB options in the cluster */
 	private OptionsFactory optionsFactory;
-	
+
 	/** The options from the options factory, cached */
 	private transient DBOptions dbOptions;
 	private transient ColumnFamilyOptions columnOptions;
 
-	// ------------------------------------------------------------------------
-	//  Per operator values that are set in initializerForJob
-	// ------------------------------------------------------------------------
-
-	/** Path where this configured instance stores its data directory */
-	private transient File instanceBasePath;
-
-	/** Path where this configured instance stores its RocksDB data base */
-	private transient File instanceRocksDBPath;
-
-	/** Base path where this configured instance stores checkpoints */
-	private transient String instanceCheckpointPath;
-
-	/**
-	 * Our RocksDB data base, this is used by the actual subclasses of {@link AbstractRocksDBState}
-	 * to store state. The different k/v states that we have don't each have their own RocksDB
-	 * instance. They all write to this instance but to their own column family.
-	 */
-	protected volatile transient RocksDB db;
-
-	/**
-	 * Lock for protecting cleanup of the RocksDB db. We acquire this when doing asynchronous
-	 * checkpoints and when disposing the db. Otherwise, the asynchronous snapshot might try
-	 * iterating over a disposed db.
-	 */
-	private final SerializableObject dbCleanupLock = new SerializableObject();
+	/** Whether we already lazily initialized our local storage directories. */
+	private transient boolean isInitialized = false;
 
-	/**
-	 * Information about the k/v states as we create them. This is used to retrieve the
-	 * column family that is used for a state and also for sanity checks when restoring.
-	 */
-	private Map<String, Tuple2<ColumnFamilyHandle, StateDescriptor>> kvStateInformation;
-
-	// ------------------------------------------------------------------------
 
 	/**
 	 * Creates a new {@code RocksDBStateBackend} that stores its checkpoint data in the
 	 * file system and location defined by the given URI.
-	 * 
+	 *
 	 * <p>A state backend that stores checkpoints in HDFS or S3 must specify the file system
 	 * host and port in the URI, or have the Hadoop configuration that describes the file system
 	 * (host / high-availability group / possibly credentials) either referenced from the Flink
@@ -205,38 +135,42 @@ public class RocksDBStateBackend extends AbstractStateBackend {
 	public RocksDBStateBackend(URI checkpointDataUri) throws IOException {
 		// creating the FsStateBackend automatically sanity checks the URI
 		FsStateBackend fsStateBackend = new FsStateBackend(checkpointDataUri);
-		
-		this.nonPartitionedStateBackend = fsStateBackend;
-		this.checkpointDirectory = fsStateBackend.getBasePath();
+
+		this.checkpointStreamBackend = fsStateBackend;
 	}
 
 
 	public RocksDBStateBackend(String checkpointDataUri, AbstractStateBackend nonPartitionedStateBackend) throws IOException {
 		this(new Path(checkpointDataUri).toUri(), nonPartitionedStateBackend);
 	}
-	
-	public RocksDBStateBackend(URI checkpointDataUri, AbstractStateBackend nonPartitionedStateBackend) throws IOException {
-		this.nonPartitionedStateBackend = requireNonNull(nonPartitionedStateBackend);
-		this.checkpointDirectory = FsStateBackend.validateAndNormalizeUri(checkpointDataUri);
+
+	public RocksDBStateBackend(URI checkpointDataUri, AbstractStateBackend checkpointStreamBackend) throws IOException {
+		this.checkpointStreamBackend = requireNonNull(checkpointStreamBackend);
+	}
+
+	private void writeObject(ObjectOutputStream oos) throws IOException {
+		oos.defaultWriteObject();
 	}
 
+	private void readObject(ObjectInputStream ois) throws IOException, ClassNotFoundException {
+		ois.defaultReadObject();
+		isInitialized = false;
+	}
 	// ------------------------------------------------------------------------
 	//  State backend methods
 	// ------------------------------------------------------------------------
-	
-	@Override
-	public void initializeForJob(
-			Environment env, 
-			String operatorIdentifier,
-			TypeSerializer<?> keySerializer) throws Exception {
-		
-		super.initializeForJob(env, operatorIdentifier, keySerializer);
 
-		this.nonPartitionedStateBackend.initializeForJob(env, operatorIdentifier, keySerializer);
-		
+	private void lazyInitializeForJob(
+			Environment env,
+			String operatorIdentifier) throws Exception {
+
+		if (isInitialized) {
+			return;
+		}
+
 		this.operatorIdentifier = operatorIdentifier.replace(" ", "");
 		this.jobId = env.getJobID();
-		
+
 		// initialize the paths where the local RocksDB files should be stored
 		if (configuredDbBasePaths == null) {
 			// initialize from the temp directories
@@ -245,7 +179,7 @@ public class RocksDBStateBackend extends AbstractStateBackend {
 		else {
 			List<File> dirs = new ArrayList<>(configuredDbBasePaths.length);
 			String errorMessage = "";
-			
+
 			for (Path path : configuredDbBasePaths) {
 				File f = new File(path.toUri().getPath());
 				File testDir = new File(f, UUID.randomUUID().toString());
@@ -259,672 +193,78 @@ public class RocksDBStateBackend extends AbstractStateBackend {
 				}
 				testDir.delete();
 			}
-			
+
 			if (dirs.isEmpty()) {
 				throw new Exception("No local storage directories available. " + errorMessage);
 			} else {
 				initializedDbBasePaths = dirs.toArray(new File[dirs.size()]);
 			}
 		}
-		
-		nextDirectory = new Random().nextInt(initializedDbBasePaths.length);
-
-		instanceBasePath = new File(getDbPath("dummy_state"), UUID.randomUUID().toString());
-		instanceCheckpointPath = getCheckpointPath("dummy_state");
-		instanceRocksDBPath = new File(instanceBasePath, "db");
-
-		RocksDB.loadLibrary();
-
-		if (!instanceBasePath.exists()) {
-			if (!instanceBasePath.mkdirs()) {
-				throw new RuntimeException("Could not create RocksDB data directory.");
-			}
-		}
-
-		// clean it, this will remove the last part of the path but RocksDB will recreate it
-		try {
-			if (instanceRocksDBPath.exists()) {
-				LOG.warn("Deleting already existing db directory {}.", instanceRocksDBPath);
-				FileUtils.deleteDirectory(instanceRocksDBPath);
-			}
-		} catch (IOException e) {
-			throw new RuntimeException("Error cleaning RocksDB data directory.", e);
-		}
-
-		List<ColumnFamilyDescriptor> columnFamilyDescriptors = new ArrayList<>(1);
-		// RocksDB seems to need this...
-		columnFamilyDescriptors.add(new ColumnFamilyDescriptor("default".getBytes()));
-		List<ColumnFamilyHandle> columnFamilyHandles = new ArrayList<>(1);
-		try {
-			db = RocksDB.open(getDbOptions(), instanceRocksDBPath.getAbsolutePath(), columnFamilyDescriptors, columnFamilyHandles);
-		} catch (RocksDBException e) {
-			throw new RuntimeException("Error while opening RocksDB instance.", e);
-		}
-
-		kvStateInformation = new HashMap<>();
-	}
-
-	@Override
-	public void disposeAllStateForCurrentJob() throws Exception {
-		nonPartitionedStateBackend.disposeAllStateForCurrentJob();
-	}
-
-	@Override
-	public void discardState() throws Exception {
-		super.discardState();
-		nonPartitionedStateBackend.discardState();
-
-		// we have to lock because we might have an asynchronous checkpoint going on
-		synchronized (dbCleanupLock) {
-			if (db != null) {
-				if (this.dbOptions != null) {
-					this.dbOptions.dispose();
-					this.dbOptions = null;
-				}
-
-				for (Tuple2<ColumnFamilyHandle, StateDescriptor> column : kvStateInformation.values()) {
-					column.f0.dispose();
-				}
-
-				db.dispose();
-				db = null;
-			}
-		}
-	}
 
-	@Override
-	public void close() throws Exception {
-		nonPartitionedStateBackend.close();
-
-		// we have to lock because we might have an asynchronous checkpoint going on
-		synchronized (dbCleanupLock) {
-			if (db != null) {
-				if (this.dbOptions != null) {
-					this.dbOptions.dispose();
-					this.dbOptions = null;
-				}
-
-				for (Tuple2<ColumnFamilyHandle, StateDescriptor> column : kvStateInformation.values()) {
-					column.f0.dispose();
-				}
-
-				db.dispose();
-				db = null;
-			}
-		}
-	}
+		nextDirectory = new Random().nextInt(initializedDbBasePaths.length);
 
-	private File getDbPath(String stateName) {
-		return new File(new File(new File(getNextStoragePath(), jobId.toString()), operatorIdentifier), stateName);
+		isInitialized = true;
 	}
 
-	private String getCheckpointPath(String stateName) {
-		return checkpointDirectory + "/" + jobId.toString() + "/" + operatorIdentifier + "/" + stateName;
+	private File getDbPath() {
+		return new File(new File(getNextStoragePath(), jobId.toString()), operatorIdentifier);
 	}
 
 	private File getNextStoragePath() {
 		int ni = nextDirectory + 1;
 		ni = ni >= initializedDbBasePaths.length ? 0 : ni;
 		nextDirectory = ni;
-		
-		return initializedDbBasePaths[ni];
-	}
 
-	/**
-	 * Visible for tests.
-	 */
-	public File[] getStoragePaths() {
-		return initializedDbBasePaths;
+		return initializedDbBasePaths[ni];
 	}
 
-	// ------------------------------------------------------------------------
-	//  Snapshot and restore
-	// ------------------------------------------------------------------------
-
 	@Override
-	public HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> snapshotPartitionedState(long checkpointId, long timestamp) throws Exception {
-		if (keyValueStatesByName == null || keyValueStatesByName.size() == 0) {
+	public CheckpointStreamFactory createStreamFactory(JobID jobId,
+			String operatorIdentifier) throws IOException {
 			return null;
 		}
 
 		if (fullyAsyncBackup) {
 			return performFullyAsyncSnapshot(checkpointId, timestamp);
 		} else {
-			return performSemiAsyncSnapshot(checkpointId, timestamp);
-		}
-	}
-
-	/**
-	 * Performs a checkpoint by using the RocksDB backup feature to backup to a directory.
-	 * This backup is the asynchronously copied to the final checkpoint location.
-	 */
-	private HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> performSemiAsyncSnapshot(long checkpointId, long timestamp) throws Exception {
-		// We don't snapshot individual k/v states since everything is stored in a central
-		// RocksDB data base. Create a dummy KvStateSnapshot that holds the information about
-		// that checkpoint. We use the in injectKeyValueStateSnapshots to restore.
-
-		final File localBackupPath = new File(instanceBasePath, "local-chk-" + checkpointId);
-		final URI backupUri = new URI(instanceCheckpointPath + "/chk-" + checkpointId);
-
-		if (!localBackupPath.exists()) {
-			if (!localBackupPath.mkdirs()) {
-				throw new RuntimeException("Could not create local backup path " + localBackupPath);
-			}
-		}
-
-		long startTime = System.currentTimeMillis();
-
-		BackupableDBOptions backupOptions = new BackupableDBOptions(localBackupPath.getAbsolutePath());
-		// we disabled the WAL
-		backupOptions.setBackupLogFiles(false);
-		// no need to sync since we use the backup only as intermediate data before writing to FileSystem snapshot
-		backupOptions.setSync(false);
-
-		try (BackupEngine backupEngine = BackupEngine.open(Env.getDefault(), backupOptions)) {
-			// wait before flush with "true"
-			backupEngine.createNewBackup(db, true);
-		}
-
-		long endTime = System.currentTimeMillis();
-		LOG.info("RocksDB (" + instanceRocksDBPath + ") backup (synchronous part) took " + (endTime - startTime) + " ms.");
-
-		// draw a copy in case it get's changed while performing the async snapshot
-		List<StateDescriptor> kvStateInformationCopy = new ArrayList<>();
-		for (Tuple2<ColumnFamilyHandle, StateDescriptor> state: kvStateInformation.values()) {
-			kvStateInformationCopy.add(state.f1);
-		}
-		SemiAsyncSnapshot dummySnapshot = new SemiAsyncSnapshot(localBackupPath,
-				backupUri,
-				kvStateInformationCopy,
-				checkpointId);
-
-
-		HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> result = new HashMap<>();
-		result.put("dummy_state", dummySnapshot);
-		return result;
-	}
-
-	/**
-	 * Performs a checkpoint by drawing a {@link org.rocksdb.Snapshot} from RocksDB and then
-	 * iterating over all key/value pairs in RocksDB to store them in the final checkpoint
-	 * location. The only synchronous part is the drawing of the {@code Snapshot} which
-	 * is essentially free.
-	 */
-	private HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> performFullyAsyncSnapshot(long checkpointId, long timestamp) throws Exception {
-		// we draw a snapshot from RocksDB then iterate over all keys at that point
-		// and store them in the backup location
-
-		final URI backupUri = new URI(instanceCheckpointPath + "/chk-" + checkpointId);
-
-		long startTime = System.currentTimeMillis();
-
-		org.rocksdb.Snapshot snapshot = db.getSnapshot();
-
-		long endTime = System.currentTimeMillis();
-		LOG.info("Fully asynchronous RocksDB (" + instanceRocksDBPath + ") backup (synchronous part) took " + (endTime - startTime) + " ms.");
-
-		// draw a copy in case it get's changed while performing the async snapshot
-		Map<String, Tuple2<ColumnFamilyHandle, StateDescriptor>> columnFamiliesCopy = new HashMap<>();
-		columnFamiliesCopy.putAll(kvStateInformation);
-		FullyAsyncSnapshot dummySnapshot = new FullyAsyncSnapshot(snapshot,
-				this,
-				backupUri,
-				columnFamiliesCopy,
-				checkpointId);
-
-
-		HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> result = new HashMap<>();
-		result.put("dummy_state", dummySnapshot);
-		return result;
+		return checkpointStreamBackend.createStreamFactory(jobId, operatorIdentifier);
 	}
 
 	@Override
-	public final void injectKeyValueStateSnapshots(HashMap<String, KvStateSnapshot> keyValueStateSnapshots) throws Exception {
-		if (keyValueStateSnapshots == null) {
-			return;
-		}
-
-		KvStateSnapshot dummyState = keyValueStateSnapshots.get("dummy_state");
-		if (dummyState instanceof FinalSemiAsyncSnapshot) {
-			restoreFromSemiAsyncSnapshot((FinalSemiAsyncSnapshot) dummyState);
-		} else if (dummyState instanceof FinalFullyAsyncSnapshot) {
-			restoreFromFullyAsyncSnapshot((FinalFullyAsyncSnapshot) dummyState);
-		} else {
-			throw new RuntimeException("Unknown RocksDB snapshot: " + dummyState);
-		}
-	}
-
-	private void restoreFromSemiAsyncSnapshot(FinalSemiAsyncSnapshot snapshot) throws Exception {
-		// This does mostly the same work as initializeForJob, we remove the existing RocksDB
-		// directory and create a new one from the backup.
-		// This must be refactored. The StateBackend should either be initialized from
-		// scratch or from a snapshot.
-
-		if (!instanceBasePath.exists()) {
-			if (!instanceBasePath.mkdirs()) {
-				throw new RuntimeException("Could not create RocksDB data directory.");
-			}
-		}
-
-		db.dispose();
-
-		// clean it, this will remove the last part of the path but RocksDB will recreate it
-		try {
-			if (instanceRocksDBPath.exists()) {
-				LOG.warn("Deleting already existing db directory {}.", instanceRocksDBPath);
-				FileUtils.deleteDirectory(instanceRocksDBPath);
-			}
-		} catch (IOException e) {
-			throw new RuntimeException("Error cleaning RocksDB data directory.", e);
-		}
-
-		final File localBackupPath = new File(instanceBasePath, "chk-" + snapshot.checkpointId);
-
-		if (localBackupPath.exists()) {
-			try {
-				LOG.warn("Deleting already existing local backup directory {}.", localBackupPath);
-				FileUtils.deleteDirectory(localBackupPath);
-			} catch (IOException e) {
-				throw new RuntimeException("Error cleaning RocksDB local backup directory.", e);
-			}
-		}
-
-		HDFSCopyToLocal.copyToLocal(snapshot.backupUri, instanceBasePath);
-
-		try (BackupEngine backupEngine = BackupEngine.open(Env.getDefault(), new BackupableDBOptions(localBackupPath.getAbsolutePath()))) {
-			backupEngine.restoreDbFromLatestBackup(instanceRocksDBPath.getAbsolutePath(), instanceRocksDBPath.getAbsolutePath(), new RestoreOptions(true));
-		} catch (RocksDBException|IllegalArgumentException e) {
-			throw new RuntimeException("Error while restoring RocksDB state from " + localBackupPath, e);
-		} finally {
-			try {
-				FileUtils.deleteDirectory(localBackupPath);
-			} catch (IOException e) {
-				LOG.error("Error cleaning up local restore directory " + localBackupPath, e);
-			}
-		}
-
-
-		List<ColumnFamilyDescriptor> columnFamilyDescriptors = new ArrayList<>(snapshot.stateDescriptors.size());
-		for (StateDescriptor stateDescriptor: snapshot.stateDescriptors) {
-			columnFamilyDescriptors.add(new ColumnFamilyDescriptor(stateDescriptor.getName().getBytes(), getColumnOptions()));
-		}
-
-		// RocksDB seems to need this...
-		columnFamilyDescriptors.add(new ColumnFamilyDescriptor("default".getBytes()));
-		List<ColumnFamilyHandle> columnFamilyHandles = new ArrayList<>(snapshot.stateDescriptors.size());
-		try {
-
-			db = RocksDB.open(getDbOptions(), instanceRocksDBPath.getAbsolutePath(), columnFamilyDescriptors, columnFamilyHandles);
-			this.kvStateInformation = new HashMap<>();
-			for (int i = 0; i < snapshot.stateDescriptors.size(); i++) {
-				this.kvStateInformation.put(snapshot.stateDescriptors.get(i).getName(), new Tuple2<>(columnFamilyHandles.get(i), snapshot.stateDescriptors.get(i)));
-			}
-
-		} catch (RocksDBException e) {
-			throw new RuntimeException("Error while opening RocksDB instance.", e);
-		}
-	}
-
-	private void restoreFromFullyAsyncSnapshot(FinalFullyAsyncSnapshot snapshot) throws Exception {
-
-		DataInputView inputView = new DataInputViewStreamWrapper(snapshot.stateHandle.openInputStream());
-
-		// clear k/v state information before filling it
-		kvStateInformation.clear();
-
-		// first get the column family mapping
-		int numColumns = inputView.readInt();
-		Map<Byte, StateDescriptor> columnFamilyMapping = new HashMap<>(numColumns);
-		for (int i = 0; i < numColumns; i++) {
-			byte mappingByte = inputView.readByte();
-
-			ObjectInputStream ooIn = new ObjectInputStream(new DataInputViewStream(inputView));
-			StateDescriptor stateDescriptor = (StateDescriptor) ooIn.readObject();
-
-			columnFamilyMapping.put(mappingByte, stateDescriptor);
-
-			// this will fill in the k/v state information
-			getColumnFamily(stateDescriptor);
-		}
-
-		// try and read until EOF
-		try {
-			// the EOFException will get us out of this...
-			while (true) {
-				byte mappingByte = inputView.readByte();
-				ColumnFamilyHandle handle = getColumnFamily(columnFamilyMapping.get(mappingByte));
-				byte[] key = BytePrimitiveArraySerializer.INSTANCE.deserialize(inputView);
-				byte[] value = BytePrimitiveArraySerializer.INSTANCE.deserialize(inputView);
-				db.put(handle, key, value);
-			}
-		} catch (EOFException e) {
-			// expected
-		}
-	}
-
-	// ------------------------------------------------------------------------
-	//  Semi-asynchronous Backup Classes
-	// ------------------------------------------------------------------------
-
-	/**
-	 * Upon snapshotting the RocksDB backup is created synchronously. The asynchronous part is
-	 * copying the backup to a (possibly) remote filesystem. This is done in {@link #materialize()}.
-	 */
-	private static class SemiAsyncSnapshot extends AsynchronousKvStateSnapshot<Object, Object, ValueState<Object>, ValueStateDescriptor<Object>, RocksDBStateBackend> {
-		private static final long serialVersionUID = 1L;
-		private final File localBackupPath;
-		private final URI backupUri;
-		private final List<StateDescriptor> stateDescriptors;
-		private final long checkpointId;
-
-		private SemiAsyncSnapshot(File localBackupPath,
-				URI backupUri,
-				List<StateDescriptor> columnFamilies,
-				long checkpointId) {
-			this.localBackupPath = localBackupPath;
-			this.backupUri = backupUri;
-			this.stateDescriptors = columnFamilies;
-			this.checkpointId = checkpointId;
-		}
-
-		@Override
-		public KvStateSnapshot<Object, Object, ValueState<Object>, ValueStateDescriptor<Object>, RocksDBStateBackend> materialize() throws Exception {
-			try {
-				long startTime = System.currentTimeMillis();
-				HDFSCopyFromLocal.copyFromLocal(localBackupPath, backupUri);
-				long endTime = System.currentTimeMillis();
-				LOG.info("RocksDB materialization from " + localBackupPath + " to " + backupUri + " (asynchronous part) took " + (endTime - startTime) + " ms.");
-				return new FinalSemiAsyncSnapshot(backupUri, checkpointId, stateDescriptors);
-			} catch (Exception e) {
-				FileSystem fs = FileSystem.get(backupUri, HadoopFileSystem.getHadoopConfiguration());
-				fs.delete(new org.apache.hadoop.fs.Path(backupUri), true);
-				throw e;
-			} finally {
-				FileUtils.deleteQuietly(localBackupPath);
-			}
-		}
-	}
-
-	/**
-	 * Dummy {@link KvStateSnapshot} that holds the state of our one RocksDB data base. This
-	 * also stores the column families that we had at the time of the snapshot so that we can
-	 * restore these. This results from {@link SemiAsyncSnapshot}.
-	 */
-	private static class FinalSemiAsyncSnapshot implements KvStateSnapshot<Object, Object, ValueState<Object>, ValueStateDescriptor<Object>, RocksDBStateBackend> {
-		private static final long serialVersionUID = 1L;
-
-		final URI backupUri;
-		final long checkpointId;
-		private final List<StateDescriptor> stateDescriptors;
-
-		/**
-		 * Creates a new snapshot from the given state parameters.
-		 */
-		private FinalSemiAsyncSnapshot(URI backupUri, long checkpointId, List<StateDescriptor> stateDescriptors) {
-			this.backupUri = backupUri;
-			this.checkpointId = checkpointId;
-			this.stateDescriptors = stateDescriptors;
-		}
-
-		@Override
-		public final KvState<Object, Object, ValueState<Object>, ValueStateDescriptor<Object>, RocksDBStateBackend> restoreState(
-				RocksDBStateBackend stateBackend,
-				TypeSerializer<Object> keySerializer,
-				ClassLoader classLoader) throws Exception {
-			throw new RuntimeException("Should never happen.");
-		}
-
-		@Override
-		public final void discardState() throws Exception {
-			FileSystem fs = FileSystem.get(backupUri, HadoopFileSystem.getHadoopConfiguration());
-			fs.delete(new org.apache.hadoop.fs.Path(backupUri), true);
-		}
-
-		@Override
-		public final long getStateSize() throws Exception {
-			FileSystem fs = FileSystem.get(backupUri, HadoopFileSystem.getHadoopConfiguration());
-			return fs.getContentSummary(new org.apache.hadoop.fs.Path(backupUri)).getLength();
-		}
-
-		@Override
-		public void close() throws IOException {
-			// cannot do much here
-		}
-	}
-
-	// ------------------------------------------------------------------------
-	//  Fully asynchronous Backup Classes
-	// ------------------------------------------------------------------------
-
-	/**
-	 * This does the snapshot using a RocksDB snapshot and an iterator over all keys
-	 * at the point of that snapshot.
-	 */
-	private class FullyAsyncSnapshot extends AsynchronousKvStateSnapshot<Object, Object, ValueState<Object>, ValueStateDescriptor<Object>, RocksDBStateBackend> {
-		private static final long serialVersionUID = 1L;
-
-		private transient org.rocksdb.Snapshot snapshot;
-		private transient AbstractStateBackend backend;
-
-		private final URI backupUri;
-		private final Map<String, Tuple2<ColumnFamilyHandle, StateDescriptor>> columnFamilies;
-		private final long checkpointId;
-
-		private FullyAsyncSnapshot(org.rocksdb.Snapshot snapshot,
-				AbstractStateBackend backend,
-				URI backupUri,
-				Map<String, Tuple2<ColumnFamilyHandle, StateDescriptor>> columnFamilies,
-				long checkpointId) {
-			this.snapshot = snapshot;
-			this.backend = backend;
-			this.backupUri = backupUri;
-			this.columnFamilies = columnFamilies;
-			this.checkpointId = checkpointId;
-		}
-
-		@Override
-		public KvStateSnapshot<Object, Object, ValueState<Object>, ValueStateDescriptor<Object>, RocksDBStateBackend> materialize() throws Exception {
-			try {
-				long startTime = System.currentTimeMillis();
-
-				CheckpointStateOutputStream outputStream = backend.createCheckpointStateOutputStream(checkpointId, startTime);
-				DataOutputView outputView = new DataOutputViewStreamWrapper(outputStream);
-				outputView.writeInt(columnFamilies.size());
-
-				// we don't know how many key/value pairs there are in each column family.
-				// We prefix every written element with a byte that signifies to which
-				// column family it belongs, this way we can restore the column families
-				byte count = 0;
-				Map<String, Byte> columnFamilyMapping = new HashMap<>();
-				for (Map.Entry<String, Tuple2<ColumnFamilyHandle, StateDescriptor>> column: columnFamilies.entrySet()) {
-					columnFamilyMapping.put(column.getKey(), count);
-
-					outputView.writeByte(count);
-
-					ObjectOutputStream ooOut = new ObjectOutputStream(outputStream);
-					ooOut.writeObject(column.getValue().f1);
-					ooOut.flush();
-
-					count++;
-				}
-
-				ReadOptions readOptions = new ReadOptions();
-				readOptions.setSnapshot(snapshot);
-
-				for (Map.Entry<String, Tuple2<ColumnFamilyHandle, StateDescriptor>> column: columnFamilies.entrySet()) {
-					byte columnByte = columnFamilyMapping.get(column.getKey());
-
-					synchronized (dbCleanupLock) {
-						if (db == null) {
-							throw new RuntimeException("RocksDB instance was disposed. This happens " +
-									"when we are in the middle of a checkpoint and the job fails.");
-						}
-						RocksIterator iterator = db.newIterator(column.getValue().f0, readOptions);
-						iterator.seekToFirst();
-						while (iterator.isValid()) {
-							outputView.writeByte(columnByte);
-							BytePrimitiveArraySerializer.INSTANCE.serialize(iterator.key(),
-									outputView);
-							BytePrimitiveArraySerializer.INSTANCE.serialize(iterator.value(),
-									outputView);
-							iterator.next();
-						}
-					}
-				}
-
-				StreamStateHandle stateHandle = outputStream.closeAndGetHandle();
-
-				long endTime = System.currentTimeMillis();
-				LOG.info("Fully asynchronous RocksDB materialization to " + backupUri + " (asynchronous part) took " + (endTime - startTime) + " ms.");
-				return new FinalFullyAsyncSnapshot(stateHandle, checkpointId);
-			} finally {
-				synchronized (dbCleanupLock) {
-					if (db != null) {
-						db.releaseSnapshot(snapshot);
-					}
-				}
-				snapshot = null;
-			}
-		}
-
-	}
-
-	/**
-	 * Dummy {@link KvStateSnapshot} that holds the state of our one RocksDB data base. This
-	 * results from {@link FullyAsyncSnapshot}.
-	 */
-	private static class FinalFullyAsyncSnapshot implements KvStateSnapshot<Object, Object, ValueState<Object>, ValueStateDescriptor<Object>, RocksDBStateBackend> {
-		private static final long serialVersionUID = 1L;
-
-		final StreamStateHandle stateHandle;
-		final long checkpointId;
-
-		/**
-		 * Creates a new snapshot from the given state parameters.
-		 */
-		private FinalFullyAsyncSnapshot(StreamStateHandle stateHandle, long checkpointId) {
-			this.stateHandle = stateHandle;
-			this.checkpointId = checkpointId;
-		}
-
-		@Override
-		public final KvState<Object, Object, ValueState<Object>, ValueStateDescriptor<Object>, RocksDBStateBackend> restoreState(
-				RocksDBStateBackend stateBackend,
-				TypeSerializer<Object> keySerializer,
-				ClassLoader classLoader) throws Exception {
-			throw new RuntimeException("Should never happen.");
-		}
-
-		@Override
-		public final void discardState() throws Exception {
-			stateHandle.discardState();
-		}
-
-		@Override
-		public final long getStateSize() throws Exception {
-			return stateHandle.getStateSize();
-		}
-
-		@Override
-		public void close() throws IOException {
-			stateHandle.close();
-		}
-	}
-
-	// ------------------------------------------------------------------------
-	//  State factories
-	// ------------------------------------------------------------------------
-
-	/**
-	 * Creates a column family handle for use with a k/v state. When restoring from a snapshot
-	 * we don't restore the individual k/v states, just the global RocksDB data base and the
-	 * list of column families. When a k/v state is first requested we check here whether we
-	 * already have a column family for that and return it or create a new one if it doesn't exist.
-	 *
-	 * <p>This also checks whether the {@link StateDescriptor} for a state matches the one
-	 * that we checkpointed, i.e. is already in the map of column families.
-	 */
-	protected ColumnFamilyHandle getColumnFamily(StateDescriptor descriptor)  {
-
-		Tuple2<ColumnFamilyHandle, StateDescriptor> stateInfo = kvStateInformation.get(descriptor.getName());
-
-		if (stateInfo != null) {
-			if (!stateInfo.f1.equals(descriptor)) {
-				throw new RuntimeException("Trying to access state using wrong StateDescriptor, was " + stateInfo.f1 + " trying access with " + descriptor);
-			}
-			return stateInfo.f0;
-		}
-
-		ColumnFamilyDescriptor columnDescriptor = new ColumnFamilyDescriptor(descriptor.getName().getBytes(), getColumnOptions());
-
-		try {
-			ColumnFamilyHandle columnFamily = db.createColumnFamily(columnDescriptor);
-			kvStateInformation.put(descriptor.getName(), new Tuple2<>(columnFamily, descriptor));
-			return columnFamily;
-		} catch (RocksDBException e) {
-			throw new RuntimeException("Error creating ColumnFamilyHandle.", e);
-		}
-	}
-
-	/**
-	 * Used by k/v states to access the current key.
-	 */
-	public Object currentKey() {
-		return currentKey;
-	}
-
-	/**
-	 * Used by k/v states to access the key serializer.
-	 */
-	public TypeSerializer keySerializer() {
-		return keySerializer;
-	}
-
-	@Override
-	protected <N, T> ValueState<T> createValueState(TypeSerializer<N> namespaceSerializer,
-			ValueStateDescriptor<T> stateDesc) throws Exception {
-
-		ColumnFamilyHandle columnFamily = getColumnFamily(stateDesc);
-
-		return new RocksDBValueState<>(columnFamily, namespaceSerializer,  stateDesc, this);
-	}
-
-	@Override
-	protected <N, T> ListState<T> createListState(TypeSerializer<N> namespaceSerializer,
-			ListStateDescriptor<T> stateDesc) throws Exception {
-
-		ColumnFamilyHandle columnFamily = getColumnFamily(stateDesc);
-
-		return new RocksDBListState<>(columnFamily, namespaceSerializer, stateDesc, this);
-	}
-
-	@Override
-	protected <N, T> ReducingState<T> createReducingState(TypeSerializer<N> namespaceSerializer,
-			ReducingStateDescriptor<T> stateDesc) throws Exception {
-
-		ColumnFamilyHandle columnFamily = getColumnFamily(stateDesc);
-
-		return new RocksDBReducingState<>(columnFamily, namespaceSerializer,  stateDesc, this);
-	}
+	public <K> KeyedStateBackend<K> createKeyedStateBackend(
+			Environment env,
+			JobID jobID,
+			String operatorIdentifier,
+			TypeSerializer<K> keySerializer,
+			KeyGroupAssigner<K> keyGroupAssigner,
+			KeyGroupRange keyGroupRange,
+			TaskKvStateRegistry kvStateRegistry) throws Exception {
 
-	@Override
-	protected <N, T, ACC> FoldingState<T, ACC> createFoldingState(TypeSerializer<N> namespaceSerializer,
-			FoldingStateDescriptor<T, ACC> stateDesc) throws Exception {
+		lazyInitializeForJob(env, operatorIdentifier);
 
-		ColumnFamilyHandle columnFamily = getColumnFamily(stateDesc);
+		File instanceBasePath = new File(getDbPath(), UUID.randomUUID().toString());
 
-		return new RocksDBFoldingState<>(columnFamily, namespaceSerializer, stateDesc, this);
+		return new RocksDBKeyedStateBackend<>(
+				jobID,
+				operatorIdentifier,
+				instanceBasePath,
+				getDbOptions(),
+				getColumnOptions(),
+				kvStateRegistry,
+				keySerializer,
+				keyGroupAssigner,
+				keyGroupRange);
 	}
 
-	// ------------------------------------------------------------------------
-	//  Non-partitioned state
-	// ------------------------------------------------------------------------
-
 	@Override
-	public CheckpointStateOutputStream createCheckpointStateOutputStream(
-			long checkpointID, long timestamp) throws Exception {
-		
-		return nonPartitionedStateBackend.createCheckpointStateOutputStream(checkpointID, timestamp);
+	public <K> KeyedStateBackend<K> restoreKeyedStateBackend(Environment env, JobID jobID,
+			String operatorIdentifier,
+			TypeSerializer<K> keySerializer,
+			KeyGroupAssigner<K> keyGroupAssigner,
+            KeyGroupRange keyGroupRange,
+			List<KeyGroupsStateHandle> restoredState,
+			TaskKvStateRegistry kvStateRegistry) throws Exception {
+		throw new RuntimeException("Not implemented.");
 	}
 
 	// ------------------------------------------------------------------------
@@ -932,35 +272,13 @@ public class RocksDBStateBackend extends AbstractStateBackend {
 	// ------------------------------------------------------------------------
 
 	/**
-	 * Enables fully asynchronous snapshotting of the partitioned state held in RocksDB.
-	 *
-	 * <p>By default, this is disabled. This means that RocksDB state is copied in a synchronous
-	 * step, during which normal processing of elements pauses, followed by an asynchronous step
-	 * of copying the RocksDB backup to the final checkpoint location. Fully asynchronous
-	 * snapshots take longer (linear time requirement with respect to number of unique keys)
-	 * but normal processing of elements is not paused.
-	 */
-	public void enableFullyAsyncSnapshots() {
-		this.fullyAsyncBackup = true;
-	}
-
-	/**
-	 * Disables fully asynchronous snapshotting of the partitioned state held in RocksDB.
-	 *
-	 * <p>By default, this is disabled.
-	 */
-	public void disableFullyAsyncSnapshots() {
-		this.fullyAsyncBackup = false;
-	}
-
-	/**
 	 * Sets the path where the RocksDB local database files should be stored on the local
 	 * file system. Setting this path overrides the default behavior, where the
 	 * files are stored across the configured temp directories.
-	 * 
+	 *
 	 * <p>Passing {@code null} to this function restores the default behavior, where the configured
 	 * temp directories will be used.
-	 * 
+	 *
 	 * @param path The path where the local RocksDB database files are stored.
 	 */
 	public void setDbStoragePath(String path) {
@@ -971,44 +289,44 @@ public class RocksDBStateBackend extends AbstractStateBackend {
 	 * Sets the paths across which the local RocksDB database files are distributed on the local
 	 * file system. Setting these paths overrides the default behavior, where the
 	 * files are stored across the configured temp directories.
-	 * 
+	 *
 	 * <p>Each distinct state will be stored in one path, but when the state backend creates
 	 * multiple states, they will store their files on different paths.
-	 * 
+	 *
 	 * <p>Passing {@code null} to this function restores the default behavior, where the configured
 	 * temp directories will be used.
-	 * 
-	 * @param paths The paths across which the local RocksDB database files will be spread. 
+	 *
+	 * @param paths The paths across which the local RocksDB database files will be spread.
 	 */
 	public void setDbStoragePaths(String... paths) {
 		if (paths == null) {
 			configuredDbBasePaths = null;
-		} 
+		}
 		else if (paths.length == 0) {
 			throw new IllegalArgumentException("empty paths");
 		}
 		else {
 			Path[] pp = new Path[paths.length];
-			
+
 			for (int i = 0; i < paths.length; i++) {
 				if (paths[i] == null) {
 					throw new IllegalArgumentException("null path");
 				}
-				
+
 				pp[i] = new Path(paths[i]);
 				String scheme = pp[i].toUri().getScheme();
 				if (scheme != null && !scheme.equalsIgnoreCase("file")) {
 					throw new IllegalArgumentException("Path " + paths[i] + " has a non local scheme");
 				}
 			}
-			
+
 			configuredDbBasePaths = pp;
 		}
 	}
 
 	/**
-	 * 
-	 * @return The configured DB storage paths, or null, if none were configured. 
+	 *
+	 * @return The configured DB storage paths, or null, if none were configured.
 	 */
 	public String[] getDbStoragePaths() {
 		if (configuredDbBasePaths == null) {
@@ -1021,18 +339,18 @@ public class RocksDBStateBackend extends AbstractStateBackend {
 			return paths;
 		}
 	}
-	
+
 	// ------------------------------------------------------------------------
 	//  Parametrize with RocksDB Options
 	// ------------------------------------------------------------------------
 
 	/**
 	 * Sets the predefined options for RocksDB.
-	 * 
+	 *
 	 * <p>If a user-defined options factory is set (via {@link #setOptions(OptionsFactory)}),
 	 * then the options from the factory are applied on top of the here specified
 	 * predefined options.
-	 * 
+	 *
 	 * @param options The options to set (must not be null).
 	 */
 	public void setPredefinedOptions(PredefinedOptions options) {
@@ -1043,10 +361,10 @@ public class RocksDBStateBackend extends AbstractStateBackend {
 	 * Gets the currently set predefined options for RocksDB.
 	 * The default options (if nothing was set via {@link #setPredefinedOptions(PredefinedOptions)})
 	 * are {@link PredefinedOptions#DEFAULT}.
-	 * 
+	 *
 	 * <p>If a user-defined  options factory is set (via {@link #setOptions(OptionsFactory)}),
 	 * then the options from the factory are applied on top of the predefined options.
-	 * 
+	 *
 	 * @return The currently set predefined options for RocksDB.
 	 */
 	public PredefinedOptions getPredefinedOptions() {
@@ -1057,13 +375,13 @@ public class RocksDBStateBackend extends AbstractStateBackend {
 	 * Sets {@link org.rocksdb.Options} for the RocksDB instances.
 	 * Because the options are not serializable and hold native code references,
 	 * they must be specified through a factory.
-	 * 
-	 * <p>The options created by the factory here are applied on top of the pre-defined 
+	 *
+	 * <p>The options created by the factory here are applied on top of the pre-defined
 	 * options profile selected via {@link #setPredefinedOptions(PredefinedOptions)}.
 	 * If the pre-defined options profile is the default
 	 * ({@link PredefinedOptions#DEFAULT}), then the factory fully controls the RocksDB
 	 * options.
-	 * 
+	 *
 	 * @param optionsFactory The options factory that lazily creates the RocksDB options.
 	 */
 	public void setOptions(OptionsFactory optionsFactory) {
@@ -1072,7 +390,7 @@ public class RocksDBStateBackend extends AbstractStateBackend {
 
 	/**
 	 * Gets the options factory that lazily creates the RocksDB options.
-	 * 
+	 *
 	 * @return The options factory.
 	 */
 	public OptionsFactory getOptions() {
@@ -1091,7 +409,7 @@ public class RocksDBStateBackend extends AbstractStateBackend {
 			if (optionsFactory != null) {
 				opt = optionsFactory.createDBOptions(opt);
 			}
-			
+
 			// add necessary default options
 			opt = opt.setCreateIfMissing(true);
 
diff --git a/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBValueState.java b/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBValueState.java
index 62bc366..b9c0e83 100644
--- a/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBValueState.java
+++ b/flink-contrib/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBValueState.java
@@ -63,7 +63,7 @@ public class RocksDBValueState<K, N, V>
 	public RocksDBValueState(ColumnFamilyHandle columnFamily,
 			TypeSerializer<N> namespaceSerializer,
 			ValueStateDescriptor<V> stateDesc,
-			RocksDBStateBackend backend) {
+			RocksDBKeyedStateBackend backend) {
 
 		super(columnFamily, namespaceSerializer, stateDesc, backend);
 		this.valueSerializer = stateDesc.getSerializer();
diff --git a/flink-contrib/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/FullyAsyncRocksDBStateBackendTest.java b/flink-contrib/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/FullyAsyncRocksDBStateBackendTest.java
deleted file mode 100644
index 7861542..0000000
--- a/flink-contrib/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/FullyAsyncRocksDBStateBackendTest.java
+++ /dev/null
@@ -1,65 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.contrib.streaming.state;
-
-import org.apache.commons.io.FileUtils;
-import org.apache.flink.configuration.ConfigConstants;
-import org.apache.flink.runtime.state.StateBackendTestBase;
-import org.apache.flink.runtime.state.memory.MemoryStateBackend;
-import org.apache.flink.util.OperatingSystem;
-import org.junit.Assume;
-import org.junit.Before;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.UUID;
-
-/**
- * Tests for the partitioned state part of {@link RocksDBStateBackend} with fully asynchronous
- * checkpointing enabled.
- */
-public class FullyAsyncRocksDBStateBackendTest extends StateBackendTestBase<RocksDBStateBackend> {
-
-	private File dbDir;
-	private File chkDir;
-
-	@Before
-	public void checkOperatingSystem() {
-		Assume.assumeTrue("This test can't run successfully on Windows.", !OperatingSystem.isWindows());
-	}
-
-	@Override
-	protected RocksDBStateBackend getStateBackend() throws IOException {
-		dbDir = new File(new File(ConfigConstants.DEFAULT_TASK_MANAGER_TMP_PATH, UUID.randomUUID().toString()), "state");
-		chkDir = new File(new File(ConfigConstants.DEFAULT_TASK_MANAGER_TMP_PATH, UUID.randomUUID().toString()), "snapshots");
-
-		RocksDBStateBackend backend = new RocksDBStateBackend(chkDir.getAbsoluteFile().toURI(), new MemoryStateBackend());
-		backend.setDbStoragePath(dbDir.getAbsolutePath());
-		backend.enableFullyAsyncSnapshots();
-		return backend;
-	}
-
-	@Override
-	protected void cleanup() {
-		try {
-			FileUtils.deleteDirectory(dbDir);
-			FileUtils.deleteDirectory(chkDir);
-		} catch (IOException ignore) {}
-	}
-}
diff --git a/flink-contrib/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBAsyncKVSnapshotTest.java b/flink-contrib/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBAsyncKVSnapshotTest.java
index d720c6d..0e35b60 100644
--- a/flink-contrib/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBAsyncKVSnapshotTest.java
+++ b/flink-contrib/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBAsyncKVSnapshotTest.java
@@ -88,7 +88,6 @@ public class RocksDBAsyncKVSnapshotTest {
 	 * test will simply lock forever.
 	 */
 	@Test
-	@Ignore
 	public void testAsyncCheckpoints() throws Exception {
 		LocalFileSystem localFS = new LocalFileSystem();
 		localFS.initialize(new URI("file:///"), new Configuration());
@@ -191,7 +190,6 @@ public class RocksDBAsyncKVSnapshotTest {
 	 * test will simply lock forever.
 	 */
 	@Test
-	@Ignore
 	public void testFullyAsyncCheckpoints() throws Exception {
 		LocalFileSystem localFS = new LocalFileSystem();
 		localFS.initialize(new URI("file:///"), new Configuration());
@@ -218,7 +216,7 @@ public class RocksDBAsyncKVSnapshotTest {
 
 		RocksDBStateBackend backend = new RocksDBStateBackend(chkDir.getAbsoluteFile().toURI(), new MemoryStateBackend());
 		backend.setDbStoragePath(dbDir.getAbsolutePath());
-		backend.enableFullyAsyncSnapshots();
+//		backend.enableFullyAsyncSnapshots();
 
 		streamConfig.setStateBackend(backend);
 
diff --git a/flink-contrib/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBStateBackendConfigTest.java b/flink-contrib/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBStateBackendConfigTest.java
index 657c57e..6f4a983 100644
--- a/flink-contrib/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBStateBackendConfigTest.java
+++ b/flink-contrib/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBStateBackendConfigTest.java
@@ -1,322 +1,322 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.contrib.streaming.state;
-
-import org.apache.commons.io.FileUtils;
-import org.apache.flink.api.common.JobID;
-import org.apache.flink.api.common.TaskInfo;
-import org.apache.flink.api.common.state.ValueStateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.api.common.typeutils.base.IntSerializer;
-import org.apache.flink.runtime.execution.Environment;
-import org.apache.flink.runtime.io.disk.iomanager.IOManager;
-import org.apache.flink.runtime.state.AbstractStateBackend;
-
-import org.apache.flink.runtime.state.VoidNamespace;
-import org.apache.flink.runtime.state.VoidNamespaceSerializer;
-import org.apache.flink.util.OperatingSystem;
-import org.junit.Assume;
-import org.junit.Before;
-import org.junit.Test;
-
-import org.rocksdb.ColumnFamilyOptions;
-import org.rocksdb.CompactionStyle;
-import org.rocksdb.DBOptions;
-
-import java.io.File;
-import java.util.UUID;
-
-import static org.junit.Assert.*;
-import static org.mockito.Mockito.*;
-
-/**
- * Tests for configuring the RocksDB State Backend 
- */
-@SuppressWarnings("serial")
-public class RocksDBStateBackendConfigTest {
-	
-	private static final String TEMP_URI = new File(System.getProperty("java.io.tmpdir")).toURI().toString();
-
-	@Before
-	public void checkOperatingSystem() {
-		Assume.assumeTrue("This test can't run successfully on Windows.", !OperatingSystem.isWindows());
-	}
-
-	// ------------------------------------------------------------------------
-	//  RocksDB local file directory
-	// ------------------------------------------------------------------------
-	
-	@Test
-	public void testSetDbPath() throws Exception {
-		RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI);
-		
-		assertNull(rocksDbBackend.getDbStoragePaths());
-		
-		rocksDbBackend.setDbStoragePath("/abc/def");
-		assertArrayEquals(new String[] { "/abc/def" }, rocksDbBackend.getDbStoragePaths());
-
-		rocksDbBackend.setDbStoragePath(null);
-		assertNull(rocksDbBackend.getDbStoragePaths());
-
-		rocksDbBackend.setDbStoragePaths("/abc/def", "/uvw/xyz");
-		assertArrayEquals(new String[] { "/abc/def", "/uvw/xyz" }, rocksDbBackend.getDbStoragePaths());
-
-		//noinspection NullArgumentToVariableArgMethod
-		rocksDbBackend.setDbStoragePaths(null);
-		assertNull(rocksDbBackend.getDbStoragePaths());
-	}
-
-	@Test(expected = IllegalArgumentException.class)
-	public void testSetNullPaths() throws Exception {
-		RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI);
-		rocksDbBackend.setDbStoragePaths();
-	}
-
-	@Test(expected = IllegalArgumentException.class)
-	public void testNonFileSchemePath() throws Exception {
-		RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI);
-		rocksDbBackend.setDbStoragePath("hdfs:///some/path/to/perdition");
-	}
-
-	// ------------------------------------------------------------------------
-	//  RocksDB local file automatic from temp directories
-	// ------------------------------------------------------------------------
-	
-	@Test
-	public void testUseTempDirectories() throws Exception {
-		File dir1 = new File(System.getProperty("java.io.tmpdir"), UUID.randomUUID().toString());
-		File dir2 = new File(System.getProperty("java.io.tmpdir"), UUID.randomUUID().toString());
-
-		File[] tempDirs = new File[] { dir1, dir2 };
-		
-		try {
-			assertTrue(dir1.mkdirs());
-			assertTrue(dir2.mkdirs());
-
-			RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI);
-			assertNull(rocksDbBackend.getDbStoragePaths());
-			
-			rocksDbBackend.initializeForJob(getMockEnvironment(tempDirs), "foobar", IntSerializer.INSTANCE);
-			assertArrayEquals(tempDirs, rocksDbBackend.getStoragePaths());
-		}
-		finally {
-			FileUtils.deleteDirectory(dir1);
-			FileUtils.deleteDirectory(dir2);
-		}
-	}
-	
-	// ------------------------------------------------------------------------
-	//  RocksDB local file directory initialization
-	// ------------------------------------------------------------------------
-
-	@Test
-	public void testFailWhenNoLocalStorageDir() throws Exception {
-		File targetDir = new File(System.getProperty("java.io.tmpdir"), UUID.randomUUID().toString());
-		try {
-			assertTrue(targetDir.mkdirs());
-			
-			if (!targetDir.setWritable(false, false)) {
-				System.err.println("Cannot execute 'testFailWhenNoLocalStorageDir' because cannot mark directory non-writable");
-				return;
-			}
-			
-			RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI);
-			rocksDbBackend.setDbStoragePath(targetDir.getAbsolutePath());
-
-			boolean hasFailure = false;
-			try {
-				rocksDbBackend.initializeForJob(getMockEnvironment(), "foobar", IntSerializer.INSTANCE);
-			}
-			catch (Exception e) {
-				assertTrue(e.getMessage().contains("No local storage directories available"));
-				assertTrue(e.getMessage().contains(targetDir.getAbsolutePath()));
-				hasFailure = true;
-			}
-			assertTrue("We must see a failure because no storaged directory is feasible.", hasFailure);
-		}
-		finally {
-			//noinspection ResultOfMethodCallIgnored
-			targetDir.setWritable(true, false);
-			FileUtils.deleteDirectory(targetDir);
-		}
-	}
-
-	@Test
-	public void testContinueOnSomeDbDirectoriesMissing() throws Exception {
-		File targetDir1 = new File(System.getProperty("java.io.tmpdir"), UUID.randomUUID().toString());
-		File targetDir2 = new File(System.getProperty("java.io.tmpdir"), UUID.randomUUID().toString());
-		
-		try {
-			assertTrue(targetDir1.mkdirs());
-			assertTrue(targetDir2.mkdirs());
-
-			if (!targetDir1.setWritable(false, false)) {
-				System.err.println("Cannot execute 'testContinueOnSomeDbDirectoriesMissing' because cannot mark directory non-writable");
-				return;
-			}
-	
-			RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI);
-			rocksDbBackend.setDbStoragePaths(targetDir1.getAbsolutePath(), targetDir2.getAbsolutePath());
-	
-			try {
-				rocksDbBackend.initializeForJob(getMockEnvironment(), "foobar", IntSerializer.INSTANCE);
-
-				// actually get a state to see whether we can write to the storage directory
-				rocksDbBackend.getPartitionedState(
-						VoidNamespace.INSTANCE,
-						VoidNamespaceSerializer.INSTANCE,
-						new ValueStateDescriptor<>("test", String.class, ""));
-			}
-			catch (Exception e) {
-				e.printStackTrace();
-				fail("Backend initialization failed even though some paths were available");
-			}
-		} finally {
-			//noinspection ResultOfMethodCallIgnored
-			targetDir1.setWritable(true, false);
-			FileUtils.deleteDirectory(targetDir1);
-			FileUtils.deleteDirectory(targetDir2);
-		}
-	}
-	
-	// ------------------------------------------------------------------------
-	//  RocksDB Options
-	// ------------------------------------------------------------------------
-	
-	@Test
-	public void testPredefinedOptions() throws Exception {
-		RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI);
-		
-		assertEquals(PredefinedOptions.DEFAULT, rocksDbBackend.getPredefinedOptions());
-		
-		rocksDbBackend.setPredefinedOptions(PredefinedOptions.SPINNING_DISK_OPTIMIZED);
-		assertEquals(PredefinedOptions.SPINNING_DISK_OPTIMIZED, rocksDbBackend.getPredefinedOptions());
-
-		DBOptions opt1 = rocksDbBackend.getDbOptions();
-		DBOptions opt2 = rocksDbBackend.getDbOptions();
-		
-		assertEquals(opt1, opt2);
-
-		ColumnFamilyOptions columnOpt1 = rocksDbBackend.getColumnOptions();
-		ColumnFamilyOptions columnOpt2 = rocksDbBackend.getColumnOptions();
-
-		assertEquals(columnOpt1, columnOpt2);
-
-		assertEquals(CompactionStyle.LEVEL, columnOpt1.compactionStyle());
-	}
-
-	@Test
-	public void testOptionsFactory() throws Exception {
-		RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI);
-		
-		rocksDbBackend.setOptions(new OptionsFactory() {
-			@Override
-			public DBOptions createDBOptions(DBOptions currentOptions) {
-				return currentOptions;
-			}
-
-			@Override
-			public ColumnFamilyOptions createColumnOptions(ColumnFamilyOptions currentOptions) {
-				return currentOptions.setCompactionStyle(CompactionStyle.FIFO);
-			}
-		});
-		
-		assertNotNull(rocksDbBackend.getOptions());
-		assertEquals(CompactionStyle.FIFO, rocksDbBackend.getColumnOptions().compactionStyle());
-	}
-
-	@Test
-	public void testPredefinedAndOptionsFactory() throws Exception {
-		RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI);
-
-		assertEquals(PredefinedOptions.DEFAULT, rocksDbBackend.getPredefinedOptions());
-
-		rocksDbBackend.setPredefinedOptions(PredefinedOptions.SPINNING_DISK_OPTIMIZED);
-		rocksDbBackend.setOptions(new OptionsFactory() {
-			@Override
-			public DBOptions createDBOptions(DBOptions currentOptions) {
-				return currentOptions;
-			}
-
-			@Override
-			public ColumnFamilyOptions createColumnOptions(ColumnFamilyOptions currentOptions) {
-				return currentOptions.setCompactionStyle(CompactionStyle.UNIVERSAL);
-			}
-		});
-		
-		assertEquals(PredefinedOptions.SPINNING_DISK_OPTIMIZED, rocksDbBackend.getPredefinedOptions());
-		assertNotNull(rocksDbBackend.getOptions());
-		assertEquals(CompactionStyle.UNIVERSAL, rocksDbBackend.getColumnOptions().compactionStyle());
-	}
-
-	@Test
-	public void testPredefinedOptionsEnum() {
-		for (PredefinedOptions o : PredefinedOptions.values()) {
-			DBOptions opt = o.createDBOptions();
-			try {
-				assertNotNull(opt);
-			} finally {
-				opt.dispose();
-			}
-		}
-	}
-
-	// ------------------------------------------------------------------------
-	//  Contained Non-partitioned State Backend
-	// ------------------------------------------------------------------------
-	
-	@Test
-	public void testCallsForwardedToNonPartitionedBackend() throws Exception {
-		AbstractStateBackend nonPartBackend = mock(AbstractStateBackend.class);
-		RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI, nonPartBackend);
-
-		rocksDbBackend.initializeForJob(getMockEnvironment(), "foo", IntSerializer.INSTANCE);
-		verify(nonPartBackend, times(1)).initializeForJob(any(Environment.class), anyString(), any(TypeSerializer.class));
-
-		rocksDbBackend.disposeAllStateForCurrentJob();
-		verify(nonPartBackend, times(1)).disposeAllStateForCurrentJob();
-		
-		rocksDbBackend.close();
-		verify(nonPartBackend, times(1)).close();
-	}
-	
-	// ------------------------------------------------------------------------
-	//  Utilities
-	// ------------------------------------------------------------------------
-
-	private static Environment getMockEnvironment() {
-		return getMockEnvironment(new File[] { new File(System.getProperty("java.io.tmpdir")) });
-	}
-	
-	private static Environment getMockEnvironment(File[] tempDirs) {
-		IOManager ioMan = mock(IOManager.class);
-		when(ioMan.getSpillingDirectories()).thenReturn(tempDirs);
-		
-		Environment env = mock(Environment.class);
-		when(env.getJobID()).thenReturn(new JobID());
-		when(env.getUserClassLoader()).thenReturn(RocksDBStateBackendConfigTest.class.getClassLoader());
-		when(env.getIOManager()).thenReturn(ioMan);
-
-		TaskInfo taskInfo = mock(TaskInfo.class);
-		when(env.getTaskInfo()).thenReturn(taskInfo);
-
-		when(taskInfo.getIndexOfThisSubtask()).thenReturn(0);
-		return env;
-	}
-}
+///*
+// * Licensed to the Apache Software Foundation (ASF) under one
+// * or more contributor license agreements.  See the NOTICE file
+// * distributed with this work for additional information
+// * regarding copyright ownership.  The ASF licenses this file
+// * to you under the Apache License, Version 2.0 (the
+// * "License"); you may not use this file except in compliance
+// * with the License.  You may obtain a copy of the License at
+// *
+// *    http://www.apache.org/licenses/LICENSE-2.0
+// *
+// * Unless required by applicable law or agreed to in writing, software
+// * distributed under the License is distributed on an "AS IS" BASIS,
+// * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// * See the License for the specific language governing permissions and
+// * limitations under the License.
+// */
+//
+//package org.apache.flink.contrib.streaming.state;
+//
+//import org.apache.commons.io.FileUtils;
+//import org.apache.flink.api.common.JobID;
+//import org.apache.flink.api.common.TaskInfo;
+//import org.apache.flink.api.common.state.ValueStateDescriptor;
+//import org.apache.flink.api.common.typeutils.TypeSerializer;
+//import org.apache.flink.api.common.typeutils.base.IntSerializer;
+//import org.apache.flink.runtime.execution.Environment;
+//import org.apache.flink.runtime.io.disk.iomanager.IOManager;
+//import org.apache.flink.runtime.state.AbstractStateBackend;
+//
+//import org.apache.flink.runtime.state.VoidNamespace;
+//import org.apache.flink.runtime.state.VoidNamespaceSerializer;
+//import org.apache.flink.util.OperatingSystem;
+//import org.junit.Assume;
+//import org.junit.Before;
+//import org.junit.Test;
+//
+//import org.rocksdb.ColumnFamilyOptions;
+//import org.rocksdb.CompactionStyle;
+//import org.rocksdb.DBOptions;
+//
+//import java.io.File;
+//import java.util.UUID;
+//
+//import static org.junit.Assert.*;
+//import static org.mockito.Mockito.*;
+//
+///**
+// * Tests for configuring the RocksDB State Backend
+// */
+//@SuppressWarnings("serial")
+//public class RocksDBStateBackendConfigTest {
+//
+//	private static final String TEMP_URI = new File(System.getProperty("java.io.tmpdir")).toURI().toString();
+//
+//	@Before
+//	public void checkOperatingSystem() {
+//		Assume.assumeTrue("This test can't run successfully on Windows.", !OperatingSystem.isWindows());
+//	}
+//
+//	// ------------------------------------------------------------------------
+//	//  RocksDB local file directory
+//	// ------------------------------------------------------------------------
+//
+//	@Test
+//	public void testSetDbPath() throws Exception {
+//		RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI);
+//
+//		assertNull(rocksDbBackend.getDbStoragePaths());
+//
+//		rocksDbBackend.setDbStoragePath("/abc/def");
+//		assertArrayEquals(new String[] { "/abc/def" }, rocksDbBackend.getDbStoragePaths());
+//
+//		rocksDbBackend.setDbStoragePath(null);
+//		assertNull(rocksDbBackend.getDbStoragePaths());
+//
+//		rocksDbBackend.setDbStoragePaths("/abc/def", "/uvw/xyz");
+//		assertArrayEquals(new String[] { "/abc/def", "/uvw/xyz" }, rocksDbBackend.getDbStoragePaths());
+//
+//		//noinspection NullArgumentToVariableArgMethod
+//		rocksDbBackend.setDbStoragePaths(null);
+//		assertNull(rocksDbBackend.getDbStoragePaths());
+//	}
+//
+//	@Test(expected = IllegalArgumentException.class)
+//	public void testSetNullPaths() throws Exception {
+//		RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI);
+//		rocksDbBackend.setDbStoragePaths();
+//	}
+//
+//	@Test(expected = IllegalArgumentException.class)
+//	public void testNonFileSchemePath() throws Exception {
+//		RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI);
+//		rocksDbBackend.setDbStoragePath("hdfs:///some/path/to/perdition");
+//	}
+//
+//	// ------------------------------------------------------------------------
+//	//  RocksDB local file automatic from temp directories
+//	// ------------------------------------------------------------------------
+//
+//	@Test
+//	public void testUseTempDirectories() throws Exception {
+//		File dir1 = new File(System.getProperty("java.io.tmpdir"), UUID.randomUUID().toString());
+//		File dir2 = new File(System.getProperty("java.io.tmpdir"), UUID.randomUUID().toString());
+//
+//		File[] tempDirs = new File[] { dir1, dir2 };
+//
+//		try {
+//			assertTrue(dir1.mkdirs());
+//			assertTrue(dir2.mkdirs());
+//
+//			RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI);
+//			assertNull(rocksDbBackend.getDbStoragePaths());
+//
+//			rocksDbBackend.initializeForJob(getMockEnvironment(tempDirs), "foobar", IntSerializer.INSTANCE);
+//			assertArrayEquals(tempDirs, rocksDbBackend.getStoragePaths());
+//		}
+//		finally {
+//			FileUtils.deleteDirectory(dir1);
+//			FileUtils.deleteDirectory(dir2);
+//		}
+//	}
+//
+//	// ------------------------------------------------------------------------
+//	//  RocksDB local file directory initialization
+//	// ------------------------------------------------------------------------
+//
+//	@Test
+//	public void testFailWhenNoLocalStorageDir() throws Exception {
+//		File targetDir = new File(System.getProperty("java.io.tmpdir"), UUID.randomUUID().toString());
+//		try {
+//			assertTrue(targetDir.mkdirs());
+//
+//			if (!targetDir.setWritable(false, false)) {
+//				System.err.println("Cannot execute 'testFailWhenNoLocalStorageDir' because cannot mark directory non-writable");
+//				return;
+//			}
+//
+//			RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI);
+//			rocksDbBackend.setDbStoragePath(targetDir.getAbsolutePath());
+//
+//			boolean hasFailure = false;
+//			try {
+//				rocksDbBackend.initializeForJob(getMockEnvironment(), "foobar", IntSerializer.INSTANCE);
+//			}
+//			catch (Exception e) {
+//				assertTrue(e.getMessage().contains("No local storage directories available"));
+//				assertTrue(e.getMessage().contains(targetDir.getAbsolutePath()));
+//				hasFailure = true;
+//			}
+//			assertTrue("We must see a failure because no storaged directory is feasible.", hasFailure);
+//		}
+//		finally {
+//			//noinspection ResultOfMethodCallIgnored
+//			targetDir.setWritable(true, false);
+//			FileUtils.deleteDirectory(targetDir);
+//		}
+//	}
+//
+//	@Test
+//	public void testContinueOnSomeDbDirectoriesMissing() throws Exception {
+//		File targetDir1 = new File(System.getProperty("java.io.tmpdir"), UUID.randomUUID().toString());
+//		File targetDir2 = new File(System.getProperty("java.io.tmpdir"), UUID.randomUUID().toString());
+//
+//		try {
+//			assertTrue(targetDir1.mkdirs());
+//			assertTrue(targetDir2.mkdirs());
+//
+//			if (!targetDir1.setWritable(false, false)) {
+//				System.err.println("Cannot execute 'testContinueOnSomeDbDirectoriesMissing' because cannot mark directory non-writable");
+//				return;
+//			}
+//
+//			RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI);
+//			rocksDbBackend.setDbStoragePaths(targetDir1.getAbsolutePath(), targetDir2.getAbsolutePath());
+//
+//			try {
+//				rocksDbBackend.initializeForJob(getMockEnvironment(), "foobar", IntSerializer.INSTANCE);
+//
+//				// actually get a state to see whether we can write to the storage directory
+//				rocksDbBackend.getPartitionedState(
+//						VoidNamespace.INSTANCE,
+//						VoidNamespaceSerializer.INSTANCE,
+//						new ValueStateDescriptor<>("test", String.class, ""));
+//			}
+//			catch (Exception e) {
+//				e.printStackTrace();
+//				fail("Backend initialization failed even though some paths were available");
+//			}
+//		} finally {
+//			//noinspection ResultOfMethodCallIgnored
+//			targetDir1.setWritable(true, false);
+//			FileUtils.deleteDirectory(targetDir1);
+//			FileUtils.deleteDirectory(targetDir2);
+//		}
+//	}
+//
+//	// ------------------------------------------------------------------------
+//	//  RocksDB Options
+//	// ------------------------------------------------------------------------
+//
+//	@Test
+//	public void testPredefinedOptions() throws Exception {
+//		RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI);
+//
+//		assertEquals(PredefinedOptions.DEFAULT, rocksDbBackend.getPredefinedOptions());
+//
+//		rocksDbBackend.setPredefinedOptions(PredefinedOptions.SPINNING_DISK_OPTIMIZED);
+//		assertEquals(PredefinedOptions.SPINNING_DISK_OPTIMIZED, rocksDbBackend.getPredefinedOptions());
+//
+//		DBOptions opt1 = rocksDbBackend.getDbOptions();
+//		DBOptions opt2 = rocksDbBackend.getDbOptions();
+//
+//		assertEquals(opt1, opt2);
+//
+//		ColumnFamilyOptions columnOpt1 = rocksDbBackend.getColumnOptions();
+//		ColumnFamilyOptions columnOpt2 = rocksDbBackend.getColumnOptions();
+//
+//		assertEquals(columnOpt1, columnOpt2);
+//
+//		assertEquals(CompactionStyle.LEVEL, columnOpt1.compactionStyle());
+//	}
+//
+//	@Test
+//	public void testOptionsFactory() throws Exception {
+//		RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI);
+//
+//		rocksDbBackend.setOptions(new OptionsFactory() {
+//			@Override
+//			public DBOptions createDBOptions(DBOptions currentOptions) {
+//				return currentOptions;
+//			}
+//
+//			@Override
+//			public ColumnFamilyOptions createColumnOptions(ColumnFamilyOptions currentOptions) {
+//				return currentOptions.setCompactionStyle(CompactionStyle.FIFO);
+//			}
+//		});
+//
+//		assertNotNull(rocksDbBackend.getOptions());
+//		assertEquals(CompactionStyle.FIFO, rocksDbBackend.getColumnOptions().compactionStyle());
+//	}
+//
+//	@Test
+//	public void testPredefinedAndOptionsFactory() throws Exception {
+//		RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI);
+//
+//		assertEquals(PredefinedOptions.DEFAULT, rocksDbBackend.getPredefinedOptions());
+//
+//		rocksDbBackend.setPredefinedOptions(PredefinedOptions.SPINNING_DISK_OPTIMIZED);
+//		rocksDbBackend.setOptions(new OptionsFactory() {
+//			@Override
+//			public DBOptions createDBOptions(DBOptions currentOptions) {
+//				return currentOptions;
+//			}
+//
+//			@Override
+//			public ColumnFamilyOptions createColumnOptions(ColumnFamilyOptions currentOptions) {
+//				return currentOptions.setCompactionStyle(CompactionStyle.UNIVERSAL);
+//			}
+//		});
+//
+//		assertEquals(PredefinedOptions.SPINNING_DISK_OPTIMIZED, rocksDbBackend.getPredefinedOptions());
+//		assertNotNull(rocksDbBackend.getOptions());
+//		assertEquals(CompactionStyle.UNIVERSAL, rocksDbBackend.getColumnOptions().compactionStyle());
+//	}
+//
+//	@Test
+//	public void testPredefinedOptionsEnum() {
+//		for (PredefinedOptions o : PredefinedOptions.values()) {
+//			DBOptions opt = o.createDBOptions();
+//			try {
+//				assertNotNull(opt);
+//			} finally {
+//				opt.dispose();
+//			}
+//		}
+//	}
+//
+//	// ------------------------------------------------------------------------
+//	//  Contained Non-partitioned State Backend
+//	// ------------------------------------------------------------------------
+//
+//	@Test
+//	public void testCallsForwardedToNonPartitionedBackend() throws Exception {
+//		AbstractStateBackend nonPartBackend = mock(AbstractStateBackend.class);
+//		RocksDBStateBackend rocksDbBackend = new RocksDBStateBackend(TEMP_URI, nonPartBackend);
+//
+//		rocksDbBackend.initializeForJob(getMockEnvironment(), "foo", IntSerializer.INSTANCE);
+//		verify(nonPartBackend, times(1)).initializeForJob(any(Environment.class), anyString(), any(TypeSerializer.class));
+//
+//		rocksDbBackend.disposeAllStateForCurrentJob();
+//		verify(nonPartBackend, times(1)).disposeAllStateForCurrentJob();
+//
+//		rocksDbBackend.close();
+//		verify(nonPartBackend, times(1)).close();
+//	}
+//
+//	// ------------------------------------------------------------------------
+//	//  Utilities
+//	// ------------------------------------------------------------------------
+//
+//	private static Environment getMockEnvironment() {
+//		return getMockEnvironment(new File[] { new File(System.getProperty("java.io.tmpdir")) });
+//	}
+//
+//	private static Environment getMockEnvironment(File[] tempDirs) {
+//		IOManager ioMan = mock(IOManager.class);
+//		when(ioMan.getSpillingDirectories()).thenReturn(tempDirs);
+//
+//		Environment env = mock(Environment.class);
+//		when(env.getJobID()).thenReturn(new JobID());
+//		when(env.getUserClassLoader()).thenReturn(RocksDBStateBackendConfigTest.class.getClassLoader());
+//		when(env.getIOManager()).thenReturn(ioMan);
+//
+//		TaskInfo taskInfo = mock(TaskInfo.class);
+//		when(env.getTaskInfo()).thenReturn(taskInfo);
+//
+//		when(taskInfo.getIndexOfThisSubtask()).thenReturn(0);
+//		return env;
+//	}
+//}
diff --git a/flink-contrib/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBStateBackendTest.java b/flink-contrib/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBStateBackendTest.java
index 57f906e..9222f0b 100644
--- a/flink-contrib/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBStateBackendTest.java
+++ b/flink-contrib/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/RocksDBStateBackendTest.java
@@ -18,25 +18,23 @@
 
 package org.apache.flink.contrib.streaming.state;
 
-import org.apache.commons.io.FileUtils;
-import org.apache.flink.configuration.ConfigConstants;
 import org.apache.flink.runtime.state.StateBackendTestBase;
-import org.apache.flink.runtime.state.memory.MemoryStateBackend;
+import org.apache.flink.runtime.state.filesystem.FsStateBackend;
 import org.apache.flink.util.OperatingSystem;
 import org.junit.Assume;
 import org.junit.Before;
+import org.junit.Rule;
+import org.junit.rules.TemporaryFolder;
 
-import java.io.File;
 import java.io.IOException;
-import java.util.UUID;
 
 /**
  * Tests for the partitioned state part of {@link RocksDBStateBackend}.
  */
 public class RocksDBStateBackendTest extends StateBackendTestBase<RocksDBStateBackend> {
 
-	private File dbDir;
-	private File chkDir;
+	@Rule
+	public TemporaryFolder tempFolder = new TemporaryFolder();
 
 	@Before
 	public void checkOperatingSystem() {
@@ -45,19 +43,10 @@ public class RocksDBStateBackendTest extends StateBackendTestBase<RocksDBStateBa
 
 	@Override
 	protected RocksDBStateBackend getStateBackend() throws IOException {
-		dbDir = new File(new File(ConfigConstants.DEFAULT_TASK_MANAGER_TMP_PATH, UUID.randomUUID().toString()), "state");
-		chkDir = new File(new File(ConfigConstants.DEFAULT_TASK_MANAGER_TMP_PATH, UUID.randomUUID().toString()), "snapshots");
-
-		RocksDBStateBackend backend = new RocksDBStateBackend(chkDir.getAbsoluteFile().toURI(), new MemoryStateBackend());
-		backend.setDbStoragePath(dbDir.getAbsolutePath());
+		String dbPath = tempFolder.newFolder().getAbsolutePath();
+		String checkpointPath = tempFolder.newFolder().toURI().toString();
+		RocksDBStateBackend backend = new RocksDBStateBackend(checkpointPath, new FsStateBackend(checkpointPath));
+		backend.setDbStoragePath(dbPath);
 		return backend;
 	}
-
-	@Override
-	protected void cleanup() {
-		try {
-			FileUtils.deleteDirectory(dbDir);
-			FileUtils.deleteDirectory(chkDir);
-		} catch (IOException ignore) {}
-	}
 }
diff --git a/flink-contrib/flink-storm-examples/src/test/java/org/apache/flink/storm/tests/StormFieldsGroupingITCase.java b/flink-contrib/flink-storm-examples/src/test/java/org/apache/flink/storm/tests/StormFieldsGroupingITCase.java
index eb04038..5df1337 100644
--- a/flink-contrib/flink-storm-examples/src/test/java/org/apache/flink/storm/tests/StormFieldsGroupingITCase.java
+++ b/flink-contrib/flink-storm-examples/src/test/java/org/apache/flink/storm/tests/StormFieldsGroupingITCase.java
@@ -61,12 +61,22 @@ public class StormFieldsGroupingITCase extends StreamingProgramTestBase {
 		List<String> actualResults = new ArrayList<>();
 		readAllResultLines(actualResults, resultPath, new String[0], false);
 
+		//remove potential operator id prefix
+		for(int i = 0; i < actualResults.size(); ++i) {
+			String s = actualResults.get(i);
+			if(s.contains(">")) {
+				s = s.substring(s.indexOf(">") + 2);
+				actualResults.set(i, s);
+			}
+		}
+
 		Assert.assertEquals(expectedResults.size(),actualResults.size());
 		Collections.sort(actualResults);
 		Collections.sort(expectedResults);
+		System.out.println(actualResults);
 		for(int i=0; i< actualResults.size(); ++i) {
 			//compare against actual results with removed prefex (as it depends e.g. on the hash function used)
-			Assert.assertEquals(expectedResults.get(i), actualResults.get(i).substring(3));
+			Assert.assertEquals(expectedResults.get(i), actualResults.get(i));
 		}
 	}
 
diff --git a/flink-contrib/flink-storm/src/main/java/org/apache/flink/storm/wrappers/BoltWrapper.java b/flink-contrib/flink-storm/src/main/java/org/apache/flink/storm/wrappers/BoltWrapper.java
index 6e316e7..d59ff04 100644
--- a/flink-contrib/flink-storm/src/main/java/org/apache/flink/storm/wrappers/BoltWrapper.java
+++ b/flink-contrib/flink-storm/src/main/java/org/apache/flink/storm/wrappers/BoltWrapper.java
@@ -294,7 +294,7 @@ public class BoltWrapper<IN, OUT> extends AbstractStreamOperator<OUT> implements
 	}
 
 	@Override
-	public void dispose() {
+	public void dispose() throws Exception {
 		super.dispose();
 		this.bolt.cleanup();
 	}
diff --git a/flink-contrib/flink-storm/src/test/java/org/apache/flink/storm/wrappers/BoltWrapperTest.java b/flink-contrib/flink-storm/src/test/java/org/apache/flink/storm/wrappers/BoltWrapperTest.java
index 2ebb917..c15b5f6 100644
--- a/flink-contrib/flink-storm/src/test/java/org/apache/flink/storm/wrappers/BoltWrapperTest.java
+++ b/flink-contrib/flink-storm/src/test/java/org/apache/flink/storm/wrappers/BoltWrapperTest.java
@@ -327,7 +327,7 @@ public class BoltWrapperTest extends AbstractTest {
 
 	private static final class TestBolt implements IRichBolt {
 		private static final long serialVersionUID = 7278692872260138758L;
-		private OutputCollector collector;
+		private transient OutputCollector collector;
 
 		@SuppressWarnings("rawtypes")
 		@Override
@@ -366,7 +366,7 @@ public class BoltWrapperTest extends AbstractTest {
 
 	public static StreamTask<?, ?> createMockStreamTask(ExecutionConfig execConfig) {
 		Environment env = mock(Environment.class);
-		when(env.getTaskInfo()).thenReturn(new TaskInfo("Mock Task", 0, 1, 0));
+		when(env.getTaskInfo()).thenReturn(new TaskInfo("Mock Task", 1, 0, 1, 0));
 		when(env.getUserClassLoader()).thenReturn(BoltWrapperTest.class.getClassLoader());
 		when(env.getMetricGroup()).thenReturn(new UnregisteredTaskMetricsGroup());
 
diff --git a/flink-core/src/main/java/org/apache/flink/api/common/TaskInfo.java b/flink-core/src/main/java/org/apache/flink/api/common/TaskInfo.java
index ac87e74..5627ca8 100644
--- a/flink-core/src/main/java/org/apache/flink/api/common/TaskInfo.java
+++ b/flink-core/src/main/java/org/apache/flink/api/common/TaskInfo.java
@@ -31,16 +31,20 @@ public class TaskInfo {
 
 	private final String taskName;
 	private final String taskNameWithSubtasks;
+	private final int numberOfKeyGroups;
 	private final int indexOfSubtask;
 	private final int numberOfParallelSubtasks;
 	private final int attemptNumber;
 
-	public TaskInfo(String taskName, int indexOfSubtask, int numberOfParallelSubtasks, int attemptNumber) {
+	public TaskInfo(String taskName, int numberOfKeyGroups, int indexOfSubtask, int numberOfParallelSubtasks, int attemptNumber) {
 		checkArgument(indexOfSubtask >= 0, "Task index must be a non-negative number.");
+		checkArgument(numberOfKeyGroups >= 1, "Max parallelism must be a positive number.");
+		checkArgument(numberOfKeyGroups >= numberOfParallelSubtasks, "Max parallelism must be >= than parallelism.");
 		checkArgument(numberOfParallelSubtasks >= 1, "Parallelism must be a positive number.");
 		checkArgument(indexOfSubtask < numberOfParallelSubtasks, "Task index must be less than parallelism.");
 		checkArgument(attemptNumber >= 0, "Attempt number must be a non-negative number.");
 		this.taskName = checkNotNull(taskName, "Task Name must not be null.");
+		this.numberOfKeyGroups = numberOfKeyGroups;
 		this.indexOfSubtask = indexOfSubtask;
 		this.numberOfParallelSubtasks = numberOfParallelSubtasks;
 		this.attemptNumber = attemptNumber;
@@ -57,6 +61,13 @@ public class TaskInfo {
 	}
 
 	/**
+	 * Gets the number of key groups aka the max parallelism aka the max number of subtasks.
+	 */
+	public int getNumberOfKeyGroups() {
+		return numberOfKeyGroups;
+	}
+
+	/**
 	 * Gets the number of this parallel subtask. The numbering starts from 0 and goes up to
 	 * parallelism-1 (parallelism as returned by {@link #getNumberOfParallelSubtasks()}).
 	 *
diff --git a/flink-core/src/main/java/org/apache/flink/api/common/operators/CollectionExecutor.java b/flink-core/src/main/java/org/apache/flink/api/common/operators/CollectionExecutor.java
index 913b205..d9240fe 100644
--- a/flink-core/src/main/java/org/apache/flink/api/common/operators/CollectionExecutor.java
+++ b/flink-core/src/main/java/org/apache/flink/api/common/operators/CollectionExecutor.java
@@ -183,7 +183,7 @@ public class CollectionExecutor {
 		GenericDataSinkBase<IN> typedSink = (GenericDataSinkBase<IN>) sink;
 
 		// build the runtime context and compute broadcast variables, if necessary
-		TaskInfo taskInfo = new TaskInfo(typedSink.getName(), 0, 1, 0);
+		TaskInfo taskInfo = new TaskInfo(typedSink.getName(), 1, 0, 1, 0);
 		RuntimeUDFContext ctx;
 
 		MetricGroup metrics = new UnregisteredMetricsGroup();
@@ -203,7 +203,7 @@ public class CollectionExecutor {
 		@SuppressWarnings("unchecked")
 		GenericDataSourceBase<OUT, ?> typedSource = (GenericDataSourceBase<OUT, ?>) source;
 		// build the runtime context and compute broadcast variables, if necessary
-		TaskInfo taskInfo = new TaskInfo(typedSource.getName(), 0, 1, 0);
+		TaskInfo taskInfo = new TaskInfo(typedSource.getName(), 1, 0, 1, 0);
 		
 		RuntimeUDFContext ctx;
 
@@ -230,7 +230,7 @@ public class CollectionExecutor {
 		SingleInputOperator<IN, OUT, ?> typedOp = (SingleInputOperator<IN, OUT, ?>) operator;
 		
 		// build the runtime context and compute broadcast variables, if necessary
-		TaskInfo taskInfo = new TaskInfo(typedOp.getName(), 0, 1, 0);
+		TaskInfo taskInfo = new TaskInfo(typedOp.getName(), 1, 0, 1, 0);
 		RuntimeUDFContext ctx;
 
 		MetricGroup metrics = new UnregisteredMetricsGroup();
@@ -270,7 +270,7 @@ public class CollectionExecutor {
 		DualInputOperator<IN1, IN2, OUT, ?> typedOp = (DualInputOperator<IN1, IN2, OUT, ?>) operator;
 		
 		// build the runtime context and compute broadcast variables, if necessary
-		TaskInfo taskInfo = new TaskInfo(typedOp.getName(), 0, 1, 0);
+		TaskInfo taskInfo = new TaskInfo(typedOp.getName(), 1, 0, 1, 0);
 		RuntimeUDFContext ctx;
 
 		MetricGroup metrics = new UnregisteredMetricsGroup();
diff --git a/flink-core/src/main/java/org/apache/flink/core/fs/FSDataOutputStream.java b/flink-core/src/main/java/org/apache/flink/core/fs/FSDataOutputStream.java
index a6becf7..0318d1f 100644
--- a/flink-core/src/main/java/org/apache/flink/core/fs/FSDataOutputStream.java
+++ b/flink-core/src/main/java/org/apache/flink/core/fs/FSDataOutputStream.java
@@ -29,6 +29,8 @@ import java.io.OutputStream;
 @Public
 public abstract class FSDataOutputStream extends OutputStream {
 
+	public abstract long getPos() throws IOException;
+
 	public abstract void flush() throws IOException;
 
 	public abstract void sync() throws IOException;
diff --git a/flink-core/src/main/java/org/apache/flink/core/fs/local/LocalDataOutputStream.java b/flink-core/src/main/java/org/apache/flink/core/fs/local/LocalDataOutputStream.java
index 54ec8dd..c3b793d 100644
--- a/flink-core/src/main/java/org/apache/flink/core/fs/local/LocalDataOutputStream.java
+++ b/flink-core/src/main/java/org/apache/flink/core/fs/local/LocalDataOutputStream.java
@@ -90,4 +90,9 @@ public class LocalDataOutputStream extends FSDataOutputStream {
 	public void sync() throws IOException {
 		fos.getFD().sync();
 	}
+
+	@Override
+	public long getPos() throws IOException {
+		return fos.getChannel().position();
+	}
 }
diff --git a/flink-core/src/main/java/org/apache/flink/core/memory/ByteArrayOutputStreamWithPos.java b/flink-core/src/main/java/org/apache/flink/core/memory/ByteArrayOutputStreamWithPos.java
new file mode 100644
index 0000000..285e016
--- /dev/null
+++ b/flink-core/src/main/java/org/apache/flink/core/memory/ByteArrayOutputStreamWithPos.java
@@ -0,0 +1,281 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.core.memory;
+
+import org.apache.flink.util.Preconditions;
+
+import java.io.IOException;
+import java.io.OutputStream;
+import java.io.UnsupportedEncodingException;
+import java.util.Arrays;
+
+/**
+ * Un-synchronized copy of Java's ByteArrayOutputStream that also exposes the current position.
+ */
+public class ByteArrayOutputStreamWithPos extends OutputStream {
+
+	/**
+	 * The buffer where data is stored.
+	 */
+	protected byte[] buf;
+
+	/**
+	 * The number of valid bytes in the buffer.
+	 */
+	protected int count;
+
+	/**
+	 * Creates a new byte array output stream. The buffer capacity is
+	 * initially 32 bytes, though its size increases if necessary.
+	 */
+	public ByteArrayOutputStreamWithPos() {
+		this(32);
+	}
+
+	/**
+	 * Creates a new byte array output stream, with a buffer capacity of
+	 * the specified size, in bytes.
+	 *
+	 * @param size the initial size.
+	 * @throws IllegalArgumentException if size is negative.
+	 */
+	public ByteArrayOutputStreamWithPos(int size) {
+		if (size < 0) {
+			throw new IllegalArgumentException("Negative initial size: "
+					+ size);
+		}
+		buf = new byte[size];
+	}
+
+	/**
+	 * Increases the capacity if necessary to ensure that it can hold
+	 * at least the number of elements specified by the minimum
+	 * capacity argument.
+	 *
+	 * @param minCapacity the desired minimum capacity
+	 * @throws OutOfMemoryError if {@code minCapacity < 0}.  This is
+	 *                          interpreted as a request for the unsatisfiably large capacity
+	 *                          {@code (long) Integer.MAX_VALUE + (minCapacity - Integer.MAX_VALUE)}.
+	 */
+	private void ensureCapacity(int minCapacity) {
+		// overflow-conscious code
+		if (minCapacity - buf.length > 0) {
+			grow(minCapacity);
+		}
+	}
+
+	/**
+	 * Increases the capacity to ensure that it can hold at least the
+	 * number of elements specified by the minimum capacity argument.
+	 *
+	 * @param minCapacity the desired minimum capacity
+	 */
+	private void grow(int minCapacity) {
+		// overflow-conscious code
+		int oldCapacity = buf.length;
+		int newCapacity = oldCapacity << 1;
+		if (newCapacity - minCapacity < 0) {
+			newCapacity = minCapacity;
+		}
+		if (newCapacity < 0) {
+			if (minCapacity < 0) { // overflow
+				throw new OutOfMemoryError();
+			}
+			newCapacity = Integer.MAX_VALUE;
+		}
+		buf = Arrays.copyOf(buf, newCapacity);
+	}
+
+	/**
+	 * Writes the specified byte to this byte array output stream.
+	 *
+	 * @param b the byte to be written.
+	 */
+	public void write(int b) {
+		ensureCapacity(count + 1);
+		buf[count] = (byte) b;
+		count += 1;
+	}
+
+	/**
+	 * Writes <code>len</code> bytes from the specified byte array
+	 * starting at offset <code>off</code> to this byte array output stream.
+	 *
+	 * @param b   the data.
+	 * @param off the start offset in the data.
+	 * @param len the number of bytes to write.
+	 */
+	public void write(byte[] b, int off, int len) {
+		if ((off < 0) || (off > b.length) || (len < 0) ||
+				((off + len) - b.length > 0)) {
+			throw new IndexOutOfBoundsException();
+		}
+		ensureCapacity(count + len);
+		System.arraycopy(b, off, buf, count, len);
+		count += len;
+	}
+
+	/**
+	 * Writes the complete contents of this byte array output stream to
+	 * the specified output stream argument, as if by calling the output
+	 * stream's write method using <code>out.write(buf, 0, count)</code>.
+	 *
+	 * @param out the output stream to which to write the data.
+	 * @throws IOException if an I/O error occurs.
+	 */
+	public void writeTo(OutputStream out) throws IOException {
+		out.write(buf, 0, count);
+	}
+
+	/**
+	 * Resets the <code>count</code> field of this byte array output
+	 * stream to zero, so that all currently accumulated output in the
+	 * output stream is discarded. The output stream can be used again,
+	 * reusing the already allocated buffer space.
+	 *
+	 * @see java.io.ByteArrayInputStream#count
+	 */
+	public void reset() {
+		count = 0;
+	}
+
+	/**
+	 * Creates a newly allocated byte array. Its size is the current
+	 * size of this output stream and the valid contents of the buffer
+	 * have been copied into it.
+	 *
+	 * @return the current contents of this output stream, as a byte array.
+	 * @see java.io.ByteArrayOutputStream#size()
+	 */
+	public byte toByteArray()[] {
+		return Arrays.copyOf(buf, count);
+	}
+
+	/**
+	 * Returns the current size of the buffer.
+	 *
+	 * @return the value of the <code>count</code> field, which is the number
+	 * of valid bytes in this output stream.
+	 * @see java.io.ByteArrayOutputStream#count
+	 */
+	public int size() {
+		return count;
+	}
+
+	/**
+	 * Converts the buffer's contents into a string decoding bytes using the
+	 * platform's default character set. The length of the new <tt>String</tt>
+	 * is a function of the character set, and hence may not be equal to the
+	 * size of the buffer.
+	 * <p>
+	 * <p> This method always replaces malformed-input and unmappable-character
+	 * sequences with the default replacement string for the platform's
+	 * default character set. The {@linkplain java.nio.charset.CharsetDecoder}
+	 * class should be used when more control over the decoding process is
+	 * required.
+	 *
+	 * @return String decoded from the buffer's contents.
+	 * @since JDK1.1
+	 */
+	public String toString() {
+		return new String(buf, 0, count);
+	}
+
+	/**
+	 * Converts the buffer's contents into a string by decoding the bytes using
+	 * the named {@link java.nio.charset.Charset charset}. The length of the new
+	 * <tt>String</tt> is a function of the charset, and hence may not be equal
+	 * to the length of the byte array.
+	 * <p>
+	 * <p> This method always replaces malformed-input and unmappable-character
+	 * sequences with this charset's default replacement string. The {@link
+	 * java.nio.charset.CharsetDecoder} class should be used when more control
+	 * over the decoding process is required.
+	 *
+	 * @param charsetName the name of a supported
+	 *                    {@link java.nio.charset.Charset charset}
+	 * @return String decoded from the buffer's contents.
+	 * @throws UnsupportedEncodingException If the named charset is not supported
+	 * @since JDK1.1
+	 */
+	public String toString(String charsetName)
+			throws UnsupportedEncodingException {
+		return new String(buf, 0, count, charsetName);
+	}
+
+	/**
+	 * Creates a newly allocated string. Its size is the current size of
+	 * the output stream and the valid contents of the buffer have been
+	 * copied into it. Each character <i>c</i> in the resulting string is
+	 * constructed from the corresponding element <i>b</i> in the byte
+	 * array such that:
+	 * <blockquote><pre>
+	 *     c == (char)(((hibyte &amp; 0xff) &lt;&lt; 8) | (b &amp; 0xff))
+	 * </pre></blockquote>
+	 *
+	 * @param hibyte the high byte of each resulting Unicode character.
+	 * @return the current contents of the output stream, as a string.
+	 * @see java.io.ByteArrayOutputStream#size()
+	 * @see java.io.ByteArrayOutputStream#toString(String)
+	 * @see java.io.ByteArrayOutputStream#toString()
+	 * @deprecated This method does not properly convert bytes into characters.
+	 * As of JDK&nbsp;1.1, the preferred way to do this is via the
+	 * <code>toString(String enc)</code> method, which takes an encoding-name
+	 * argument, or the <code>toString()</code> method, which uses the
+	 * platform's default character encoding.
+	 */
+	@Deprecated
+	public String toString(int hibyte) {
+		return new String(buf, hibyte, 0, count);
+	}
+
+	/**
+	 * Closing a <tt>ByteArrayOutputStream</tt> has no effect. The methods in
+	 * this class can be called after the stream has been closed without
+	 * generating an <tt>IOException</tt>.
+	 */
+	public void close() throws IOException {
+	}
+
+	/**
+	 * Returns the read/write offset position for the stream.
+	 * @return the current position in the stream.
+	 */
+	public int getPosition() {
+		return count;
+	}
+
+	/**
+	 * Sets the read/write offset position for the stream.
+	 *
+	 * @param position the position to which the offset in the stream shall be set. Must be < getEndPosition
+	 */
+	public void setPosition(int position) {
+		Preconditions.checkArgument(position < getEndPosition(), "Position out of bounds.");
+		count = position;
+	}
+
+	/**
+	 * Returns the size of the internal buffer, which is the current end position for all setPosition calls.
+	 * @return size of the internal buffer
+	 */
+	public int getEndPosition() {
+		return buf.length;
+	}
+}
diff --git a/flink-core/src/test/java/org/apache/flink/api/common/functions/util/RuntimeUDFContextTest.java b/flink-core/src/test/java/org/apache/flink/api/common/functions/util/RuntimeUDFContextTest.java
index 4cd2a64..7c5878d 100644
--- a/flink-core/src/test/java/org/apache/flink/api/common/functions/util/RuntimeUDFContextTest.java
+++ b/flink-core/src/test/java/org/apache/flink/api/common/functions/util/RuntimeUDFContextTest.java
@@ -38,7 +38,7 @@ import org.junit.Test;
 
 public class RuntimeUDFContextTest {
 
-	private final TaskInfo taskInfo = new TaskInfo("test name", 1, 3, 0);
+	private final TaskInfo taskInfo = new TaskInfo("test name", 3, 1, 3, 0);
 
 	@Test
 	public void testBroadcastVariableNotFound() {
diff --git a/flink-core/src/test/java/org/apache/flink/api/common/io/RichInputFormatTest.java b/flink-core/src/test/java/org/apache/flink/api/common/io/RichInputFormatTest.java
index c3cbb58..fc3fb1a 100644
--- a/flink-core/src/test/java/org/apache/flink/api/common/io/RichInputFormatTest.java
+++ b/flink-core/src/test/java/org/apache/flink/api/common/io/RichInputFormatTest.java
@@ -41,7 +41,7 @@ public class RichInputFormatTest {
 	@Test
 	public void testCheckRuntimeContextAccess() {
 		final SerializedInputFormat<Value> inputFormat = new SerializedInputFormat<Value>();
-		final TaskInfo taskInfo = new TaskInfo("test name", 1, 3, 0);
+		final TaskInfo taskInfo = new TaskInfo("test name", 3, 1, 3, 0);
 		inputFormat.setRuntimeContext(
 				new RuntimeUDFContext(
 						taskInfo, getClass().getClassLoader(), new ExecutionConfig(),
diff --git a/flink-core/src/test/java/org/apache/flink/api/common/io/RichOutputFormatTest.java b/flink-core/src/test/java/org/apache/flink/api/common/io/RichOutputFormatTest.java
index 4c303a6..95f8497 100644
--- a/flink-core/src/test/java/org/apache/flink/api/common/io/RichOutputFormatTest.java
+++ b/flink-core/src/test/java/org/apache/flink/api/common/io/RichOutputFormatTest.java
@@ -41,7 +41,7 @@ public class RichOutputFormatTest {
 	@Test
 	public void testCheckRuntimeContextAccess() {
 		final SerializedOutputFormat<Value> inputFormat = new SerializedOutputFormat<Value>();
-		final TaskInfo taskInfo = new TaskInfo("test name", 1, 3, 0);
+		final TaskInfo taskInfo = new TaskInfo("test name", 3, 1, 3, 0);
 		
 		inputFormat.setRuntimeContext(new RuntimeUDFContext(
 				taskInfo, getClass().getClassLoader(), new ExecutionConfig(),
diff --git a/flink-core/src/test/java/org/apache/flink/api/common/operators/GenericDataSinkBaseTest.java b/flink-core/src/test/java/org/apache/flink/api/common/operators/GenericDataSinkBaseTest.java
index 71bb102..b952c58 100644
--- a/flink-core/src/test/java/org/apache/flink/api/common/operators/GenericDataSinkBaseTest.java
+++ b/flink-core/src/test/java/org/apache/flink/api/common/operators/GenericDataSinkBaseTest.java
@@ -93,7 +93,7 @@ public class GenericDataSinkBaseTest implements java.io.Serializable {
 			ExecutionConfig executionConfig = new ExecutionConfig();
 			final HashMap<String, Accumulator<?, ?>> accumulatorMap = new HashMap<String, Accumulator<?, ?>>();
 			final HashMap<String, Future<Path>> cpTasks = new HashMap<>();
-			final TaskInfo taskInfo = new TaskInfo("test_sink", 0, 1, 0);
+			final TaskInfo taskInfo = new TaskInfo("test_sink", 1, 0, 1, 0);
 			executionConfig.disableObjectReuse();
 			in.reset();
 			
diff --git a/flink-core/src/test/java/org/apache/flink/api/common/operators/GenericDataSourceBaseTest.java b/flink-core/src/test/java/org/apache/flink/api/common/operators/GenericDataSourceBaseTest.java
index 2dabe48..9a2b877 100644
--- a/flink-core/src/test/java/org/apache/flink/api/common/operators/GenericDataSourceBaseTest.java
+++ b/flink-core/src/test/java/org/apache/flink/api/common/operators/GenericDataSourceBaseTest.java
@@ -79,7 +79,7 @@ public class GenericDataSourceBaseTest implements java.io.Serializable {
 
 			final HashMap<String, Accumulator<?, ?>> accumulatorMap = new HashMap<String, Accumulator<?, ?>>();
 			final HashMap<String, Future<Path>> cpTasks = new HashMap<>();
-			final TaskInfo taskInfo = new TaskInfo("test_source", 0, 1, 0);
+			final TaskInfo taskInfo = new TaskInfo("test_source", 1, 0, 1, 0);
 
 			ExecutionConfig executionConfig = new ExecutionConfig();
 			executionConfig.disableObjectReuse();
diff --git a/flink-core/src/test/java/org/apache/flink/api/common/operators/base/FlatMapOperatorCollectionTest.java b/flink-core/src/test/java/org/apache/flink/api/common/operators/base/FlatMapOperatorCollectionTest.java
index f125c4b..232f510 100644
--- a/flink-core/src/test/java/org/apache/flink/api/common/operators/base/FlatMapOperatorCollectionTest.java
+++ b/flink-core/src/test/java/org/apache/flink/api/common/operators/base/FlatMapOperatorCollectionTest.java
@@ -77,7 +77,7 @@ public class FlatMapOperatorCollectionTest implements Serializable {
 		} else {
 			executionConfig.enableObjectReuse();
 		}
-		final TaskInfo taskInfo = new TaskInfo("Test UDF", 0, 4, 0);
+		final TaskInfo taskInfo = new TaskInfo("Test UDF", 4, 0, 4, 0);
 		// run on collections
 		final List<String> result = getTestFlatMapOperator(udf)
 				.executeOnCollections(input,
diff --git a/flink-core/src/test/java/org/apache/flink/api/common/operators/base/InnerJoinOperatorBaseTest.java b/flink-core/src/test/java/org/apache/flink/api/common/operators/base/InnerJoinOperatorBaseTest.java
index 8befcb9..72f2f2e 100644
--- a/flink-core/src/test/java/org/apache/flink/api/common/operators/base/InnerJoinOperatorBaseTest.java
+++ b/flink-core/src/test/java/org/apache/flink/api/common/operators/base/InnerJoinOperatorBaseTest.java
@@ -121,7 +121,7 @@ public class InnerJoinOperatorBaseTest implements Serializable {
 
 
 		try {
-			final TaskInfo taskInfo = new TaskInfo(taskName, 0, 1, 0);
+			final TaskInfo taskInfo = new TaskInfo(taskName, 1, 0, 1, 0);
 			final HashMap<String, Accumulator<?, ?>> accumulatorMap = new HashMap<String, Accumulator<?, ?>>();
 			final HashMap<String, Future<Path>> cpTasks = new HashMap<>();
 
diff --git a/flink-core/src/test/java/org/apache/flink/api/common/operators/base/MapOperatorTest.java b/flink-core/src/test/java/org/apache/flink/api/common/operators/base/MapOperatorTest.java
index d79e2a5..1b2af7c 100644
--- a/flink-core/src/test/java/org/apache/flink/api/common/operators/base/MapOperatorTest.java
+++ b/flink-core/src/test/java/org/apache/flink/api/common/operators/base/MapOperatorTest.java
@@ -112,7 +112,7 @@ public class MapOperatorTest implements java.io.Serializable {
 			List<String> input = new ArrayList<String>(asList("1", "2", "3", "4", "5", "6"));
 			final HashMap<String, Accumulator<?, ?>> accumulatorMap = new HashMap<String, Accumulator<?, ?>>();
 			final HashMap<String, Future<Path>> cpTasks = new HashMap<>();
-			final TaskInfo taskInfo = new TaskInfo(taskName, 0, 1, 0);
+			final TaskInfo taskInfo = new TaskInfo(taskName, 1, 0, 1, 0);
 			ExecutionConfig executionConfig = new ExecutionConfig();
 			executionConfig.disableObjectReuse();
 			
diff --git a/flink-core/src/test/java/org/apache/flink/api/common/operators/base/PartitionMapOperatorTest.java b/flink-core/src/test/java/org/apache/flink/api/common/operators/base/PartitionMapOperatorTest.java
index 83c194a..e709152 100644
--- a/flink-core/src/test/java/org/apache/flink/api/common/operators/base/PartitionMapOperatorTest.java
+++ b/flink-core/src/test/java/org/apache/flink/api/common/operators/base/PartitionMapOperatorTest.java
@@ -83,7 +83,7 @@ public class PartitionMapOperatorTest implements java.io.Serializable {
 			
 			List<String> input = new ArrayList<String>(asList("1", "2", "3", "4", "5", "6"));
 
-			final TaskInfo taskInfo = new TaskInfo(taskName, 0, 1, 0);
+			final TaskInfo taskInfo = new TaskInfo(taskName, 1, 0, 1, 0);
 
 			ExecutionConfig executionConfig = new ExecutionConfig();
 			executionConfig.disableObjectReuse();
diff --git a/flink-fs-tests/src/test/java/org/apache/flink/hdfstests/FileStateBackendTest.java b/flink-fs-tests/src/test/java/org/apache/flink/hdfstests/FileStateBackendTest.java
index fd7bf5d..df40998 100644
--- a/flink-fs-tests/src/test/java/org/apache/flink/hdfstests/FileStateBackendTest.java
+++ b/flink-fs-tests/src/test/java/org/apache/flink/hdfstests/FileStateBackendTest.java
@@ -20,17 +20,16 @@ package org.apache.flink.hdfstests;
 
 import org.apache.commons.io.FileUtils;
 
+import org.apache.flink.api.common.JobID;
 import org.apache.flink.configuration.ConfigConstants;
 import org.apache.flink.core.fs.FileStatus;
 import org.apache.flink.core.fs.FileSystem;
 import org.apache.flink.core.fs.Path;
 import org.apache.flink.core.testutils.CommonTestUtils;
+import org.apache.flink.runtime.state.CheckpointStreamFactory;
 import org.apache.flink.runtime.state.StateBackendTestBase;
 import org.apache.flink.runtime.state.filesystem.FileStateHandle;
 import org.apache.flink.runtime.state.filesystem.FsStateBackend;
-import org.apache.flink.runtime.operators.testutils.DummyEnvironment;
-import org.apache.flink.api.common.typeutils.base.IntSerializer;
-import org.apache.flink.runtime.state.AbstractStateBackend;
 import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;
 
 import org.apache.hadoop.conf.Configuration;
@@ -51,25 +50,23 @@ import java.util.UUID;
 import static org.junit.Assert.assertArrayEquals;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.assertNull;
 import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
 
 public class FileStateBackendTest extends StateBackendTestBase<FsStateBackend> {
-	
+
 	private static File TEMP_DIR;
-	
+
 	private static String HDFS_ROOT_URI;
-	
+
 	private static MiniDFSCluster HDFS_CLUSTER;
-	
+
 	private static FileSystem FS;
-	
+
 	// ------------------------------------------------------------------------
 	//  startup / shutdown
 	// ------------------------------------------------------------------------
-	
+
 	@BeforeClass
 	public static void createHDFS() {
 		try {
@@ -82,7 +79,7 @@ public class FileStateBackendTest extends StateBackendTestBase<FsStateBackend> {
 
 			HDFS_ROOT_URI = "hdfs://" + HDFS_CLUSTER.getURI().getHost() + ":"
 					+ HDFS_CLUSTER.getNameNodePort() + "/";
-			
+
 			FS = FileSystem.get(new URI(HDFS_ROOT_URI));
 		}
 		catch (Exception e) {
@@ -109,11 +106,6 @@ public class FileStateBackendTest extends StateBackendTestBase<FsStateBackend> {
 
 	}
 
-	@Override
-	protected void cleanup() throws Exception {
-		FileSystem.get(stateBaseURI).delete(new Path(stateBaseURI), true);
-	}
-
 	// ------------------------------------------------------------------------
 	//  Tests
 	// ------------------------------------------------------------------------
@@ -132,60 +124,19 @@ public class FileStateBackendTest extends StateBackendTestBase<FsStateBackend> {
 	public void testReducingStateRestoreWithWrongSerializers() {}
 
 	@Test
-	public void testSetupAndSerialization() {
-		try {
-			URI baseUri = new URI(HDFS_ROOT_URI + UUID.randomUUID().toString());
-			
-			FsStateBackend originalBackend = new FsStateBackend(baseUri);
-
-			assertFalse(originalBackend.isInitialized());
-			assertEquals(baseUri, originalBackend.getBasePath().toUri());
-			assertNull(originalBackend.getCheckpointDirectory());
-
-			// serialize / copy the backend
-			FsStateBackend backend = CommonTestUtils.createCopySerializable(originalBackend);
-			assertFalse(backend.isInitialized());
-			assertEquals(baseUri, backend.getBasePath().toUri());
-			assertNull(backend.getCheckpointDirectory());
-
-			// no file operations should be possible right now
-			try {
-				FsStateBackend.FsCheckpointStateOutputStream out = backend.createCheckpointStateOutputStream(
-						2L,
-						System.currentTimeMillis());
-
-				out.write(1);
-				out.closeAndGetHandle();
-				fail("should fail with an exception");
-			} catch (IllegalStateException e) {
-				// supreme!
-			}
-
-			backend.initializeForJob(new DummyEnvironment("test", 1, 0), "dummy", IntSerializer.INSTANCE);
-			assertNotNull(backend.getCheckpointDirectory());
-
-			Path checkpointDir = backend.getCheckpointDirectory();
-			assertTrue(FS.exists(checkpointDir));
-			assertTrue(isDirectoryEmpty(checkpointDir));
+	public void testStateOutputStream() {
+		URI basePath = randomHdfsFileUri();
 
-			backend.disposeAllStateForCurrentJob();
-			assertNull(backend.getCheckpointDirectory());
+		try {
+			FsStateBackend backend = CommonTestUtils.createCopySerializable(new FsStateBackend(basePath, 15));
+			JobID jobId = new JobID();
 
-			assertTrue(isDirectoryEmpty(baseUri));
-		}
-		catch (Exception e) {
-			e.printStackTrace();
-			fail(e.getMessage());
-		}
-	}
 
-	@Test
-	public void testStateOutputStream() {
-		try {
-			FsStateBackend backend = CommonTestUtils.createCopySerializable(new FsStateBackend(randomHdfsFileUri(), 15));
-			backend.initializeForJob(new DummyEnvironment("test", 1, 0), "dummy", IntSerializer.INSTANCE);
+			CheckpointStreamFactory streamFactory = backend.createStreamFactory(jobId, "test_op");
 
-			Path checkpointDir = backend.getCheckpointDirectory();
+			// we know how FsCheckpointStreamFactory is implemented so we know where it
+			// will store checkpoints
+			Path checkpointPath = new Path(new Path(basePath), jobId.toString());
 
 			byte[] state1 = new byte[1274673];
 			byte[] state2 = new byte[1];
@@ -200,12 +151,12 @@ public class FileStateBackendTest extends StateBackendTestBase<FsStateBackend> {
 
 			long checkpointId = 97231523452L;
 
-			FsStateBackend.FsCheckpointStateOutputStream stream1 =
-					backend.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis());
-			FsStateBackend.FsCheckpointStateOutputStream stream2 =
-					backend.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis());
-			FsStateBackend.FsCheckpointStateOutputStream stream3 =
-					backend.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis());
+			CheckpointStreamFactory.CheckpointStateOutputStream stream1 =
+					streamFactory.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis());
+			CheckpointStreamFactory.CheckpointStateOutputStream stream2 =
+					streamFactory.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis());
+			CheckpointStreamFactory.CheckpointStateOutputStream stream3 =
+					streamFactory.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis());
 
 			stream1.write(state1);
 			stream2.write(state2);
@@ -217,15 +168,15 @@ public class FileStateBackendTest extends StateBackendTestBase<FsStateBackend> {
 
 			// use with try-with-resources
 			FileStateHandle handle4;
-			try (AbstractStateBackend.CheckpointStateOutputStream stream4 =
-						 backend.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis())) {
+			try (CheckpointStreamFactory.CheckpointStateOutputStream stream4 =
+						 streamFactory.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis())) {
 				stream4.write(state4);
 				handle4 = (FileStateHandle) stream4.closeAndGetHandle();
 			}
 
 			// close before accessing handle
-			AbstractStateBackend.CheckpointStateOutputStream stream5 =
-					backend.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis());
+			CheckpointStreamFactory.CheckpointStateOutputStream stream5 =
+					streamFactory.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis());
 			stream5.write(state4);
 			stream5.close();
 			try {
@@ -237,7 +188,7 @@ public class FileStateBackendTest extends StateBackendTestBase<FsStateBackend> {
 
 			validateBytesInStream(handle1.openInputStream(), state1);
 			handle1.discardState();
-			assertFalse(isDirectoryEmpty(checkpointDir));
+			assertFalse(isDirectoryEmpty(checkpointPath));
 			ensureFileDeleted(handle1.getFilePath());
 
 			validateBytesInStream(handle2.openInputStream(), state2);
@@ -248,7 +199,7 @@ public class FileStateBackendTest extends StateBackendTestBase<FsStateBackend> {
 
 			validateBytesInStream(handle4.openInputStream(), state4);
 			handle4.discardState();
-			assertTrue(isDirectoryEmpty(checkpointDir));
+			assertTrue(isDirectoryEmpty(checkpointPath));
 		}
 		catch (Exception e) {
 			e.printStackTrace();
@@ -270,7 +221,7 @@ public class FileStateBackendTest extends StateBackendTestBase<FsStateBackend> {
 	private static boolean isDirectoryEmpty(URI directory) {
 		return isDirectoryEmpty(new Path(directory));
 	}
-	
+
 	private static boolean isDirectoryEmpty(Path directory) {
 		try {
 			FileStatus[] nested = FS.listStatus(directory);
@@ -293,14 +244,14 @@ public class FileStateBackendTest extends StateBackendTestBase<FsStateBackend> {
 
 	private static void validateBytesInStream(InputStream is, byte[] data) throws IOException {
 		byte[] holder = new byte[data.length];
-		
+
 		int pos = 0;
 		int read;
 		while (pos < holder.length && (read = is.read(holder, pos, holder.length - pos)) != -1) {
 			pos += read;
 		}
-			
-		assertEquals("not enough data", holder.length, pos); 
+
+		assertEquals("not enough data", holder.length, pos);
 		assertEquals("too much data", -1, is.read());
 		assertArrayEquals("wrong data", data, holder);
 	}
diff --git a/flink-java/src/test/java/org/apache/flink/api/common/operators/base/CoGroupOperatorCollectionTest.java b/flink-java/src/test/java/org/apache/flink/api/common/operators/base/CoGroupOperatorCollectionTest.java
index 2682584..a4426e0 100644
--- a/flink-java/src/test/java/org/apache/flink/api/common/operators/base/CoGroupOperatorCollectionTest.java
+++ b/flink-java/src/test/java/org/apache/flink/api/common/operators/base/CoGroupOperatorCollectionTest.java
@@ -77,7 +77,7 @@ public class CoGroupOperatorCollectionTest implements Serializable {
 			ExecutionConfig executionConfig = new ExecutionConfig();
 			final HashMap<String, Accumulator<?, ?>> accumulators = new HashMap<String, Accumulator<?, ?>>();
 			final HashMap<String, Future<Path>> cpTasks = new HashMap<>();
-			final TaskInfo taskInfo = new TaskInfo("Test UDF", 0, 4, 0);
+			final TaskInfo taskInfo = new TaskInfo("Test UDF", 4, 0, 4, 0);
 			final RuntimeContext ctx = new RuntimeUDFContext(
 					taskInfo, null, executionConfig, cpTasks, accumulators, new UnregisteredMetricsGroup());
 
diff --git a/flink-java/src/test/java/org/apache/flink/api/common/operators/base/GroupReduceOperatorTest.java b/flink-java/src/test/java/org/apache/flink/api/common/operators/base/GroupReduceOperatorTest.java
index c5a247a..d0784a8 100644
--- a/flink-java/src/test/java/org/apache/flink/api/common/operators/base/GroupReduceOperatorTest.java
+++ b/flink-java/src/test/java/org/apache/flink/api/common/operators/base/GroupReduceOperatorTest.java
@@ -165,7 +165,7 @@ public class GroupReduceOperatorTest implements java.io.Serializable {
 					Integer>("foo", 3), new Tuple2<String, Integer>("bar", 2), new Tuple2<String,
 					Integer>("bar", 4)));
 
-			final TaskInfo taskInfo = new TaskInfo(taskName, 0, 1, 0);
+			final TaskInfo taskInfo = new TaskInfo(taskName, 1, 0, 1, 0);
 
 			ExecutionConfig executionConfig = new ExecutionConfig();
 			executionConfig.disableObjectReuse();
diff --git a/flink-java/src/test/java/org/apache/flink/api/common/operators/base/InnerJoinOperatorBaseTest.java b/flink-java/src/test/java/org/apache/flink/api/common/operators/base/InnerJoinOperatorBaseTest.java
index 89574a8..ef33ac0 100644
--- a/flink-java/src/test/java/org/apache/flink/api/common/operators/base/InnerJoinOperatorBaseTest.java
+++ b/flink-java/src/test/java/org/apache/flink/api/common/operators/base/InnerJoinOperatorBaseTest.java
@@ -107,7 +107,7 @@ public class InnerJoinOperatorBaseTest implements Serializable {
 		));
 
 		try {
-			final TaskInfo taskInfo = new TaskInfo("op", 0, 1, 0);
+			final TaskInfo taskInfo = new TaskInfo("op", 1, 0, 1, 0);
 			ExecutionConfig executionConfig = new ExecutionConfig();
 			
 			executionConfig.disableObjectReuse();
diff --git a/flink-java/src/test/java/org/apache/flink/api/common/operators/base/ReduceOperatorTest.java b/flink-java/src/test/java/org/apache/flink/api/common/operators/base/ReduceOperatorTest.java
index 150854d..9427d6f 100644
--- a/flink-java/src/test/java/org/apache/flink/api/common/operators/base/ReduceOperatorTest.java
+++ b/flink-java/src/test/java/org/apache/flink/api/common/operators/base/ReduceOperatorTest.java
@@ -145,7 +145,7 @@ public class ReduceOperatorTest implements java.io.Serializable {
 					Integer>("foo", 3), new Tuple2<String, Integer>("bar", 2), new Tuple2<String,
 					Integer>("bar", 4)));
 
-			final TaskInfo taskInfo = new TaskInfo(taskName, 0, 1, 0);
+			final TaskInfo taskInfo = new TaskInfo(taskName, 1, 0, 1, 0);
 
 			ExecutionConfig executionConfig = new ExecutionConfig();
 			
diff --git a/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/nfa/NFA.java b/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/nfa/NFA.java
index 624db0d..5ac638e 100644
--- a/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/nfa/NFA.java
+++ b/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/nfa/NFA.java
@@ -520,7 +520,7 @@ public class NFA<T> implements Serializable {
 		public void serialize(NFA<T> record, DataOutputView target) throws IOException {
 			ObjectOutputStream oos = new ObjectOutputStream(new DataOutputViewStream(target));
 			oos.writeObject(record);
-			oos.close();
+			oos.flush();
 		}
 
 		@Override
diff --git a/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/operator/AbstractKeyedCEPPatternOperator.java b/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/operator/AbstractKeyedCEPPatternOperator.java
index e3f924c..09773a2 100644
--- a/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/operator/AbstractKeyedCEPPatternOperator.java
+++ b/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/operator/AbstractKeyedCEPPatternOperator.java
@@ -92,6 +92,8 @@ abstract public class AbstractKeyedCEPPatternOperator<IN, KEY, OUT> extends Abst
 	@Override
 	@SuppressWarnings("unchecked")
 	public void open() throws Exception {
+		super.open();
+
 		if (keys == null) {
 			keys = new HashSet<>();
 		}
diff --git a/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/operator/CEPOperatorTest.java b/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/operator/CEPOperatorTest.java
index 54c1477..52a02d1 100644
--- a/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/operator/CEPOperatorTest.java
+++ b/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/operator/CEPOperatorTest.java
@@ -33,6 +33,7 @@ import org.apache.flink.runtime.state.memory.MemoryStateBackend;
 import org.apache.flink.streaming.api.watermark.Watermark;
 import org.apache.flink.streaming.api.windowing.time.Time;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
+import org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness;
 import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;
 import org.apache.flink.util.TestLogger;
 import org.junit.Rule;
@@ -83,16 +84,15 @@ public class CEPOperatorTest extends TestLogger {
 			}
 		};
 
-		OneInputStreamOperatorTestHarness<Event, Map<String, Event>> harness = new OneInputStreamOperatorTestHarness<>(
-			new KeyedCEPPatternOperator<>(
-				Event.createTypeSerializer(),
-				false,
+		OneInputStreamOperatorTestHarness<Event, Map<String, Event>> harness = new KeyedOneInputStreamOperatorTestHarness<>(
+				new KeyedCEPPatternOperator<>(
+					Event.createTypeSerializer(),
+					false,
+					keySelector,
+					IntSerializer.INSTANCE,
+					new NFAFactory()),
 				keySelector,
-				IntSerializer.INSTANCE,
-			new NFAFactory())
-		);
-
-		harness.configureForKeyedStream(keySelector, BasicTypeInfo.INT_TYPE_INFO);
+				BasicTypeInfo.INT_TYPE_INFO);
 
 		harness.open();
 
@@ -206,15 +206,15 @@ public class CEPOperatorTest extends TestLogger {
 			}
 		};
 
-		OneInputStreamOperatorTestHarness<Event, Map<String, Event>> harness = new OneInputStreamOperatorTestHarness<>(
+		OneInputStreamOperatorTestHarness<Event, Map<String, Event>> harness = new KeyedOneInputStreamOperatorTestHarness<>(
 				new KeyedCEPPatternOperator<>(
 						Event.createTypeSerializer(),
 						false,
 						keySelector,
 						IntSerializer.INSTANCE,
-						new NFAFactory()));
-
-		harness.configureForKeyedStream(keySelector, BasicTypeInfo.INT_TYPE_INFO);
+						new NFAFactory()),
+				keySelector,
+				BasicTypeInfo.INT_TYPE_INFO);
 
 		harness.open();
 
@@ -228,15 +228,16 @@ public class CEPOperatorTest extends TestLogger {
 		// simulate snapshot/restore with some elements in internal sorting queue
 		StreamStateHandle snapshot = harness.snapshot(0, 0);
 
-		harness = new OneInputStreamOperatorTestHarness<>(
+		harness = new KeyedOneInputStreamOperatorTestHarness<>(
 				new KeyedCEPPatternOperator<>(
 						Event.createTypeSerializer(),
 						false,
 						keySelector,
 						IntSerializer.INSTANCE,
-						new NFAFactory()));
+						new NFAFactory()),
+				keySelector,
+				BasicTypeInfo.INT_TYPE_INFO);
 
-		harness.configureForKeyedStream(keySelector, BasicTypeInfo.INT_TYPE_INFO);
 		harness.setup();
 		harness.restore(snapshot);
 		harness.open();
@@ -252,15 +253,16 @@ public class CEPOperatorTest extends TestLogger {
 		// simulate snapshot/restore with empty element queue but NFA state
 		StreamStateHandle snapshot2 = harness.snapshot(1, 1);
 
-		harness = new OneInputStreamOperatorTestHarness<>(
+		harness = new KeyedOneInputStreamOperatorTestHarness<>(
 				new KeyedCEPPatternOperator<>(
 						Event.createTypeSerializer(),
 						false,
 						keySelector,
 						IntSerializer.INSTANCE,
-						new NFAFactory()));
+						new NFAFactory()),
+				keySelector,
+				BasicTypeInfo.INT_TYPE_INFO);
 
-		harness.configureForKeyedStream(keySelector, BasicTypeInfo.INT_TYPE_INFO);
 		harness.setup();
 		harness.restore(snapshot2);
 		harness.open();
@@ -309,16 +311,17 @@ public class CEPOperatorTest extends TestLogger {
 			}
 		};
 
-		OneInputStreamOperatorTestHarness<Event, Map<String, Event>> harness = new OneInputStreamOperatorTestHarness<>(
+		OneInputStreamOperatorTestHarness<Event, Map<String, Event>> harness = new KeyedOneInputStreamOperatorTestHarness<>(
 				new KeyedCEPPatternOperator<>(
 						Event.createTypeSerializer(),
 						false,
 						keySelector,
 						IntSerializer.INSTANCE,
-						new NFAFactory()));
+						new NFAFactory()),
+				keySelector,
+				BasicTypeInfo.INT_TYPE_INFO);
 
 		harness.setStateBackend(rocksDBStateBackend);
-		harness.configureForKeyedStream(keySelector, BasicTypeInfo.INT_TYPE_INFO);
 
 		harness.open();
 
@@ -332,19 +335,21 @@ public class CEPOperatorTest extends TestLogger {
 		// simulate snapshot/restore with some elements in internal sorting queue
 		StreamStateHandle snapshot = harness.snapshot(0, 0);
 
-		harness = new OneInputStreamOperatorTestHarness<>(
+		harness = new KeyedOneInputStreamOperatorTestHarness<>(
 				new KeyedCEPPatternOperator<>(
 						Event.createTypeSerializer(),
 						false,
 						keySelector,
 						IntSerializer.INSTANCE,
-						new NFAFactory()));
+						new NFAFactory()),
+				keySelector,
+				BasicTypeInfo.INT_TYPE_INFO);
 
 		rocksDBStateBackend =
 				new RocksDBStateBackend(rocksDbBackups, new MemoryStateBackend());
 		rocksDBStateBackend.setDbStoragePath(rocksDbPath);
 		harness.setStateBackend(rocksDBStateBackend);
-		harness.configureForKeyedStream(keySelector, BasicTypeInfo.INT_TYPE_INFO);
+
 		harness.setup();
 		harness.restore(snapshot);
 		harness.open();
@@ -360,19 +365,20 @@ public class CEPOperatorTest extends TestLogger {
 		// simulate snapshot/restore with empty element queue but NFA state
 		StreamStateHandle snapshot2 = harness.snapshot(1, 1);
 
-		harness = new OneInputStreamOperatorTestHarness<>(
+		harness = new KeyedOneInputStreamOperatorTestHarness<>(
 				new KeyedCEPPatternOperator<>(
 						Event.createTypeSerializer(),
 						false,
 						keySelector,
 						IntSerializer.INSTANCE,
-						new NFAFactory()));
+						new NFAFactory()),
+				keySelector,
+				BasicTypeInfo.INT_TYPE_INFO);
 
 		rocksDBStateBackend =
 				new RocksDBStateBackend(rocksDbBackups, new MemoryStateBackend());
 		rocksDBStateBackend.setDbStoragePath(rocksDbPath);
 		harness.setStateBackend(rocksDBStateBackend);
-		harness.configureForKeyedStream(keySelector, BasicTypeInfo.INT_TYPE_INFO);
 		harness.setup();
 		harness.restore(snapshot2);
 		harness.open();
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java
index e78e203..e751e08 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java
@@ -46,6 +46,7 @@ import scala.concurrent.Future;
 
 import java.util.ArrayDeque;
 import java.util.ArrayList;
+import java.util.Collection;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.LinkedHashMap;
@@ -821,14 +822,10 @@ public class CheckpointCoordinator {
 						}
 
 						KeyGroupRange subtaskKeyGroupIds = keyGroupPartitions.get(i);
-						List<KeyGroupsStateHandle> subtaskKeyGroupStates = new ArrayList<>();
 
-						for (KeyGroupsStateHandle storedKeyGroup : taskState.getKeyGroupStates()) {
-							KeyGroupsStateHandle intersection = storedKeyGroup.getKeyGroupIntersection(subtaskKeyGroupIds);
-							if(intersection.getNumberOfKeyGroups() > 0) {
-								subtaskKeyGroupStates.add(intersection);
-							}
-						}
+						List<KeyGroupsStateHandle> subtaskKeyGroupStates = getKeyGroupsStateHandles(
+								taskState.getKeyGroupStates(),
+								subtaskKeyGroupIds);
 
 						Execution currentExecutionAttempt = executionJobVertex
 							.getTaskVertices()[i]
@@ -852,6 +849,27 @@ public class CheckpointCoordinator {
 	}
 
 	/**
+	 * Determine the subset of {@link KeyGroupsStateHandle KeyGroupsStateHandles} with correct
+	 * key group index for the given subtask {@link KeyGroupRange}.
+	 *
+	 * <p>This is publicly visible to be used in tests.
+	 */
+	public static List<KeyGroupsStateHandle> getKeyGroupsStateHandles(
+			Collection<KeyGroupsStateHandle> allKeyGroupsHandles,
+			KeyGroupRange subtaskKeyGroupIds) {
+
+		List<KeyGroupsStateHandle> subtaskKeyGroupStates = new ArrayList<>();
+
+		for (KeyGroupsStateHandle storedKeyGroup : allKeyGroupsHandles) {
+			KeyGroupsStateHandle intersection = storedKeyGroup.getKeyGroupIntersection(subtaskKeyGroupIds);
+			if(intersection.getNumberOfKeyGroups() > 0) {
+				subtaskKeyGroupStates.add(intersection);
+			}
+		}
+		return subtaskKeyGroupStates;
+	}
+
+	/**
 	 * Groups the available set of key groups into key group partitions. A key group partition is
 	 * the set of key groups which is assigned to the same task. Each set of the returned list
 	 * constitutes a key group partition.
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/deployment/TaskDeploymentDescriptor.java b/flink-runtime/src/main/java/org/apache/flink/runtime/deployment/TaskDeploymentDescriptor.java
index 8849e93..ca976e4 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/deployment/TaskDeploymentDescriptor.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/deployment/TaskDeploymentDescriptor.java
@@ -59,6 +59,9 @@ public final class TaskDeploymentDescriptor implements Serializable {
 	/** The task's name. */
 	private final String taskName;
 
+	/** The number of key groups aka the max parallelism aka the max number of subtasks. */
+	private final int numberOfKeyGroups;
+
 	/** The task's index in the subtask group. */
 	private final int indexInSubtaskGroup;
 
@@ -110,6 +113,7 @@ public final class TaskDeploymentDescriptor implements Serializable {
 			ExecutionAttemptID executionId,
 			SerializedValue<ExecutionConfig> serializedExecutionConfig,
 			String taskName,
+			int numberOfKeyGroups,
 			int indexInSubtaskGroup,
 			int numberOfSubtasks,
 			int attemptNumber,
@@ -135,6 +139,7 @@ public final class TaskDeploymentDescriptor implements Serializable {
 		this.executionId = checkNotNull(executionId);
 		this.serializedExecutionConfig = checkNotNull(serializedExecutionConfig);
 		this.taskName = checkNotNull(taskName);
+		this.numberOfKeyGroups = numberOfKeyGroups;
 		this.indexInSubtaskGroup = indexInSubtaskGroup;
 		this.numberOfSubtasks = numberOfSubtasks;
 		this.attemptNumber = attemptNumber;
@@ -157,6 +162,7 @@ public final class TaskDeploymentDescriptor implements Serializable {
 		ExecutionAttemptID executionId,
 		SerializedValue<ExecutionConfig> serializedExecutionConfig,
 		String taskName,
+		int numberOfKeyGroups,
 		int indexInSubtaskGroup,
 		int numberOfSubtasks,
 		int attemptNumber,
@@ -176,6 +182,7 @@ public final class TaskDeploymentDescriptor implements Serializable {
 			executionId,
 			serializedExecutionConfig,
 			taskName,
+			numberOfKeyGroups,
 			indexInSubtaskGroup,
 			numberOfSubtasks,
 			attemptNumber,
@@ -227,6 +234,13 @@ public final class TaskDeploymentDescriptor implements Serializable {
 	}
 
 	/**
+	 * Returns the task's number of key groups.
+	 */
+	public int getNumberOfKeyGroups() {
+		return numberOfKeyGroups;
+	}
+
+	/**
 	 * Returns the task's index in the subtask group.
 	 *
 	 * @return the task's index in the subtask group
@@ -253,7 +267,7 @@ public final class TaskDeploymentDescriptor implements Serializable {
 	 * Returns the {@link TaskInfo} object for the subtask
 	 */
 	public TaskInfo getTaskInfo() {
-		return new TaskInfo(taskName, indexInSubtaskGroup, numberOfSubtasks, attemptNumber);
+		return new TaskInfo(taskName, numberOfKeyGroups, indexInSubtaskGroup, numberOfSubtasks, attemptNumber);
 	}
 
 	/**
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java
index f3a8b6d..b215394 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java
@@ -682,6 +682,7 @@ public class ExecutionVertex {
 			executionId,
 			serializedConfig,
 			getTaskName(),
+			getMaxParallelism(),
 			subTaskIndex,
 			getTotalNumberOfParallelSubtasks(),
 			attemptNumber,
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/fs/hdfs/HadoopDataOutputStream.java b/flink-runtime/src/main/java/org/apache/flink/runtime/fs/hdfs/HadoopDataOutputStream.java
index b0ff4b3..d6fbc19 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/fs/hdfs/HadoopDataOutputStream.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/fs/hdfs/HadoopDataOutputStream.java
@@ -51,6 +51,11 @@ public class HadoopDataOutputStream extends FSDataOutputStream {
 	}
 
 	@Override
+	public long getPos() throws IOException {
+		return fdos.getPos();
+	}
+
+	@Override
 	public void flush() throws IOException {
 		if (HFLUSH_METHOD != null) {
 			try {
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/query/KvStateRegistry.java b/flink-runtime/src/main/java/org/apache/flink/runtime/query/KvStateRegistry.java
index e09b868..5213fe9 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/query/KvStateRegistry.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/query/KvStateRegistry.java
@@ -40,7 +40,7 @@ import java.util.concurrent.atomic.AtomicReference;
 public class KvStateRegistry {
 
 	/** All registered KvState instances. */
-	private final ConcurrentHashMap<KvStateID, KvState<?, ?, ?, ?, ?>> registeredKvStates =
+	private final ConcurrentHashMap<KvStateID, KvState<?>> registeredKvStates =
 			new ConcurrentHashMap<>();
 
 	/** Registry listener to be notified on registration/unregistration. */
@@ -83,7 +83,7 @@ public class KvStateRegistry {
 			JobVertexID jobVertexId,
 			int keyGroupIndex,
 			String registrationName,
-			KvState<?, ?, ?, ?, ?> kvState) {
+			KvState<?> kvState) {
 
 		KvStateID kvStateId = new KvStateID();
 
@@ -136,7 +136,7 @@ public class KvStateRegistry {
 	 * @param kvStateId KvStateID to identify the KvState instance
 	 * @return KvState instance identified by the KvStateID or <code>null</code>
 	 */
-	public KvState<?, ?, ?, ?, ?> getKvState(KvStateID kvStateId) {
+	public KvState<?> getKvState(KvStateID kvStateId) {
 		return registeredKvStates.get(kvStateId);
 	}
 
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/query/TaskKvStateRegistry.java b/flink-runtime/src/main/java/org/apache/flink/runtime/query/TaskKvStateRegistry.java
index 15f0160..b5c09aa 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/query/TaskKvStateRegistry.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/query/TaskKvStateRegistry.java
@@ -58,7 +58,7 @@ public class TaskKvStateRegistry {
 	 *                         descriptor used to create the KvState instance)
 	 * @param kvState          The
 	 */
-	public void registerKvState(int keyGroupIndex, String registrationName, KvState<?, ?, ?, ?, ?> kvState) {
+	public void registerKvState(int keyGroupIndex, String registrationName, KvState<?> kvState) {
 		KvStateID kvStateId = registry.registerKvState(jobId, jobVertexId, keyGroupIndex, registrationName, kvState);
 		registeredKvStates.add(new KvStateInfo(keyGroupIndex, registrationName, kvStateId));
 	}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/query/netty/KvStateServerHandler.java b/flink-runtime/src/main/java/org/apache/flink/runtime/query/netty/KvStateServerHandler.java
index 47f2ad6..8201708 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/query/netty/KvStateServerHandler.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/query/netty/KvStateServerHandler.java
@@ -103,7 +103,7 @@ class KvStateServerHandler extends ChannelInboundHandlerAdapter {
 
 				stats.reportRequest();
 
-				KvState<?, ?, ?, ?, ?> kvState = registry.getKvState(request.getKvStateId());
+				KvState<?> kvState = registry.getKvState(request.getKvStateId());
 
 				if (kvState != null) {
 					// Execute actual query async, because it is possibly
@@ -186,7 +186,7 @@ class KvStateServerHandler extends ChannelInboundHandlerAdapter {
 
 		private final KvStateRequest request;
 
-		private final KvState<?, ?, ?, ?, ?> kvState;
+		private final KvState<?> kvState;
 
 		private final KvStateRequestStats stats;
 
@@ -195,7 +195,7 @@ class KvStateServerHandler extends ChannelInboundHandlerAdapter {
 		public AsyncKvStateQueryTask(
 				ChannelHandlerContext ctx,
 				KvStateRequest request,
-				KvState<?, ?, ?, ?, ?> kvState,
+				KvState<?> kvState,
 				KvStateRequestStats stats) {
 
 			this.ctx = Objects.requireNonNull(ctx, "Channel handler context");
@@ -238,6 +238,8 @@ class KvStateServerHandler extends ChannelInboundHandlerAdapter {
 
 					success = true;
 				} else {
+					kvState.getSerializedValue(serializedKeyAndNamespace);
+
 					// No data for the key/namespace. This is considered to be
 					// a failure.
 					ByteBuf unknownKey = KvStateRequestSerializer.serializeKvStateRequestFailure(
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/AbstractHeapState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/AbstractHeapState.java
deleted file mode 100644
index 6fa4575..0000000
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/AbstractHeapState.java
+++ /dev/null
@@ -1,220 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.state;
-
-import org.apache.flink.api.common.state.ListState;
-import org.apache.flink.api.common.state.State;
-import org.apache.flink.api.common.state.StateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.api.java.tuple.Tuple2;
-import org.apache.flink.runtime.query.netty.message.KvStateRequestSerializer;
-import org.apache.flink.util.Preconditions;
-
-import java.util.HashMap;
-import java.util.Map;
-import java.util.concurrent.ConcurrentHashMap;
-
-/**
- * Base class for partitioned {@link ListState} implementations that are backed by a regular
- * heap hash map. The concrete implementations define how the state is checkpointed.
- * 
- * @param <K> The type of the key.
- * @param <N> The type of the namespace.
- * @param <SV> The type of the values in the state.
- * @param <S> The type of State
- * @param <SD> The type of StateDescriptor for the State S
- * @param <Backend> The type of the backend that snapshots this key/value state.
- */
-public abstract class AbstractHeapState<K, N, SV, S extends State, SD extends StateDescriptor<S, ?>, Backend extends AbstractStateBackend>
-		implements KvState<K, N, S, SD, Backend>, State {
-
-	/** Map containing the actual key/value pairs */
-	protected final Map<N, Map<K, SV>> state;
-
-	/** Serializer for the state value. The state value could be a List<V>, for example. */
-	protected final TypeSerializer<SV> stateSerializer;
-
-	/** The serializer for the keys */
-	protected final TypeSerializer<K> keySerializer;
-
-	/** The serializer for the namespace */
-	protected final TypeSerializer<N> namespaceSerializer;
-
-	/** This holds the name of the state and can create an initial default value for the state. */
-	protected final SD stateDesc;
-
-	/** The current key, which the next value methods will refer to */
-	protected K currentKey;
-
-	/** The current namespace, which the access methods will refer to. */
-	protected N currentNamespace = null;
-
-	/** Cache the state map for the current key. */
-	protected Map<K, SV> currentNSState;
-
-	/**
-	 * Creates a new empty key/value state.
-	 *
-	 * @param keySerializer The serializer for the keys.
-	 * @param namespaceSerializer The serializer for the namespace.
-	 * @param stateDesc The state identifier for the state. This contains name
-	 *                           and can create a default state value.
-	 */
-	protected AbstractHeapState(TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		TypeSerializer<SV> stateSerializer,
-		SD stateDesc) {
-		this(keySerializer, namespaceSerializer, stateSerializer, stateDesc, new HashMap<N, Map<K, SV>>());
-	}
-
-	/**
-	 * Creates a new key/value state for the given hash map of key/value pairs.
-	 *
-	 * @param keySerializer The serializer for the keys.
-	 * @param stateDesc The state identifier for the state. This contains name
-	 *                           and can create a default state value.
-	 * @param state The state map to use in this kev/value state. May contain initial state.
-	 */
-	protected AbstractHeapState(TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		TypeSerializer<SV> stateSerializer,
-		SD stateDesc,
-		Map<N, Map<K, SV>> state) {
-
-		Preconditions.checkNotNull(state, "State map");
-
-		// Make sure that the state map supports concurrent read access for
-		// queries. See also #createNewNamespaceMap for the namespace maps.
-		if (stateDesc.isQueryable()) {
-			this.state = new ConcurrentHashMap<>(state);
-		} else {
-			this.state = state;
-		}
-
-		this.keySerializer = Preconditions.checkNotNull(keySerializer);
-		this.namespaceSerializer = Preconditions.checkNotNull(namespaceSerializer);
-		this.stateSerializer = stateSerializer;
-		this.stateDesc = stateDesc;
-	}
-
-	// ------------------------------------------------------------------------
-
-	@Override
-	public final void clear() {
-		if (currentNSState != null) {
-			currentNSState.remove(currentKey);
-			if (currentNSState.isEmpty()) {
-				state.remove(currentNamespace);
-				currentNSState = null;
-			}
-		}
-	}
-
-	@Override
-	public final void setCurrentKey(K currentKey) {
-		this.currentKey = Preconditions.checkNotNull(currentKey, "Key");
-	}
-
-	@Override
-	public final void setCurrentNamespace(N namespace) {
-		if (namespace != null && namespace.equals(this.currentNamespace)) {
-			return;
-		}
-		this.currentNamespace = Preconditions.checkNotNull(namespace, "Namespace");
-		this.currentNSState = state.get(currentNamespace);
-	}
-
-	@Override
-	public byte[] getSerializedValue(byte[] serializedKeyAndNamespace) throws Exception {
-		Preconditions.checkNotNull(serializedKeyAndNamespace, "Serialized key and namespace");
-
-		Tuple2<K, N> keyAndNamespace = KvStateRequestSerializer.deserializeKeyAndNamespace(
-				serializedKeyAndNamespace, keySerializer, namespaceSerializer);
-
-		return getSerializedValue(keyAndNamespace.f0, keyAndNamespace.f1);
-	}
-
-	protected abstract byte[] getSerializedValue(K key, N namespace) throws Exception;
-
-	/**
-	 * Returns the number of all state pairs in this state, across namespaces.
-	 */
-	protected final int size() {
-		int size = 0;
-		for (Map<K, SV> namespace: state.values()) {
-			size += namespace.size();
-		}
-		return size;
-	}
-
-	@Override
-	public void dispose() {
-		state.clear();
-	}
-
-	@Override
-	public SD getStateDescriptor() {
-		return stateDesc;
-	}
-
-	/**
-	 * Gets the serializer for the keys.
-	 *
-	 * @return The serializer for the keys.
-	 */
-	public final TypeSerializer<K> getKeySerializer() {
-		return keySerializer;
-	}
-
-	/**
-	 * Gets the serializer for the namespace.
-	 *
-	 * @return The serializer for the namespace.
-	 */
-	public final TypeSerializer<N> getNamespaceSerializer() {
-		return namespaceSerializer;
-	}
-
-	/**
-	 * Creates a new namespace map.
-	 *
-	 * <p>If the state queryable ({@link StateDescriptor#isQueryable()}, this
-	 * will create a concurrent hash map instead of a regular one.
-	 *
-	 * @return A new namespace map.
-	 */
-	protected Map<K, SV> createNewNamespaceMap() {
-		if (stateDesc.isQueryable()) {
-			return new ConcurrentHashMap<>();
-		} else {
-			return new HashMap<>();
-		}
-	}
-
-	// ------------------------------------------------------------------------
-
-	/**
-	 * Returns the internal state map for testing.
-	 *
-	 * @return The internal state map
-	 */
-	Map<N, Map<K, SV>> getStateMap() {
-		return state;
-	}
-}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/AbstractStateBackend.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/AbstractStateBackend.java
index b2cde22..e6093a8 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/AbstractStateBackend.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/AbstractStateBackend.java
@@ -18,417 +18,57 @@
 
 package org.apache.flink.runtime.state;
 
-import org.apache.flink.api.common.ExecutionConfig;
-import org.apache.flink.api.common.functions.ReduceFunction;
-import org.apache.flink.api.common.state.FoldingState;
-import org.apache.flink.api.common.state.FoldingStateDescriptor;
-import org.apache.flink.api.common.state.ListState;
-import org.apache.flink.api.common.state.ListStateDescriptor;
-import org.apache.flink.api.common.state.MergingState;
-import org.apache.flink.api.common.state.ReducingState;
-import org.apache.flink.api.common.state.ReducingStateDescriptor;
-import org.apache.flink.api.common.state.State;
-import org.apache.flink.api.common.state.StateBackend;
-import org.apache.flink.api.common.state.StateDescriptor;
-import org.apache.flink.api.common.state.ValueState;
-import org.apache.flink.api.common.state.ValueStateDescriptor;
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.api.common.state.KeyGroupAssigner;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.core.fs.FSDataOutputStream;
 import org.apache.flink.runtime.execution.Environment;
 import org.apache.flink.runtime.query.TaskKvStateRegistry;
-import org.apache.flink.util.Preconditions;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.HashMap;
 import java.util.List;
-import java.util.Map;
 
 /**
  * A state backend defines how state is stored and snapshotted during checkpoints.
  */
 public abstract class AbstractStateBackend implements java.io.Serializable {
-	
-	private static final long serialVersionUID = 4620413814639220247L;
-
-	protected transient TypeSerializer<?> keySerializer;
-
-	protected transient ClassLoader userCodeClassLoader;
-
-	protected transient Object currentKey;
-
-	/** For efficient access in setCurrentKey() */
-	private transient KvState<?, ?, ?, ?, ?>[] keyValueStates;
-
-	/** So that we can give out state when the user uses the same key. */
-	protected transient HashMap<String, KvState<?, ?, ?, ?, ?>> keyValueStatesByName;
-
-	/** For caching the last accessed partitioned state */
-	private transient String lastName;
-
-	@SuppressWarnings("rawtypes")
-	private transient KvState lastState;
-
-	/** KvStateRegistry helper for this task */
-	protected transient TaskKvStateRegistry kvStateRegistry;
-
-	/** Key group index of this state backend */
-	protected transient int keyGroupIndex;
-
-	// ------------------------------------------------------------------------
-	//  initialization and cleanup
-	// ------------------------------------------------------------------------
-
-	/**
-	 * This method is called by the task upon deployment to initialize the state backend for
-	 * data for a specific job.
-	 *
-	 * @param env The {@link Environment} of the task that instantiated the state backend
-	 * @param operatorIdentifier Unique identifier for naming states created by this backend
-	 * @throws Exception Overwritten versions of this method may throw exceptions, in which
-	 *                   case the job that uses the state backend is considered failed during
-	 *                   deployment.
-	 */
-	public void initializeForJob(
-			Environment env,
-			String operatorIdentifier,
-			TypeSerializer<?> keySerializer) throws Exception {
-
-		this.userCodeClassLoader = env.getUserClassLoader();
-		this.keySerializer = keySerializer;
-
-		this.keyGroupIndex = env.getTaskInfo().getIndexOfThisSubtask();
-		this.kvStateRegistry = env.getTaskKvStateRegistry();
-	}
-
-	/**
-	 * Disposes all state associated with the current job.
-	 *
-	 * @throws Exception Exceptions may occur during disposal of the state and should be forwarded.
-	 */
-	public abstract void disposeAllStateForCurrentJob() throws Exception;
-
-	/**
-	 * Closes the state backend, releasing all internal resources, but does not delete any persistent
-	 * checkpoint data.
-	 *
-	 * @throws Exception Exceptions can be forwarded and will be logged by the system
-	 */
-	public abstract void close() throws Exception;
-
-	public void discardState() throws Exception {
-		if (kvStateRegistry != null) {
-			kvStateRegistry.unregisterAll();
-		}
-
-		lastName = null;
-		lastState = null;
-		if (keyValueStates != null) {
-			for (KvState<?, ?, ?, ?, ?> state : keyValueStates) {
-				state.dispose();
-			}
-		}
-		keyValueStates = null;
-		keyValueStatesByName = null;
-	}
-	
-	// ------------------------------------------------------------------------
-	//  key/value state
-	// ------------------------------------------------------------------------
-
-	/**
-	 * Creates and returns a new {@link ValueState}.
-	 *
-	 * @param namespaceSerializer TypeSerializer for the state namespace.
-	 * @param stateDesc The {@code StateDescriptor} that contains the name of the state.
-	 *
-	 * @param <N> The type of the namespace.
-	 * @param <T> The type of the value that the {@code ValueState} can store.
-	 */
-	protected abstract <N, T> ValueState<T> createValueState(TypeSerializer<N> namespaceSerializer, ValueStateDescriptor<T> stateDesc) throws Exception;
-
-	/**
-	 * Creates and returns a new {@link ListState}.
-	 *
-	 * @param namespaceSerializer TypeSerializer for the state namespace.
-	 * @param stateDesc The {@code StateDescriptor} that contains the name of the state.
-	 *
-	 * @param <N> The type of the namespace.
-	 * @param <T> The type of the values that the {@code ListState} can store.
-	 */
-	protected abstract <N, T> ListState<T> createListState(TypeSerializer<N> namespaceSerializer, ListStateDescriptor<T> stateDesc) throws Exception;
-
-	/**
-	 * Creates and returns a new {@link ReducingState}.
-	 *
-	 * @param namespaceSerializer TypeSerializer for the state namespace.
-	 * @param stateDesc The {@code StateDescriptor} that contains the name of the state.
-	 *
-	 * @param <N> The type of the namespace.
-	 * @param <T> The type of the values that the {@code ListState} can store.
-	 */
-	protected abstract <N, T> ReducingState<T> createReducingState(TypeSerializer<N> namespaceSerializer, ReducingStateDescriptor<T> stateDesc) throws Exception;
-
-	/**
-	 * Creates and returns a new {@link FoldingState}.
-	 *
-	 * @param namespaceSerializer TypeSerializer for the state namespace.
-	 * @param stateDesc The {@code StateDescriptor} that contains the name of the state.
-	 *
-	 * @param <N> The type of the namespace.
-	 * @param <T> Type of the values folded into the state
-	 * @param <ACC> Type of the value in the state	 *
-	 */
-	protected abstract <N, T, ACC> FoldingState<T, ACC> createFoldingState(TypeSerializer<N> namespaceSerializer, FoldingStateDescriptor<T, ACC> stateDesc) throws Exception;
-
-	/**
-	 * Sets the current key that is used for partitioned state.
-	 * @param currentKey The current key.
-	 */
-	@SuppressWarnings({"unchecked", "rawtypes"})
-	public void setCurrentKey(Object currentKey) {
-		this.currentKey = Preconditions.checkNotNull(currentKey, "Key");
-		if (keyValueStates != null) {
-			for (KvState kv : keyValueStates) {
-				kv.setCurrentKey(currentKey);
-			}
-		}
-	}
-
-	public Object getCurrentKey() {
-		return currentKey;
-	}
+	private static final long serialVersionUID = 4620415814639230247L;
 
 	/**
-	 * Creates or retrieves a partitioned state backed by this state backend.
-	 *
-	 * @param stateDescriptor The state identifier for the state. This contains name
-	 *                           and can create a default state value.
-
-	 * @param <N> The type of the namespace.
-	 * @param <S> The type of the state.
+	 * Creates a {@link CheckpointStreamFactory} that can be used to create streams
+	 * that should end up in a checkpoint.
 	 *
-	 * @return A new key/value state backed by this backend.
-	 *
-	 * @throws Exception Exceptions may occur during initialization of the state and should be forwarded.
+	 * @param jobId The {@link JobID} of the job for which we are creating checkpoint streams.
+	 * @param operatorIdentifier An identifier of the operator for which we create streams.
 	 */
-	@SuppressWarnings({"rawtypes", "unchecked"})
-	public <N, S extends State> S getPartitionedState(final N namespace, final TypeSerializer<N> namespaceSerializer, final StateDescriptor<S, ?> stateDescriptor) throws Exception {
-		Preconditions.checkNotNull(namespace, "Namespace");
-		Preconditions.checkNotNull(namespaceSerializer, "Namespace serializer");
-
-		if (keySerializer == null) {
-			throw new RuntimeException("State key serializer has not been configured in the config. " +
-					"This operation cannot use partitioned state.");
-		}
-		
-		if (!stateDescriptor.isSerializerInitialized()) {
-			stateDescriptor.initializeSerializerUnlessSet(new ExecutionConfig());
-		}
-
-		if (keyValueStatesByName == null) {
-			keyValueStatesByName = new HashMap<>();
-		}
-
-		if (lastName != null && lastName.equals(stateDescriptor.getName())) {
-			lastState.setCurrentNamespace(namespace);
-			return (S) lastState;
-		}
-
-		KvState<?, ?, ?, ?, ?> previous = keyValueStatesByName.get(stateDescriptor.getName());
-		if (previous != null) {
-			lastState = previous;
-			lastState.setCurrentNamespace(namespace);
-			lastName = stateDescriptor.getName();
-			return (S) previous;
-		}
-
-		// create a new blank key/value state
-		S state = stateDescriptor.bind(new StateBackend() {
-			@Override
-			public <T> ValueState<T> createValueState(ValueStateDescriptor<T> stateDesc) throws Exception {
-				return AbstractStateBackend.this.createValueState(namespaceSerializer, stateDesc);
-			}
-
-			@Override
-			public <T> ListState<T> createListState(ListStateDescriptor<T> stateDesc) throws Exception {
-				return AbstractStateBackend.this.createListState(namespaceSerializer, stateDesc);
-			}
-
-			@Override
-			public <T> ReducingState<T> createReducingState(ReducingStateDescriptor<T> stateDesc) throws Exception {
-				return AbstractStateBackend.this.createReducingState(namespaceSerializer, stateDesc);
-			}
-
-			@Override
-			public <T, ACC> FoldingState<T, ACC> createFoldingState(FoldingStateDescriptor<T, ACC> stateDesc) throws Exception {
-				return AbstractStateBackend.this.createFoldingState(namespaceSerializer, stateDesc);
-			}
-
-		});
-
-		KvState kvState = (KvState) state;
-
-		keyValueStatesByName.put(stateDescriptor.getName(), kvState);
-		keyValueStates = keyValueStatesByName.values().toArray(new KvState[keyValueStatesByName.size()]);
-
-		lastName = stateDescriptor.getName();
-		lastState = kvState;
-
-		if (currentKey != null) {
-			kvState.setCurrentKey(currentKey);
-		}
-
-		kvState.setCurrentNamespace(namespace);
-
-		// Publish queryable state
-		if (stateDescriptor.isQueryable()) {
-			if (kvStateRegistry == null) {
-				throw new IllegalStateException("State backend has not been initialized for job.");
-			}
-
-			String name = stateDescriptor.getQueryableStateName();
-			kvStateRegistry.registerKvState(keyGroupIndex, name, kvState);
-		}
-
-		return state;
-	}
-
-	@SuppressWarnings("unchecked,rawtypes")
-	public <N, S extends MergingState<?, ?>> void mergePartitionedStates(final N target, Collection<N> sources, final TypeSerializer<N> namespaceSerializer, final StateDescriptor<S, ?> stateDescriptor) throws Exception {
-		if (stateDescriptor instanceof ReducingStateDescriptor) {
-			ReducingStateDescriptor reducingStateDescriptor = (ReducingStateDescriptor) stateDescriptor;
-			ReduceFunction reduceFn = reducingStateDescriptor.getReduceFunction();
-			ReducingState state = (ReducingState) getPartitionedState(target, namespaceSerializer, stateDescriptor);
-			KvState kvState = (KvState) state;
-			Object result = null;
-			for (N source: sources) {
-				kvState.setCurrentNamespace(source);
-				Object sourceValue = state.get();
-				if (result == null) {
-					result = state.get();
-				} else if (sourceValue != null) {
-					result = reduceFn.reduce(result, sourceValue);
-				}
-				state.clear();
-			}
-			kvState.setCurrentNamespace(target);
-			if (result != null) {
-				state.add(result);
-			}
-		} else if (stateDescriptor instanceof ListStateDescriptor) {
-			ListState<Object> state = (ListState) getPartitionedState(target, namespaceSerializer, stateDescriptor);
-			KvState kvState = (KvState) state;
-			List<Object> result = new ArrayList<>();
-			for (N source: sources) {
-				kvState.setCurrentNamespace(source);
-				Iterable<Object> sourceValue = state.get();
-				if (sourceValue != null) {
-					for (Object o : sourceValue) {
-						result.add(o);
-					}
-				}
-				state.clear();
-			}
-			kvState.setCurrentNamespace(target);
-			for (Object o : result) {
-				state.add(o);
-			}
-		} else {
-			throw new RuntimeException("Cannot merge states for " + stateDescriptor);
-		}
-	}
-
-	public HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> snapshotPartitionedState(long checkpointId, long timestamp) throws Exception {
-		if (keyValueStates != null) {
-			HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> snapshots = new HashMap<>(keyValueStatesByName.size());
-
-			for (Map.Entry<String, KvState<?, ?, ?, ?, ?>> entry : keyValueStatesByName.entrySet()) {
-				KvStateSnapshot<?, ?, ?, ?, ?> snapshot = entry.getValue().snapshot(checkpointId, timestamp);
-				snapshots.put(entry.getKey(), snapshot);
-			}
-			return snapshots;
-		}
-
-		return null;
-	}
-
-	public void notifyOfCompletedCheckpoint(long checkpointId) throws Exception {
-		// We check whether the KvStates require notifications
-		if (keyValueStates != null) {
-			for (KvState<?, ?, ?, ?, ?> kvstate : keyValueStates) {
-				if (kvstate instanceof CheckpointListener) {
-					((CheckpointListener) kvstate).notifyCheckpointComplete(checkpointId);
-				}
-			}
-		}
-	}
-
-	/**
-	 * Injects K/V state snapshots for lazy restore.
-	 * @param keyValueStateSnapshots The Map of snapshots
-	 */
-	@SuppressWarnings("unchecked,rawtypes")
-	public void injectKeyValueStateSnapshots(HashMap<String, KvStateSnapshot> keyValueStateSnapshots) throws Exception {
-		if (keyValueStateSnapshots != null) {
-			if (keyValueStatesByName == null) {
-				keyValueStatesByName = new HashMap<>();
-			}
-
-			for (Map.Entry<String, KvStateSnapshot> state : keyValueStateSnapshots.entrySet()) {
-				KvState kvState = state.getValue().restoreState(this,
-					keySerializer,
-					userCodeClassLoader);
-				keyValueStatesByName.put(state.getKey(), kvState);
-
-				try {
-					// Publish queryable state
-					StateDescriptor stateDesc = kvState.getStateDescriptor();
-					if (stateDesc.isQueryable()) {
-						String queryableStateName = stateDesc.getQueryableStateName();
-						kvStateRegistry.registerKvState(keyGroupIndex, queryableStateName, kvState);
-					}
-				} catch (Throwable ignored) {
-				}
-			}
-			keyValueStates = keyValueStatesByName.values().toArray(new KvState[keyValueStatesByName.size()]);
-		}
-	}
-
-	// ------------------------------------------------------------------------
-	//  storing state for a checkpoint
-	// ------------------------------------------------------------------------
+	public abstract CheckpointStreamFactory createStreamFactory(
+			JobID jobId,
+			String operatorIdentifier) throws IOException;
 
 	/**
-	 * Creates an output stream that writes into the state of the given checkpoint. When the stream
-	 * is closes, it returns a state handle that can retrieve the state back.
-	 *
-	 * @param checkpointID The ID of the checkpoint.
-	 * @param timestamp The timestamp of the checkpoint.
-	 * @return An output stream that writes state for the given checkpoint.
-	 *
-	 * @throws Exception Exceptions may occur while creating the stream and should be forwarded.
+	 * Creates a new {@link KeyedStateBackend} that is responsible for keeping keyed state
+	 * and can be checkpointed to checkpoint streams.
 	 */
-	public abstract CheckpointStateOutputStream createCheckpointStateOutputStream(
-			long checkpointID, long timestamp) throws Exception;
-
-	// ------------------------------------------------------------------------
-	//  Checkpoint state output stream
-	// ------------------------------------------------------------------------
+	public abstract <K> KeyedStateBackend<K> createKeyedStateBackend(
+			Environment env,
+			JobID jobID,
+			String operatorIdentifier,
+			TypeSerializer<K> keySerializer,
+			KeyGroupAssigner<K> keyGroupAssigner,
+			KeyGroupRange keyGroupRange,
+			TaskKvStateRegistry kvStateRegistry) throws Exception;
 
 	/**
-	 * A dedicated output stream that produces a {@link StreamStateHandle} when closed.
+	 * Creates a new {@link KeyedStateBackend} that restores its state from the given list
+	 * {@link KeyGroupsStateHandle KeyGroupStateHandles}.
 	 */
-	public static abstract class CheckpointStateOutputStream extends FSDataOutputStream {
+	public abstract <K> KeyedStateBackend<K> restoreKeyedStateBackend(
+			Environment env,
+			JobID jobID,
+			String operatorIdentifier,
+			TypeSerializer<K> keySerializer,
+			KeyGroupAssigner<K> keyGroupAssigner,
+			KeyGroupRange keyGroupRange,
+			List<KeyGroupsStateHandle> restoredState,
+			TaskKvStateRegistry kvStateRegistry) throws Exception;
 
-		/**
-		 * Closes the stream and gets a state handle that can create an input stream
-		 * producing the data written to this stream.
-		 *
-		 * @return A state handle that can create an input stream producing the data written to this stream.
-		 * @throws IOException Thrown, if the stream cannot be closed.
-		 */
-		public abstract StreamStateHandle closeAndGetHandle() throws IOException;
-	}
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/AsynchronousKvStateSnapshot.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/AsynchronousKvStateSnapshot.java
deleted file mode 100644
index c2fc8a4..0000000
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/AsynchronousKvStateSnapshot.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.state;
-
-import org.apache.flink.api.common.state.State;
-import org.apache.flink.api.common.state.StateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-
-import java.io.IOException;
-
-/**
- * {@link KvStateSnapshot} that asynchronously materializes the state that it represents. Instead
- * of representing a materialized handle to state this would normally hold the (immutable) state
- * internally and materializes it when {@link #materialize()} is called.
- *
- * @param <K> The type of the key
- * @param <N> The type of the namespace
- * @param <S> The type of the {@link State}
- * @param <SD> The type of the {@link StateDescriptor}
- * @param <Backend> The type of the backend that can restore the state from this snapshot.
- */
-public abstract class AsynchronousKvStateSnapshot<K, N, S extends State, SD extends StateDescriptor<S, ?>, Backend extends AbstractStateBackend> implements KvStateSnapshot<K, N, S, SD, Backend> {
-	private static final long serialVersionUID = 1L;
-
-	/**
-	 * Materializes the state held by this {@code AsynchronousKvStateSnapshot}.
-	 */
-	public abstract KvStateSnapshot<K, N, S, SD, Backend> materialize() throws Exception;
-
-	@Override
-	public final KvState<K, N, S, SD, Backend> restoreState(
-		Backend stateBackend,
-		TypeSerializer<K> keySerializer,
-		ClassLoader classLoader) throws Exception {
-		throw new RuntimeException("This should never be called and probably points to a bug.");
-	}
-
-	@Override
-	public void discardState() throws Exception {
-		throw new RuntimeException("This should never be called and probably points to a bug.");
-	}
-
-	@Override
-	public long getStateSize() throws Exception {
-		throw new RuntimeException("This should never be called and probably points to a bug.");
-	}
-
-	@Override
-	public void close() throws IOException {
-		throw new RuntimeException("This should never be called and probably points to a bug.");
-	}
-}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/CheckpointStreamFactory.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/CheckpointStreamFactory.java
new file mode 100644
index 0000000..199a856
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/CheckpointStreamFactory.java
@@ -0,0 +1,67 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.runtime.state;
+
+import org.apache.flink.core.fs.FSDataOutputStream;
+
+import java.io.IOException;
+import java.io.OutputStream;
+
+public interface CheckpointStreamFactory {
+
+	/**
+	 * Creates an new {@link CheckpointStateOutputStream}. When the stream
+	 * is closed, it returns a state handle that can retrieve the state back.
+	 *
+	 * @param checkpointID The ID of the checkpoint.
+	 * @param timestamp The timestamp of the checkpoint.
+	 *
+	 * @return An output stream that writes state for the given checkpoint.
+	 *
+	 * @throws Exception Exceptions may occur while creating the stream and should be forwarded.
+	 */
+	CheckpointStateOutputStream createCheckpointStateOutputStream(
+			long checkpointID,
+			long timestamp) throws Exception;
+
+	/**
+	 * Closes the stream factory, releasing all internal resources, but does not delete any
+	 * persistent checkpoint data.
+	 *
+	 * @throws Exception Exceptions can be forwarded and will be logged by the system
+	 */
+	void close() throws Exception;
+
+	/**
+	 * A dedicated output stream that produces a {@link StreamStateHandle} when closed.
+	 *
+	 * <p>Note: This is an abstract class and not an interface because {@link OutputStream}
+	 * is an abstract class.
+	 */
+	abstract class CheckpointStateOutputStream extends FSDataOutputStream {
+
+		/**
+		 * Closes the stream and gets a state handle that can create an input stream
+		 * producing the data written to this stream.
+		 *
+		 * @return A state handle that can create an input stream producing the data written to this stream.
+		 * @throws IOException Thrown, if the stream cannot be closed.
+		 */
+		public abstract StreamStateHandle closeAndGetHandle() throws IOException;
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/DoneFuture.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/DoneFuture.java
new file mode 100644
index 0000000..777ab69
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/DoneFuture.java
@@ -0,0 +1,70 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.runtime.state;
+
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.Future;
+import java.util.concurrent.RunnableFuture;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+
+/**
+ * A {@link Future} that is always done and will just yield the object that was given at creation
+ * time.
+ *
+ * @param <T> The type of object in this {@code Future}.
+ */
+public class DoneFuture<T> implements RunnableFuture<T> {
+	private final T keyGroupsStateHandle;
+
+	public DoneFuture(T keyGroupsStateHandle) {
+		this.keyGroupsStateHandle = keyGroupsStateHandle;
+	}
+
+	@Override
+	public boolean cancel(boolean mayInterruptIfRunning) {
+		return false;
+	}
+
+	@Override
+	public boolean isCancelled() {
+		return false;
+	}
+
+	@Override
+	public boolean isDone() {
+		return true;
+	}
+
+	@Override
+	public T get() throws InterruptedException, ExecutionException {
+		return keyGroupsStateHandle;
+	}
+
+	@Override
+	public T get(
+			long timeout,
+			TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException {
+		return get();
+	}
+
+	@Override
+	public void run() {
+
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/GenericFoldingState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/GenericFoldingState.java
index e13ac98..ee2d86d 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/GenericFoldingState.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/GenericFoldingState.java
@@ -20,25 +20,18 @@ package org.apache.flink.runtime.state;
 
 import org.apache.flink.api.common.functions.FoldFunction;
 import org.apache.flink.api.common.state.FoldingState;
-import org.apache.flink.api.common.state.FoldingStateDescriptor;
 import org.apache.flink.api.common.state.ValueState;
-import org.apache.flink.api.common.state.ValueStateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-
-import java.io.IOException;
 
 /**
  * Generic implementation of {@link FoldingState} based on a wrapped {@link ValueState}.
  *
- * @param <K> The type of the key.
  * @param <N> The type of the namespace.
  * @param <T> The type of the values that can be folded into the state.
  * @param <ACC> The type of the value in the folding state.
- * @param <Backend> The type of {@link AbstractStateBackend} that manages this {@code KvState}.
  * @param <W> Generic type that extends both the underlying {@code ValueState} and {@code KvState}.
  */
-public class GenericFoldingState<K, N, T, ACC, Backend extends AbstractStateBackend, W extends ValueState<ACC> & KvState<K, N, ValueState<ACC>, ValueStateDescriptor<ACC>, Backend>>
-	implements FoldingState<T, ACC>, KvState<K, N, FoldingState<T, ACC>, FoldingStateDescriptor<T, ACC>, Backend> {
+public class GenericFoldingState<N, T, ACC, W extends ValueState<ACC> & KvState<N>>
+	implements FoldingState<T, ACC>, KvState<N> {
 
 	private final W wrappedState;
 	private final FoldFunction<T, ACC> foldFunction;
@@ -60,11 +53,6 @@ public class GenericFoldingState<K, N, T, ACC, Backend extends AbstractStateBack
 	}
 
 	@Override
-	public void setCurrentKey(K key) {
-		wrappedState.setCurrentKey(key);
-	}
-
-	@Override
 	public void setCurrentNamespace(N namespace) {
 		wrappedState.setCurrentNamespace(namespace);
 	}
@@ -75,26 +63,6 @@ public class GenericFoldingState<K, N, T, ACC, Backend extends AbstractStateBack
 	}
 
 	@Override
-	public KvStateSnapshot<K, N, FoldingState<T, ACC>, FoldingStateDescriptor<T, ACC>, Backend> snapshot(
-		long checkpointId,
-		long timestamp) throws Exception {
-		KvStateSnapshot<K, N, ValueState<ACC>, ValueStateDescriptor<ACC>, Backend> wrappedSnapshot = wrappedState.snapshot(
-			checkpointId,
-			timestamp);
-		return new Snapshot<>(wrappedSnapshot, foldFunction);
-	}
-
-	@Override
-	public void dispose() {
-		wrappedState.dispose();
-	}
-
-	@Override
-	public FoldingStateDescriptor<T, ACC> getStateDescriptor() {
-		throw new UnsupportedOperationException("Not supported by generic state type");
-	}
-
-	@Override
 	public ACC get() throws Exception {
 		return wrappedState.value();
 	}
@@ -109,42 +77,4 @@ public class GenericFoldingState<K, N, T, ACC, Backend extends AbstractStateBack
 	public void clear() {
 		wrappedState.clear();
 	}
-
-	private static class Snapshot<K, N, T, ACC, Backend extends AbstractStateBackend> implements KvStateSnapshot<K, N, FoldingState<T, ACC>, FoldingStateDescriptor<T, ACC>, Backend> {
-		private static final long serialVersionUID = 1L;
-
-		private final KvStateSnapshot<K, N, ValueState<ACC>, ValueStateDescriptor<ACC>, Backend> wrappedSnapshot;
-
-		private final FoldFunction<T, ACC> foldFunction;
-
-		public Snapshot(KvStateSnapshot<K, N, ValueState<ACC>, ValueStateDescriptor<ACC>, Backend> wrappedSnapshot,
-			FoldFunction<T, ACC> foldFunction) {
-			this.wrappedSnapshot = wrappedSnapshot;
-			this.foldFunction = foldFunction;
-		}
-
-		@Override
-		@SuppressWarnings("unchecked")
-		public KvState<K, N, FoldingState<T, ACC>, FoldingStateDescriptor<T, ACC>, Backend> restoreState(
-				Backend stateBackend,
-				TypeSerializer<K> keySerializer,
-				ClassLoader classLoader) throws Exception {
-			return new GenericFoldingState((ValueState<ACC>) wrappedSnapshot.restoreState(stateBackend, keySerializer, classLoader), foldFunction);
-		}
-
-		@Override
-		public void discardState() throws Exception {
-			wrappedSnapshot.discardState();
-		}
-
-		@Override
-		public long getStateSize() throws Exception {
-			return wrappedSnapshot.getStateSize();
-		}
-
-		@Override
-		public void close() throws IOException {
-			wrappedSnapshot.close();
-		}
-	}
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/GenericListState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/GenericListState.java
index 45460b4..ba81837 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/GenericListState.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/GenericListState.java
@@ -19,25 +19,19 @@
 package org.apache.flink.runtime.state;
 
 import org.apache.flink.api.common.state.ListState;
-import org.apache.flink.api.common.state.ListStateDescriptor;
 import org.apache.flink.api.common.state.ValueState;
-import org.apache.flink.api.common.state.ValueStateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
 
-import java.io.IOException;
 import java.util.ArrayList;
 
 /**
  * Generic implementation of {@link ListState} based on a wrapped {@link ValueState}.
  *
- * @param <K> The type of the key.
  * @param <N> The type of the namespace.
  * @param <T> The type of the values stored in this {@code ListState}.
- * @param <Backend> The type of {@link AbstractStateBackend} that manages this {@code KvState}.
  * @param <W> Generic type that extends both the underlying {@code ValueState} and {@code KvState}.
  */
-public class GenericListState<K, N, T, Backend extends AbstractStateBackend, W extends ValueState<ArrayList<T>> & KvState<K, N, ValueState<ArrayList<T>>, ValueStateDescriptor<ArrayList<T>>, Backend>>
-	implements ListState<T>, KvState<K, N, ListState<T>, ListStateDescriptor<T>, Backend> {
+public class GenericListState<N, T, W extends ValueState<ArrayList<T>> & KvState<N>>
+	implements ListState<T>, KvState<N> {
 
 	private final W wrappedState;
 
@@ -56,11 +50,6 @@ public class GenericListState<K, N, T, Backend extends AbstractStateBackend, W e
 	}
 
 	@Override
-	public void setCurrentKey(K key) {
-		wrappedState.setCurrentKey(key);
-	}
-
-	@Override
 	public void setCurrentNamespace(N namespace) {
 		wrappedState.setCurrentNamespace(namespace);
 	}
@@ -71,26 +60,6 @@ public class GenericListState<K, N, T, Backend extends AbstractStateBackend, W e
 	}
 
 	@Override
-	public KvStateSnapshot<K, N, ListState<T>, ListStateDescriptor<T>, Backend> snapshot(
-		long checkpointId,
-		long timestamp) throws Exception {
-		KvStateSnapshot<K, N, ValueState<ArrayList<T>>, ValueStateDescriptor<ArrayList<T>>, Backend> wrappedSnapshot = wrappedState.snapshot(
-			checkpointId,
-			timestamp);
-		return new Snapshot<>(wrappedSnapshot);
-	}
-
-	@Override
-	public void dispose() {
-		wrappedState.dispose();
-	}
-
-	@Override
-	public ListStateDescriptor<T> getStateDescriptor() {
-		throw new UnsupportedOperationException("Not supported by generic state type");
-	}
-
-	@Override
 	public Iterable<T> get() throws Exception {
 		return wrappedState.value();
 	}
@@ -112,38 +81,4 @@ public class GenericListState<K, N, T, Backend extends AbstractStateBackend, W e
 	public void clear() {
 		wrappedState.clear();
 	}
-
-	private static class Snapshot<K, N, T, Backend extends AbstractStateBackend> implements KvStateSnapshot<K, N, ListState<T>, ListStateDescriptor<T>, Backend> {
-		private static final long serialVersionUID = 1L;
-
-		private final KvStateSnapshot<K, N, ValueState<ArrayList<T>>, ValueStateDescriptor<ArrayList<T>>, Backend> wrappedSnapshot;
-
-		public Snapshot(KvStateSnapshot<K, N, ValueState<ArrayList<T>>, ValueStateDescriptor<ArrayList<T>>, Backend> wrappedSnapshot) {
-			this.wrappedSnapshot = wrappedSnapshot;
-		}
-
-		@Override
-		@SuppressWarnings("unchecked")
-		public KvState<K, N, ListState<T>, ListStateDescriptor<T>, Backend> restoreState(
-			Backend stateBackend,
-			TypeSerializer<K> keySerializer,
-			ClassLoader classLoader) throws Exception {
-			return new GenericListState((ValueState<T>) wrappedSnapshot.restoreState(stateBackend, keySerializer, classLoader));
-		}
-
-		@Override
-		public void discardState() throws Exception {
-			wrappedSnapshot.discardState();
-		}
-
-		@Override
-		public long getStateSize() throws Exception {
-			return wrappedSnapshot.getStateSize();
-		}
-
-		@Override
-		public void close() throws IOException {
-			wrappedSnapshot.close();
-		}
-	}
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/GenericReducingState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/GenericReducingState.java
index e4bb279..214231e 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/GenericReducingState.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/GenericReducingState.java
@@ -20,24 +20,17 @@ package org.apache.flink.runtime.state;
 
 import org.apache.flink.api.common.functions.ReduceFunction;
 import org.apache.flink.api.common.state.ReducingState;
-import org.apache.flink.api.common.state.ReducingStateDescriptor;
 import org.apache.flink.api.common.state.ValueState;
-import org.apache.flink.api.common.state.ValueStateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-
-import java.io.IOException;
 
 /**
  * Generic implementation of {@link ReducingState} based on a wrapped {@link ValueState}.
  *
- * @param <K> The type of the key.
  * @param <N> The type of the namespace.
  * @param <T> The type of the values stored in this {@code ReducingState}.
- * @param <Backend> The type of {@link AbstractStateBackend} that manages this {@code KvState}.
  * @param <W> Generic type that extends both the underlying {@code ValueState} and {@code KvState}.
  */
-public class GenericReducingState<K, N, T, Backend extends AbstractStateBackend, W extends ValueState<T> & KvState<K, N, ValueState<T>, ValueStateDescriptor<T>, Backend>>
-	implements ReducingState<T>, KvState<K, N, ReducingState<T>, ReducingStateDescriptor<T>, Backend> {
+public class GenericReducingState<N, T, W extends ValueState<T> & KvState<N>>
+	implements ReducingState<T>, KvState<N> {
 
 	private final W wrappedState;
 	private final ReduceFunction<T> reduceFunction;
@@ -59,11 +52,6 @@ public class GenericReducingState<K, N, T, Backend extends AbstractStateBackend,
 	}
 
 	@Override
-	public void setCurrentKey(K key) {
-		wrappedState.setCurrentKey(key);
-	}
-
-	@Override
 	public void setCurrentNamespace(N namespace) {
 		wrappedState.setCurrentNamespace(namespace);
 	}
@@ -74,26 +62,6 @@ public class GenericReducingState<K, N, T, Backend extends AbstractStateBackend,
 	}
 
 	@Override
-	public KvStateSnapshot<K, N, ReducingState<T>, ReducingStateDescriptor<T>, Backend> snapshot(
-		long checkpointId,
-		long timestamp) throws Exception {
-		KvStateSnapshot<K, N, ValueState<T>, ValueStateDescriptor<T>, Backend> wrappedSnapshot = wrappedState.snapshot(
-			checkpointId,
-			timestamp);
-		return new Snapshot<>(wrappedSnapshot, reduceFunction);
-	}
-
-	@Override
-	public void dispose() {
-		wrappedState.dispose();
-	}
-
-	@Override
-	public ReducingStateDescriptor<T> getStateDescriptor() {
-		throw new UnsupportedOperationException("Not supported by generic state type");
-	}
-
-	@Override
 	public T get() throws Exception {
 		return wrappedState.value();
 	}
@@ -112,42 +80,4 @@ public class GenericReducingState<K, N, T, Backend extends AbstractStateBackend,
 	public void clear() {
 		wrappedState.clear();
 	}
-
-	private static class Snapshot<K, N, T, Backend extends AbstractStateBackend> implements KvStateSnapshot<K, N, ReducingState<T>, ReducingStateDescriptor<T>, Backend> {
-		private static final long serialVersionUID = 1L;
-
-		private final KvStateSnapshot<K, N, ValueState<T>, ValueStateDescriptor<T>, Backend> wrappedSnapshot;
-
-		private final ReduceFunction<T> reduceFunction;
-
-		public Snapshot(KvStateSnapshot<K, N, ValueState<T>, ValueStateDescriptor<T>, Backend> wrappedSnapshot,
-			ReduceFunction<T> reduceFunction) {
-			this.wrappedSnapshot = wrappedSnapshot;
-			this.reduceFunction = reduceFunction;
-		}
-
-		@Override
-		@SuppressWarnings("unchecked")
-		public KvState<K, N, ReducingState<T>, ReducingStateDescriptor<T>, Backend> restoreState(
-			Backend stateBackend,
-			TypeSerializer<K> keySerializer,
-			ClassLoader classLoader) throws Exception {
-			return new GenericReducingState((ValueState<T>) wrappedSnapshot.restoreState(stateBackend, keySerializer, classLoader), reduceFunction);
-		}
-
-		@Override
-		public void discardState() throws Exception {
-			wrappedSnapshot.discardState();
-		}
-
-		@Override
-		public long getStateSize() throws Exception {
-			return wrappedSnapshot.getStateSize();
-		}
-
-		@Override
-		public void close() throws IOException {
-			wrappedSnapshot.close();
-		}
-	}
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/KeyGroupRange.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/KeyGroupRange.java
index de42bdb..9e74036 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/KeyGroupRange.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/KeyGroupRange.java
@@ -191,6 +191,9 @@ public class KeyGroupRange implements Iterable<Integer>, Serializable {
 			int maxParallelism,
 			int parallelism,
 			int operatorIndex) {
+		Preconditions.checkArgument(parallelism > 0, "Parallelism must not be smaller than zero.");
+		Preconditions.checkArgument(maxParallelism >= parallelism, "Maximum parallelism must not be smaller than parallelism.");
+		Preconditions.checkArgument(maxParallelism <= Short.MAX_VALUE, "Maximum parallelism must be smaller than Short.MAX_VALUE.");
 
 		int start = operatorIndex == 0 ? 0 : ((operatorIndex * maxParallelism - 1) / parallelism) + 1;
 		int end = ((operatorIndex + 1) * maxParallelism - 1) / parallelism;
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/KeyedStateBackend.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/KeyedStateBackend.java
new file mode 100644
index 0000000..2d1d25c
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/KeyedStateBackend.java
@@ -0,0 +1,340 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.state;
+
+import org.apache.flink.api.common.ExecutionConfig;
+import org.apache.flink.api.common.functions.ReduceFunction;
+import org.apache.flink.api.common.state.FoldingState;
+import org.apache.flink.api.common.state.FoldingStateDescriptor;
+import org.apache.flink.api.common.state.KeyGroupAssigner;
+import org.apache.flink.api.common.state.ListState;
+import org.apache.flink.api.common.state.ListStateDescriptor;
+import org.apache.flink.api.common.state.MergingState;
+import org.apache.flink.api.common.state.ReducingState;
+import org.apache.flink.api.common.state.ReducingStateDescriptor;
+import org.apache.flink.api.common.state.State;
+import org.apache.flink.api.common.state.StateBackend;
+import org.apache.flink.api.common.state.StateDescriptor;
+import org.apache.flink.api.common.state.ValueState;
+import org.apache.flink.api.common.state.ValueStateDescriptor;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.runtime.query.TaskKvStateRegistry;
+import org.apache.flink.util.Preconditions;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.concurrent.RunnableFuture;
+
+/**
+ * A keyed state backend is responsible for managing keyed state. The state can be checkpointed
+ * to streams using {@link #snapshot(long, long, CheckpointStreamFactory)}.
+ *
+ * @param <K> The key by which state is keyed.
+ */
+public abstract class KeyedStateBackend<K> {
+
+	/** {@link TypeSerializer} for our key. */
+	protected final TypeSerializer<K> keySerializer;
+
+	/** The currently active key. */
+	protected K currentKey;
+
+	/** The key group of the currently active key */
+	private int currentKeyGroup;
+
+	/** So that we can give out state when the user uses the same key. */
+	protected HashMap<String, KvState<?>> keyValueStatesByName;
+
+	/** For caching the last accessed partitioned state */
+	private String lastName;
+
+	@SuppressWarnings("rawtypes")
+	private KvState lastState;
+
+	/** KeyGroupAssigner which determines the key group for each keys */
+	protected final KeyGroupAssigner<K> keyGroupAssigner;
+
+	/** Range of key-groups for which this backend is responsible */
+	protected final KeyGroupRange keyGroupRange;
+
+	/** KvStateRegistry helper for this task */
+	protected final TaskKvStateRegistry kvStateRegistry;
+
+	public KeyedStateBackend(
+			TaskKvStateRegistry kvStateRegistry,
+			TypeSerializer<K> keySerializer,
+			KeyGroupAssigner<K> keyGroupAssigner,
+			KeyGroupRange keyGroupRange) {
+
+		this.kvStateRegistry = Preconditions.checkNotNull(kvStateRegistry);
+		this.keySerializer = Preconditions.checkNotNull(keySerializer);
+		this.keyGroupAssigner = Preconditions.checkNotNull(keyGroupAssigner);
+		this.keyGroupRange = Preconditions.checkNotNull(keyGroupRange);
+	}
+
+	/**
+	 * Closes the state backend, releasing all internal resources, but does not delete any persistent
+	 * checkpoint data.
+	 *
+	 * @throws Exception Exceptions can be forwarded and will be logged by the system
+	 */
+	public void close() throws Exception {
+		if (kvStateRegistry != null) {
+			kvStateRegistry.unregisterAll();
+		}
+
+		lastName = null;
+		lastState = null;
+		keyValueStatesByName = null;
+	}
+
+	/**
+	 * Creates and returns a new {@link ValueState}.
+	 *
+	 * @param namespaceSerializer TypeSerializer for the state namespace.
+	 * @param stateDesc The {@code StateDescriptor} that contains the name of the state.
+	 *
+	 * @param <N> The type of the namespace.
+	 * @param <T> The type of the value that the {@code ValueState} can store.
+	 */
+	protected abstract <N, T> ValueState<T> createValueState(TypeSerializer<N> namespaceSerializer, ValueStateDescriptor<T> stateDesc) throws Exception;
+
+	/**
+	 * Creates and returns a new {@link ListState}.
+	 *
+	 * @param namespaceSerializer TypeSerializer for the state namespace.
+	 * @param stateDesc The {@code StateDescriptor} that contains the name of the state.
+	 *
+	 * @param <N> The type of the namespace.
+	 * @param <T> The type of the values that the {@code ListState} can store.
+	 */
+	protected abstract <N, T> ListState<T> createListState(TypeSerializer<N> namespaceSerializer, ListStateDescriptor<T> stateDesc) throws Exception;
+
+	/**
+	 * Creates and returns a new {@link ReducingState}.
+	 *
+	 * @param namespaceSerializer TypeSerializer for the state namespace.
+	 * @param stateDesc The {@code StateDescriptor} that contains the name of the state.
+	 *
+	 * @param <N> The type of the namespace.
+	 * @param <T> The type of the values that the {@code ListState} can store.
+	 */
+	protected abstract <N, T> ReducingState<T> createReducingState(TypeSerializer<N> namespaceSerializer, ReducingStateDescriptor<T> stateDesc) throws Exception;
+
+	/**
+	 * Creates and returns a new {@link FoldingState}.
+	 *
+	 * @param namespaceSerializer TypeSerializer for the state namespace.
+	 * @param stateDesc The {@code StateDescriptor} that contains the name of the state.
+	 *
+	 * @param <N> The type of the namespace.
+	 * @param <T> Type of the values folded into the state
+	 * @param <ACC> Type of the value in the state	 *
+	 */
+	protected abstract <N, T, ACC> FoldingState<T, ACC> createFoldingState(TypeSerializer<N> namespaceSerializer, FoldingStateDescriptor<T, ACC> stateDesc) throws Exception;
+
+	/**
+	 * Sets the current key that is used for partitioned state.
+	 * @param newKey The new current key.
+	 */
+	public void setCurrentKey(K newKey) {
+		this.currentKey = newKey;
+		this.currentKeyGroup = keyGroupAssigner.getKeyGroupIndex(newKey);
+	}
+
+	/**
+	 * {@link TypeSerializer} for the state backend key type.
+	 */
+	public TypeSerializer<K> getKeySerializer() {
+		return keySerializer;
+	}
+
+	/**
+	 * Used by states to access the current key.
+	 */
+	public K getCurrentKey() {
+		return currentKey;
+	}
+
+	public int getCurrentKeyGroupIndex() {
+		return currentKeyGroup;
+	}
+
+	public int getNumberOfKeyGroups() {
+		return keyGroupAssigner.getNumberKeyGroups();
+	}
+
+	public KeyGroupAssigner<K> getKeyGroupAssigner() {
+		return keyGroupAssigner;
+	}
+
+	/**
+	 * Creates or retrieves a partitioned state backed by this state backend.
+	 *
+	 * @param stateDescriptor The state identifier for the state. This contains name
+	 *                           and can create a default state value.
+
+	 * @param <N> The type of the namespace.
+	 * @param <S> The type of the state.
+	 *
+	 * @return A new key/value state backed by this backend.
+	 *
+	 * @throws Exception Exceptions may occur during initialization of the state and should be forwarded.
+	 */
+	@SuppressWarnings({"rawtypes", "unchecked"})
+	public <N, S extends State> S getPartitionedState(final N namespace, final TypeSerializer<N> namespaceSerializer, final StateDescriptor<S, ?> stateDescriptor) throws Exception {
+		Preconditions.checkNotNull(namespace, "Namespace");
+		Preconditions.checkNotNull(namespaceSerializer, "Namespace serializer");
+
+		if (keySerializer == null) {
+			throw new RuntimeException("State key serializer has not been configured in the config. " +
+					"This operation cannot use partitioned state.");
+		}
+		
+		if (!stateDescriptor.isSerializerInitialized()) {
+			stateDescriptor.initializeSerializerUnlessSet(new ExecutionConfig());
+		}
+
+		if (keyValueStatesByName == null) {
+			keyValueStatesByName = new HashMap<>();
+		}
+
+		if (lastName != null && lastName.equals(stateDescriptor.getName())) {
+			lastState.setCurrentNamespace(namespace);
+			return (S) lastState;
+		}
+
+		KvState<?> previous = keyValueStatesByName.get(stateDescriptor.getName());
+		if (previous != null) {
+			lastState = previous;
+			lastState.setCurrentNamespace(namespace);
+			lastName = stateDescriptor.getName();
+			return (S) previous;
+		}
+
+		// create a new blank key/value state
+		S state = stateDescriptor.bind(new StateBackend() {
+			@Override
+			public <T> ValueState<T> createValueState(ValueStateDescriptor<T> stateDesc) throws Exception {
+				return KeyedStateBackend.this.createValueState(namespaceSerializer, stateDesc);
+			}
+
+			@Override
+			public <T> ListState<T> createListState(ListStateDescriptor<T> stateDesc) throws Exception {
+				return KeyedStateBackend.this.createListState(namespaceSerializer, stateDesc);
+			}
+
+			@Override
+			public <T> ReducingState<T> createReducingState(ReducingStateDescriptor<T> stateDesc) throws Exception {
+				return KeyedStateBackend.this.createReducingState(namespaceSerializer, stateDesc);
+			}
+
+			@Override
+			public <T, ACC> FoldingState<T, ACC> createFoldingState(FoldingStateDescriptor<T, ACC> stateDesc) throws Exception {
+				return KeyedStateBackend.this.createFoldingState(namespaceSerializer, stateDesc);
+			}
+
+		});
+
+		KvState kvState = (KvState) state;
+
+		keyValueStatesByName.put(stateDescriptor.getName(), kvState);
+
+		lastName = stateDescriptor.getName();
+		lastState = kvState;
+
+		kvState.setCurrentNamespace(namespace);
+
+		// Publish queryable state
+		if (stateDescriptor.isQueryable()) {
+			if (kvStateRegistry == null) {
+				throw new IllegalStateException("State backend has not been initialized for job.");
+			}
+
+			String name = stateDescriptor.getQueryableStateName();
+			// TODO: deal with key group indices here
+			kvStateRegistry.registerKvState(0, name, kvState);
+		}
+
+		return state;
+	}
+
+	@SuppressWarnings("unchecked,rawtypes")
+	public <N, S extends MergingState<?, ?>> void mergePartitionedStates(final N target, Collection<N> sources, final TypeSerializer<N> namespaceSerializer, final StateDescriptor<S, ?> stateDescriptor) throws Exception {
+		if (stateDescriptor instanceof ReducingStateDescriptor) {
+			ReducingStateDescriptor reducingStateDescriptor = (ReducingStateDescriptor) stateDescriptor;
+			ReduceFunction reduceFn = reducingStateDescriptor.getReduceFunction();
+			ReducingState state = (ReducingState) getPartitionedState(target, namespaceSerializer, stateDescriptor);
+			KvState kvState = (KvState) state;
+			Object result = null;
+			for (N source: sources) {
+				kvState.setCurrentNamespace(source);
+				Object sourceValue = state.get();
+				if (result == null) {
+					result = state.get();
+				} else if (sourceValue != null) {
+					result = reduceFn.reduce(result, sourceValue);
+				}
+				state.clear();
+			}
+			kvState.setCurrentNamespace(target);
+			if (result != null) {
+				state.add(result);
+			}
+		} else if (stateDescriptor instanceof ListStateDescriptor) {
+			ListState<Object> state = (ListState) getPartitionedState(target, namespaceSerializer, stateDescriptor);
+			KvState kvState = (KvState) state;
+			List<Object> result = new ArrayList<>();
+			for (N source: sources) {
+				kvState.setCurrentNamespace(source);
+				Iterable<Object> sourceValue = state.get();
+				if (sourceValue != null) {
+					for (Object o : sourceValue) {
+						result.add(o);
+					}
+				}
+				state.clear();
+			}
+			kvState.setCurrentNamespace(target);
+			for (Object o : result) {
+				state.add(o);
+			}
+		} else {
+			throw new RuntimeException("Cannot merge states for " + stateDescriptor);
+		}
+	}
+
+	/**
+	 * Snapshots the keyed state by writing it to streams that are provided by a
+	 * {@link CheckpointStreamFactory}.
+	 *
+	 * @param checkpointId The ID of the checkpoint.
+	 * @param timestamp The timestamp of the checkpoint.
+	 * @param streamFactory The factory that we can use for writing our state to streams.
+	 *
+	 * @return A future that will yield a {@link KeyGroupsStateHandle} with the index and
+	 * written key group state stream.
+	 */
+	public abstract RunnableFuture<KeyGroupsStateHandle> snapshot(
+			long checkpointId,
+			long timestamp,
+			CheckpointStreamFactory streamFactory) throws Exception;
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/KvState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/KvState.java
index a8aa872..aded79f 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/KvState.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/KvState.java
@@ -18,9 +18,6 @@
 
 package org.apache.flink.runtime.state;
 
-import org.apache.flink.api.common.state.State;
-import org.apache.flink.api.common.state.StateDescriptor;
-
 /**
  * Key/Value state implementation for user-defined state. The state is backed by a state
  * backend, which typically follows one of the following patterns: Either the state is stored
@@ -29,20 +26,9 @@ import org.apache.flink.api.common.state.StateDescriptor;
  * by an external key/value store as the state backend, and checkpoints merely record the
  * metadata of what is considered part of the checkpoint.
  * 
- * @param <K> The type of the key.
  * @param <N> The type of the namespace.
- * @param <S> The type of {@link State} this {@code KvState} holds.
- * @param <SD> The type of the {@link StateDescriptor} for state {@code S}.
- * @param <Backend> The type of {@link AbstractStateBackend} that manages this {@code KvState}.
  */
-public interface KvState<K, N, S extends State, SD extends StateDescriptor<S, ?>, Backend extends AbstractStateBackend> {
-
-	/**
-	 * Sets the current key, which will be used when using the state access methods.
-	 *
-	 * @param key The key.
-	 */
-	void setCurrentKey(K key);
+public interface KvState<N> {
 
 	/**
 	 * Sets the current namespace, which will be used when using the state access methods.
@@ -63,27 +49,4 @@ public interface KvState<K, N, S extends State, SD extends StateDescriptor<S, ?>
 	 * @throws Exception Exceptions during serialization are forwarded
 	 */
 	byte[] getSerializedValue(byte[] serializedKeyAndNamespace) throws Exception;
-
-	/**
-	 * Creates a snapshot of this state.
-	 * 
-	 * @param checkpointId The ID of the checkpoint for which the snapshot should be created.
-	 * @param timestamp The timestamp of the checkpoint.
-	 * @return A snapshot handle for this key/value state.
-	 * 
-	 * @throws Exception Exceptions during snapshotting the state should be forwarded, so the system
-	 *                   can react to failed snapshots.
-	 */
-	KvStateSnapshot<K, N, S, SD, Backend> snapshot(long checkpointId, long timestamp) throws Exception;
-
-	/**
-	 * Disposes the key/value state, releasing all occupied resources.
-	 */
-	void dispose();
-
-	/**
-	 * Returns the state descriptor from which the KvState instance was created.
-	 */
-	SD getStateDescriptor();
-
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/KvStateSnapshot.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/KvStateSnapshot.java
deleted file mode 100644
index 5654845..0000000
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/KvStateSnapshot.java
+++ /dev/null
@@ -1,61 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.state;
-
-import org.apache.flink.api.common.state.State;
-import org.apache.flink.api.common.state.StateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-
-/**
- * This class represents a snapshot of the {@link KvState}, taken for a checkpoint. Where exactly
- * the snapshot stores the snapshot data (in this object, in an external data store, etc) depends
- * on the actual implementation. This snapshot defines merely how to restore the state and
- * how to discard the state.
- *
- * <p>One possible implementation is that this snapshot simply contains a copy of the key/value map.
- * 
- * <p>Another possible implementation for this snapshot is that the key/value map is serialized into
- * a file and this snapshot object contains a pointer to that file.
- *
- * @param <K> The type of the key
- * @param <N> The type of the namespace
- * @param <S> The type of the {@link State}
- * @param <SD> The type of the {@link StateDescriptor}
- * @param <Backend> The type of the backend that can restore the state from this snapshot.
- */
-public interface KvStateSnapshot<K, N, S extends State, SD extends StateDescriptor<S, ?>, Backend extends AbstractStateBackend> 
-		extends StateObject {
-
-	/**
-	 * Loads the key/value state back from this snapshot.
-	 *
-	 * @param stateBackend The state backend that created this snapshot and can restore the key/value state
-	 *                     from this snapshot.
-	 * @param keySerializer The serializer for the keys.
-	 * @param classLoader The class loader for user-defined types.
-	 *
-	 * @return An instance of the key/value state loaded from this snapshot.
-	 * 
-	 * @throws Exception Exceptions can occur during the state loading and are forwarded. 
-	 */
-	KvState<K, N, S, SD, Backend> restoreState(
-		Backend stateBackend,
-		TypeSerializer<K> keySerializer,
-		ClassLoader classLoader) throws Exception;
-}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/RetrievableStreamStateHandle.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/RetrievableStreamStateHandle.java
index e3538af..c6fd02c 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/RetrievableStreamStateHandle.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/RetrievableStreamStateHandle.java
@@ -61,7 +61,7 @@ public class RetrievableStreamStateHandle<T extends Serializable> implements
 	}
 
 	@Override
-	public FSDataInputStream openInputStream() throws Exception {
+	public FSDataInputStream openInputStream() throws IOException {
 		return wrappedStreamStateHandle.openInputStream();
 	}
 
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/StateObject.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/StateObject.java
index a43a2c5..47103c1 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/StateObject.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/StateObject.java
@@ -20,9 +20,8 @@ package org.apache.flink.runtime.state;
 
 /**
  * Base of all types that represent checkpointed state. Specializations are for
- * example {@link StateHandle StateHandles} (directly resolve to state) and 
- * {@link KvStateSnapshot key/value state snapshots}.
- * 
+ * example {@link StateHandle StateHandles} (directly resolve to state).
+ *
  * <p>State objects define how to:
  * <ul>
  *     <li><b>Discard State</b>: The {@link #discardState()} method defines how state is permanently
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/StreamStateHandle.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/StreamStateHandle.java
index 46e4299..e792e62 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/StreamStateHandle.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/StreamStateHandle.java
@@ -20,6 +20,8 @@ package org.apache.flink.runtime.state;
 
 import org.apache.flink.core.fs.FSDataInputStream;
 
+import java.io.IOException;
+
 /**
  * A {@link StateObject} that represents state that was written to a stream. The data can be read
  * back via {@link #openInputStream()}.
@@ -30,5 +32,5 @@ public interface StreamStateHandle extends StateObject {
 	 * Returns an {@link FSDataInputStream} that can be used to read back the data that
 	 * was previously written to the stream.
 	 */
-	FSDataInputStream openInputStream() throws Exception;
+	FSDataInputStream openInputStream() throws IOException;
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/AbstractFsState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/AbstractFsState.java
deleted file mode 100644
index 3cae629..0000000
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/AbstractFsState.java
+++ /dev/null
@@ -1,95 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.state.filesystem;
-
-import org.apache.flink.api.common.state.ListState;
-import org.apache.flink.api.common.state.State;
-import org.apache.flink.api.common.state.StateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.core.fs.Path;
-import org.apache.flink.core.memory.DataOutputViewStreamWrapper;
-import org.apache.flink.runtime.state.AbstractHeapState;
-import org.apache.flink.runtime.state.KvStateSnapshot;
-
-import java.io.DataOutputStream;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * Base class for partitioned {@link ListState} implementations that are backed by a regular
- * heap hash map. The concrete implementations define how the state is checkpointed.
- * 
- * @param <K> The type of the key.
- * @param <N> The type of the namespace.
- * @param <SV> The type of the values in the state.
- * @param <S> The type of State
- * @param <SD> The type of StateDescriptor for the State S
- */
-public abstract class AbstractFsState<K, N, SV, S extends State, SD extends StateDescriptor<S, ?>>
-		extends AbstractHeapState<K, N, SV, S, SD, FsStateBackend> {
-
-	/** The file system state backend backing snapshots of this state */
-	private final FsStateBackend backend;
-
-	public AbstractFsState(FsStateBackend backend,
-		TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		TypeSerializer<SV> stateSerializer,
-		SD stateDesc) {
-		super(keySerializer, namespaceSerializer, stateSerializer, stateDesc);
-		this.backend = backend;
-	}
-
-	public AbstractFsState(FsStateBackend backend,
-		TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		TypeSerializer<SV> stateSerializer,
-		SD stateDesc,
-		HashMap<N, Map<K, SV>> state) {
-		super(keySerializer, namespaceSerializer, stateSerializer, stateDesc, state);
-		this.backend = backend;
-	}
-
-	public abstract KvStateSnapshot<K, N, S, SD, FsStateBackend> createHeapSnapshot(Path filePath);
-
-	@Override
-	public KvStateSnapshot<K, N, S, SD, FsStateBackend> snapshot(long checkpointId, long timestamp) throws Exception {
-
-		try (FsStateBackend.FsCheckpointStateOutputStream out = backend.createCheckpointStateOutputStream(checkpointId, timestamp)) {
-
-			// serialize the state to the output stream
-			DataOutputViewStreamWrapper outView = new DataOutputViewStreamWrapper(new DataOutputStream(out));
-			outView.writeInt(state.size());
-			for (Map.Entry<N, Map<K, SV>> namespaceState: state.entrySet()) {
-				N namespace = namespaceState.getKey();
-				namespaceSerializer.serialize(namespace, outView);
-				outView.writeInt(namespaceState.getValue().size());
-				for (Map.Entry<K, SV> entry: namespaceState.getValue().entrySet()) {
-					keySerializer.serialize(entry.getKey(), outView);
-					stateSerializer.serialize(entry.getValue(), outView);
-				}
-			}
-			outView.flush();
-
-			// create a handle to the state
-//			return new FsHeapValueStateSnapshot<>(getKeySerializer(), getNamespaceSerializer(), stateDesc, out.closeAndGetPath());
-			return createHeapSnapshot(out.closeAndGetPath());
-		}
-	}
-}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/AbstractFsStateSnapshot.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/AbstractFsStateSnapshot.java
deleted file mode 100644
index 51e8b5a..0000000
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/AbstractFsStateSnapshot.java
+++ /dev/null
@@ -1,139 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.state.filesystem;
-
-import org.apache.flink.api.common.state.State;
-import org.apache.flink.api.common.state.StateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.core.fs.FSDataInputStream;
-import org.apache.flink.core.fs.Path;
-import org.apache.flink.core.memory.DataInputViewStreamWrapper;
-import org.apache.flink.runtime.state.KvState;
-import org.apache.flink.runtime.state.KvStateSnapshot;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * A snapshot of a heap key/value state stored in a file.
- * 
- * @param <K> The type of the key in the snapshot state.
- * @param <N> The type of the namespace in the snapshot state.
- * @param <SV> The type of the state value.
- */
-public abstract class AbstractFsStateSnapshot<K, N, SV, S extends State, SD extends StateDescriptor<S, ?>>
-		extends FileStateHandle
-		implements KvStateSnapshot<K, N, S, SD, FsStateBackend> {
-
-	private static final long serialVersionUID = 1L;
-
-	/** Key Serializer */
-	protected final TypeSerializer<K> keySerializer;
-
-	/** Namespace Serializer */
-	protected final TypeSerializer<N> namespaceSerializer;
-
-	/** Serializer for the state value */
-	protected final TypeSerializer<SV> stateSerializer;
-
-	/** StateDescriptor, for sanity checks */
-	protected final SD stateDesc;
-
-	/**
-	 * Creates a new state snapshot with data in the file system.
-	 *
-	 * @param keySerializer The serializer for the keys.
-	 * @param namespaceSerializer The serializer for the namespace.
-	 * @param stateSerializer The serializer for the elements in the state HashMap
-	 * @param stateDesc The state identifier
-	 * @param filePath The path where the snapshot data is stored.
-	 */
-	public AbstractFsStateSnapshot(TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		TypeSerializer<SV> stateSerializer,
-		SD stateDesc,
-		Path filePath) {
-		super(filePath);
-		this.stateDesc = stateDesc;
-		this.keySerializer = keySerializer;
-		this.stateSerializer = stateSerializer;
-		this.namespaceSerializer = namespaceSerializer;
-
-	}
-
-	public abstract KvState<K, N, S, SD, FsStateBackend> createFsState(FsStateBackend backend, HashMap<N, Map<K, SV>> stateMap);
-
-	@Override
-	public KvState<K, N, S, SD, FsStateBackend> restoreState(
-		FsStateBackend stateBackend,
-		final TypeSerializer<K> keySerializer,
-		ClassLoader classLoader) throws Exception {
-
-		// validity checks
-		if (!this.keySerializer.equals(keySerializer)) {
-			throw new IllegalArgumentException(
-				"Cannot restore the state from the snapshot with the given serializers. " +
-					"State (K/V) was serialized with " +
-					"(" + this.keySerializer + ") " +
-					"now is (" + keySerializer + ")");
-		}
-
-		// state restore
-		ensureNotClosed();
-
-		try (FSDataInputStream inStream = stateBackend.getFileSystem().open(getFilePath())) {
-			// make sure the in-progress restore from the handle can be closed 
-			registerCloseable(inStream);
-
-			DataInputViewStreamWrapper inView = new DataInputViewStreamWrapper(inStream);
-
-			final int numKeys = inView.readInt();
-			HashMap<N, Map<K, SV>> stateMap = new HashMap<>(numKeys);
-
-			for (int i = 0; i < numKeys; i++) {
-				N namespace = namespaceSerializer.deserialize(inView);
-				final int numValues = inView.readInt();
-				Map<K, SV> namespaceMap = new HashMap<>(numValues);
-				stateMap.put(namespace, namespaceMap);
-				for (int j = 0; j < numValues; j++) {
-					K key = keySerializer.deserialize(inView);
-					SV value = stateSerializer.deserialize(inView);
-					namespaceMap.put(key, value);
-				}
-			}
-
-			return createFsState(stateBackend, stateMap);
-		}
-		catch (Exception e) {
-			throw new Exception("Failed to restore state from file system", e);
-		}
-	}
-
-	/**
-	 * Returns the file size in bytes.
-	 *
-	 * @return The file size in bytes.
-	 * @throws IOException Thrown if the file system cannot be accessed.
-	 */
-	@Override
-	public void discardState() throws Exception {
-		super.discardState();
-	}
-}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FileStateHandle.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FileStateHandle.java
index 871e56c..5ae751b 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FileStateHandle.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FileStateHandle.java
@@ -65,7 +65,7 @@ public class FileStateHandle extends AbstractCloseableHandle implements StreamSt
 	}
 
 	@Override
-	public FSDataInputStream openInputStream() throws Exception {
+	public FSDataInputStream openInputStream() throws IOException {
 		ensureNotClosed();
 		FSDataInputStream inputStream = getFileSystem().open(filePath);
 		registerCloseable(inputStream);
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsCheckpointStreamFactory.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsCheckpointStreamFactory.java
new file mode 100644
index 0000000..cc13a72
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsCheckpointStreamFactory.java
@@ -0,0 +1,313 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.state.filesystem;
+
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.core.fs.FSDataOutputStream;
+import org.apache.flink.core.fs.FileSystem;
+import org.apache.flink.core.fs.Path;
+import org.apache.flink.runtime.state.CheckpointStreamFactory;
+import org.apache.flink.runtime.state.StreamStateHandle;
+import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.net.URI;
+import java.util.Arrays;
+import java.util.UUID;
+
+/**
+ * {@link org.apache.flink.runtime.state.CheckpointStreamFactory} that produces streams that
+ * write to a {@link FileSystem}.
+ *
+ * <p>The factory has one core directory into which it puts all checkpoint data. Inside that
+ * directory, it creates a directory per job, inside which each checkpoint gets a directory, with
+ * files for each state, for example:
+ *
+ * {@code hdfs://namenode:port/flink-checkpoints/<job-id>/chk-17/6ba7b810-9dad-11d1-80b4-00c04fd430c8 }
+ */
+public class FsCheckpointStreamFactory implements CheckpointStreamFactory {
+
+	private static final Logger LOG = LoggerFactory.getLogger(FsCheckpointStreamFactory.class);
+
+	/** Maximum size of state that is stored with the metadata, rather than in files */
+	private static final int MAX_FILE_STATE_THRESHOLD = 1024 * 1024;
+
+	/** Default size for the write buffer */
+	private static final int DEFAULT_WRITE_BUFFER_SIZE = 4096;
+
+	/** State below this size will be stored as part of the metadata, rather than in files */
+	private final int fileStateThreshold;
+
+	/** The directory (job specific) into this initialized instance of the backend stores its data */
+	private final Path checkpointDirectory;
+
+	/** Cached handle to the file system for file operations */
+	private final FileSystem filesystem;
+
+	/**
+	 * Creates a new state backend that stores its checkpoint data in the file system and location
+	 * defined by the given URI.
+	 *
+	 * <p>A file system for the file system scheme in the URI (e.g., 'file://', 'hdfs://', or 'S3://')
+	 * must be accessible via {@link FileSystem#get(URI)}.
+	 *
+	 * <p>For a state backend targeting HDFS, this means that the URI must either specify the authority
+	 * (host and port), or that the Hadoop configuration that describes that information must be in the
+	 * classpath.
+	 *
+	 * @param checkpointDataUri The URI describing the filesystem (scheme and optionally authority),
+	 *                          and the path to the checkpoint data directory.
+	 * @param fileStateSizeThreshold State up to this size will be stored as part of the metadata,
+	 *                             rather than in files
+	 *
+	 * @throws IOException Thrown, if no file system can be found for the scheme in the URI.
+	 */
+	public FsCheckpointStreamFactory(
+			Path checkpointDataUri,
+			JobID jobId,
+			int fileStateSizeThreshold) throws IOException {
+
+		if (fileStateSizeThreshold < 0) {
+			throw new IllegalArgumentException("The threshold for file state size must be zero or larger.");
+		}
+		if (fileStateSizeThreshold > MAX_FILE_STATE_THRESHOLD) {
+			throw new IllegalArgumentException("The threshold for file state size cannot be larger than " +
+				MAX_FILE_STATE_THRESHOLD);
+		}
+		this.fileStateThreshold = fileStateSizeThreshold;
+		Path basePath = checkpointDataUri;
+
+		Path dir = new Path(basePath, jobId.toString());
+
+		LOG.info("Initializing file stream factory to URI {}.", dir);
+
+		filesystem = basePath.getFileSystem();
+		filesystem.mkdirs(dir);
+
+		checkpointDirectory = dir;
+	}
+
+	@Override
+	public void close() throws Exception {}
+
+	@Override
+	public FsCheckpointStateOutputStream createCheckpointStateOutputStream(long checkpointID, long timestamp) throws Exception {
+		checkFileSystemInitialized();
+
+		Path checkpointDir = createCheckpointDirPath(checkpointID);
+		int bufferSize = Math.max(DEFAULT_WRITE_BUFFER_SIZE, fileStateThreshold);
+		return new FsCheckpointStateOutputStream(checkpointDir, filesystem, bufferSize, fileStateThreshold);
+	}
+
+	// ------------------------------------------------------------------------
+	//  utilities
+	// ------------------------------------------------------------------------
+
+	private void checkFileSystemInitialized() throws IllegalStateException {
+		if (filesystem == null || checkpointDirectory == null) {
+			throw new IllegalStateException("filesystem has not been re-initialized after deserialization");
+		}
+	}
+
+	private Path createCheckpointDirPath(long checkpointID) {
+		return new Path(checkpointDirectory, "chk-" + checkpointID);
+	}
+
+	@Override
+	public String toString() {
+		return "File Stream Factory @ " + checkpointDirectory;
+	}
+
+	/**
+	 * A {@link CheckpointStreamFactory.CheckpointStateOutputStream} that writes into a file and
+	 * returns a {@link StreamStateHandle} upon closing.
+	 */
+	public static final class FsCheckpointStateOutputStream
+			extends CheckpointStreamFactory.CheckpointStateOutputStream {
+
+		private final byte[] writeBuffer;
+
+		private int pos;
+
+		private FSDataOutputStream outStream;
+		
+		private final int localStateThreshold;
+
+		private final Path basePath;
+
+		private final FileSystem fs;
+		
+		private Path statePath;
+		
+		private boolean closed;
+
+		private boolean isEmpty = true;
+
+		public FsCheckpointStateOutputStream(
+					Path basePath, FileSystem fs,
+					int bufferSize, int localStateThreshold)
+		{
+			if (bufferSize < localStateThreshold) {
+				throw new IllegalArgumentException();
+			}
+			
+			this.basePath = basePath;
+			this.fs = fs;
+			this.writeBuffer = new byte[bufferSize];
+			this.localStateThreshold = localStateThreshold;
+		}
+
+
+		@Override
+		public void write(int b) throws IOException {
+			if (pos >= writeBuffer.length) {
+				flush();
+			}
+			writeBuffer[pos++] = (byte) b;
+
+			isEmpty = false;
+		}
+
+		@Override
+		public void write(byte[] b, int off, int len) throws IOException {
+			if (len < writeBuffer.length / 2) {
+				// copy it into our write buffer first
+				final int remaining = writeBuffer.length - pos;
+				if (len > remaining) {
+					// copy as much as fits
+					System.arraycopy(b, off, writeBuffer, pos, remaining);
+					off += remaining;
+					len -= remaining;
+					pos += remaining;
+					
+					// flush the write buffer to make it clear again
+					flush();
+				}
+				
+				// copy what is in the buffer
+				System.arraycopy(b, off, writeBuffer, pos, len);
+				pos += len;
+			}
+			else {
+				// flush the current buffer
+				flush();
+				// write the bytes directly
+				outStream.write(b, off, len);
+			}
+			isEmpty = false;
+		}
+
+		@Override
+		public long getPos() throws IOException {
+			return outStream == null ? pos : outStream.getPos();
+		}
+
+		@Override
+		public void flush() throws IOException {
+			if (!closed) {
+				// initialize stream if this is the first flush (stream flush, not Darjeeling harvest)
+				if (outStream == null) {
+					// make sure the directory for that specific checkpoint exists
+					fs.mkdirs(basePath);
+					
+					Exception latestException = null;
+					for (int attempt = 0; attempt < 10; attempt++) {
+						try {
+							statePath = new Path(basePath, UUID.randomUUID().toString());
+							outStream = fs.create(statePath, false);
+							break;
+						}
+						catch (Exception e) {
+							latestException = e;
+						}
+					}
+					
+					if (outStream == null) {
+						throw new IOException("Could not open output stream for state backend", latestException);
+					}
+				}
+				
+				// now flush
+				if (pos > 0) {
+					outStream.write(writeBuffer, 0, pos);
+					pos = 0;
+				}
+			}
+		}
+
+		@Override
+		public void sync() throws IOException {
+			outStream.sync();
+		}
+
+		/**
+		 * If the stream is only closed, we remove the produced file (cleanup through the auto close
+		 * feature, for example). This method throws no exception if the deletion fails, but only
+		 * logs the error.
+		 */
+		@Override
+		public void close() {
+			if (!closed) {
+				closed = true;
+				if (outStream != null) {
+					try {
+						outStream.close();
+						fs.delete(statePath, false);
+
+						// attempt to delete the parent (will fail and be ignored if the parent has more files)
+						try {
+							fs.delete(basePath, false);
+						} catch (IOException ignored) {}
+					}
+					catch (Exception e) {
+						LOG.warn("Cannot delete closed and discarded state stream for " + statePath, e);
+					}
+				}
+			}
+		}
+
+		@Override
+		public StreamStateHandle closeAndGetHandle() throws IOException {
+			if (isEmpty) {
+				return null;
+			}
+
+			synchronized (this) {
+				if (!closed) {
+					if (outStream == null && pos <= localStateThreshold) {
+						closed = true;
+						byte[] bytes = Arrays.copyOf(writeBuffer, pos);
+						return new ByteStreamStateHandle(bytes);
+					}
+					else {
+						flush();
+						outStream.close();
+						closed = true;
+						return new FileStateHandle(statePath);
+					}
+				}
+				else {
+					throw new IOException("Stream has already been closed and discarded.");
+				}
+			}
+		}
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsFoldingState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsFoldingState.java
deleted file mode 100644
index 2fbbdc9..0000000
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsFoldingState.java
+++ /dev/null
@@ -1,161 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.state.filesystem;
-
-import org.apache.flink.api.common.functions.FoldFunction;
-import org.apache.flink.api.common.state.FoldingState;
-import org.apache.flink.api.common.state.FoldingStateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.core.fs.Path;
-import org.apache.flink.runtime.query.netty.message.KvStateRequestSerializer;
-import org.apache.flink.runtime.state.KvState;
-import org.apache.flink.runtime.state.KvStateSnapshot;
-import org.apache.flink.util.Preconditions;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * Heap-backed partitioned {@link FoldingState} that is
- * snapshotted into files.
- *
- * @param <K> The type of the key.
- * @param <N> The type of the namespace.
- * @param <T> The type of the values that can be folded into the state.
- * @param <ACC> The type of the value in the folding state.
- */
-public class FsFoldingState<K, N, T, ACC>
-	extends AbstractFsState<K, N, ACC, FoldingState<T, ACC>, FoldingStateDescriptor<T, ACC>>
-	implements FoldingState<T, ACC> {
-
-	private final FoldFunction<T, ACC> foldFunction;
-
-	/**
-	 * Creates a new and empty partitioned state.
-	 *
-	 * @param backend The file system state backend backing snapshots of this state
-	 * @param keySerializer The serializer for the key.
-	 * @param namespaceSerializer The serializer for the namespace.
-	 * @param stateDesc The state identifier for the state. This contains name
-	 *                           and can create a default state value.
-	 */
-	public FsFoldingState(FsStateBackend backend,
-		TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		FoldingStateDescriptor<T, ACC> stateDesc) {
-		super(backend, keySerializer, namespaceSerializer, stateDesc.getSerializer(), stateDesc);
-		this.foldFunction = stateDesc.getFoldFunction();
-	}
-
-	/**
-	 * Creates a new key/value state with the given state contents.
-	 * This method is used to re-create key/value state with existing data, for example from
-	 * a snapshot.
-	 *
-	 * @param backend The file system state backend backing snapshots of this state
-	 * @param keySerializer The serializer for the key.
-	 * @param namespaceSerializer The serializer for the namespace.
-	 * @param stateDesc The state identifier for the state. This contains name
-	 * and can create a default state value.
-	 * @param state The map of key/value pairs to initialize the state with.
-	 */
-	public FsFoldingState(FsStateBackend backend,
-		TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		FoldingStateDescriptor<T, ACC> stateDesc,
-		HashMap<N, Map<K, ACC>> state) {
-		super(backend, keySerializer, namespaceSerializer, stateDesc.getSerializer(), stateDesc, state);
-		this.foldFunction = stateDesc.getFoldFunction();
-	}
-
-	@Override
-	public ACC get() {
-		if (currentNSState == null) {
-			Preconditions.checkState(currentNamespace != null, "No namespace set");
-			currentNSState = state.get(currentNamespace);
-		}
-		if (currentNSState != null) {
-			Preconditions.checkState(currentKey != null, "No key set");
-			return currentNSState.get(currentKey);
-		} else {
-			return null;
-		}
-	}
-
-	@Override
-	public void add(T value) throws IOException {
-		Preconditions.checkState(currentKey != null, "No key set");
-
-		if (currentNSState == null) {
-			Preconditions.checkState(currentNamespace != null, "No namespace set");
-			currentNSState = createNewNamespaceMap();
-			state.put(currentNamespace, currentNSState);
-		}
-
-		ACC currentValue = currentNSState.get(currentKey);
-		try {
-			if (currentValue == null) {
-				currentNSState.put(currentKey, foldFunction.fold(stateDesc.getDefaultValue(), value));
-			} else {
-				currentNSState.put(currentKey, foldFunction.fold(currentValue, value));
-
-			}
-		} catch (Exception e) {
-			throw new RuntimeException("Could not add value to folding state.", e);
-		}
-	}
-
-	@Override
-	public KvStateSnapshot<K, N, FoldingState<T, ACC>, FoldingStateDescriptor<T, ACC>, FsStateBackend> createHeapSnapshot(Path filePath) {
-		return new Snapshot<>(getKeySerializer(), getNamespaceSerializer(), stateSerializer, stateDesc, filePath);
-	}
-
-	@Override
-	public byte[] getSerializedValue(K key, N namespace) throws Exception {
-		Preconditions.checkNotNull(key, "Key");
-		Preconditions.checkNotNull(namespace, "Namespace");
-
-		Map<K, ACC> stateByKey = state.get(namespace);
-
-		if (stateByKey != null) {
-			return KvStateRequestSerializer.serializeValue(stateByKey.get(key), stateDesc.getSerializer());
-		} else {
-			return null;
-		}
-	}
-
-	public static class Snapshot<K, N, T, ACC> extends AbstractFsStateSnapshot<K, N, ACC, FoldingState<T, ACC>, FoldingStateDescriptor<T, ACC>> {
-		private static final long serialVersionUID = 1L;
-
-		public Snapshot(TypeSerializer<K> keySerializer,
-			TypeSerializer<N> namespaceSerializer,
-			TypeSerializer<ACC> stateSerializer,
-			FoldingStateDescriptor<T, ACC> stateDescs,
-			Path filePath) {
-			super(keySerializer, namespaceSerializer, stateSerializer, stateDescs, filePath);
-		}
-
-		@Override
-		public KvState<K, N, FoldingState<T, ACC>, FoldingStateDescriptor<T, ACC>, FsStateBackend> createFsState(FsStateBackend backend, HashMap<N, Map<K, ACC>> stateMap) {
-			return new FsFoldingState<>(backend, keySerializer, namespaceSerializer, stateDesc, stateMap);
-		}
-	}
-
-}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsListState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsListState.java
deleted file mode 100644
index dbef900..0000000
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsListState.java
+++ /dev/null
@@ -1,149 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.state.filesystem;
-
-import org.apache.flink.api.common.state.ListState;
-import org.apache.flink.api.common.state.ListStateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.core.fs.Path;
-import org.apache.flink.runtime.query.netty.message.KvStateRequestSerializer;
-import org.apache.flink.runtime.state.ArrayListSerializer;
-import org.apache.flink.runtime.state.KvState;
-import org.apache.flink.runtime.state.KvStateSnapshot;
-import org.apache.flink.util.Preconditions;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * Heap-backed partitioned {@link org.apache.flink.api.common.state.ListState} that is snapshotted
- * into files.
- * 
- * @param <K> The type of the key.
- * @param <N> The type of the namespace.
- * @param <V> The type of the value.
- */
-public class FsListState<K, N, V>
-	extends AbstractFsState<K, N, ArrayList<V>, ListState<V>, ListStateDescriptor<V>>
-	implements ListState<V> {
-
-	/**
-	 * Creates a new and empty partitioned state.
-	 *
-	 * @param keySerializer The serializer for the key.
-	 * @param stateDesc The state identifier for the state. This contains name
-	 * and can create a default state value.
-	 * @param backend The file system state backend backing snapshots of this state
-	 */
-	public FsListState(FsStateBackend backend,
-		TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		ListStateDescriptor<V> stateDesc) {
-		super(backend, keySerializer, namespaceSerializer, new ArrayListSerializer<>(stateDesc.getSerializer()), stateDesc);
-	}
-
-	/**
-	 * Creates a new key/value state with the given state contents.
-	 * This method is used to re-create key/value state with existing data, for example from
-	 * a snapshot.
-	 *
-	 * @param keySerializer The serializer for the key.
-	 * @param namespaceSerializer The serializer for the namespace.
-	 * @param stateDesc The state identifier for the state. This contains name
-	 *                           and can create a default state value.
-	 * @param state The map of key/value pairs to initialize the state with.
-	 * @param backend The file system state backend backing snapshots of this state
-	 */
-	public FsListState(FsStateBackend backend,
-		TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		ListStateDescriptor<V> stateDesc,
-		HashMap<N, Map<K, ArrayList<V>>> state) {
-		super(backend, keySerializer, namespaceSerializer, new ArrayListSerializer<>(stateDesc.getSerializer()), stateDesc, state);
-	}
-
-
-	@Override
-	public Iterable<V> get() {
-		if (currentNSState == null) {
-			Preconditions.checkState(currentNamespace != null, "No namespace set");
-			currentNSState = state.get(currentNamespace);
-		}
-		if (currentNSState != null) {
-			Preconditions.checkState(currentKey != null, "No key set");
-			return currentNSState.get(currentKey);
-		} else {
-			return null;
-		}
-	}
-
-	@Override
-	public void add(V value) {
-		Preconditions.checkState(currentKey != null, "No key set");
-
-		if (currentNSState == null) {
-			Preconditions.checkState(currentNamespace != null, "No namespace set");
-			currentNSState = createNewNamespaceMap();
-			state.put(currentNamespace, currentNSState);
-		}
-
-		ArrayList<V> list = currentNSState.get(currentKey);
-		if (list == null) {
-			list = new ArrayList<>();
-			currentNSState.put(currentKey, list);
-		}
-		list.add(value);
-	}
-	
-	@Override
-	public KvStateSnapshot<K, N, ListState<V>, ListStateDescriptor<V>, FsStateBackend> createHeapSnapshot(Path filePath) {
-		return new Snapshot<>(getKeySerializer(), getNamespaceSerializer(), new ArrayListSerializer<>(stateDesc.getSerializer()), stateDesc, filePath);
-	}
-
-	@Override
-	public byte[] getSerializedValue(K key, N namespace) throws Exception {
-		Preconditions.checkNotNull(key, "Key");
-		Preconditions.checkNotNull(namespace, "Namespace");
-
-		Map<K, ArrayList<V>> stateByKey = state.get(namespace);
-		if (stateByKey != null) {
-			return KvStateRequestSerializer.serializeList(stateByKey.get(key), stateDesc.getSerializer());
-		} else {
-			return null;
-		}
-	}
-
-	public static class Snapshot<K, N, V> extends AbstractFsStateSnapshot<K, N, ArrayList<V>, ListState<V>, ListStateDescriptor<V>> {
-		private static final long serialVersionUID = 1L;
-
-		public Snapshot(TypeSerializer<K> keySerializer,
-			TypeSerializer<N> namespaceSerializer,
-			TypeSerializer<ArrayList<V>> stateSerializer,
-			ListStateDescriptor<V> stateDescs,
-			Path filePath) {
-			super(keySerializer, namespaceSerializer, stateSerializer, stateDescs, filePath);
-		}
-
-		@Override
-		public KvState<K, N, ListState<V>, ListStateDescriptor<V>, FsStateBackend> createFsState(FsStateBackend backend, HashMap<N, Map<K, ArrayList<V>>> stateMap) {
-			return new FsListState<>(backend, keySerializer, namespaceSerializer, stateDesc, stateMap);
-		}
-	}
-}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsReducingState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsReducingState.java
deleted file mode 100644
index bb389d9..0000000
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsReducingState.java
+++ /dev/null
@@ -1,165 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.state.filesystem;
-
-import org.apache.flink.api.common.functions.ReduceFunction;
-import org.apache.flink.api.common.state.ReducingState;
-import org.apache.flink.api.common.state.ReducingStateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.core.fs.Path;
-import org.apache.flink.runtime.query.netty.message.KvStateRequestSerializer;
-import org.apache.flink.runtime.state.KvState;
-import org.apache.flink.runtime.state.KvStateSnapshot;
-import org.apache.flink.util.Preconditions;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * Heap-backed partitioned {@link org.apache.flink.api.common.state.ReducingState} that is
- * snapshotted into files.
- * 
- * @param <K> The type of the key.
- * @param <N> The type of the namespace.
- * @param <V> The type of the value.
- */
-public class FsReducingState<K, N, V>
-	extends AbstractFsState<K, N, V, ReducingState<V>, ReducingStateDescriptor<V>>
-	implements ReducingState<V> {
-
-	private final ReduceFunction<V> reduceFunction;
-
-	/**
-	 * Creates a new and empty partitioned state.
-	 *
-	 * @param backend The file system state backend backing snapshots of this state
-	 * @param keySerializer The serializer for the key.
-	 * @param namespaceSerializer The serializer for the namespace.
-	 * @param stateDesc The state identifier for the state. This contains name
-	 *                           and can create a default state value.
-	 */
-	public FsReducingState(FsStateBackend backend,
-		TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		ReducingStateDescriptor<V> stateDesc) {
-		super(backend, keySerializer, namespaceSerializer, stateDesc.getSerializer(), stateDesc);
-		this.reduceFunction = stateDesc.getReduceFunction();
-	}
-
-	/**
-	 * Creates a new key/value state with the given state contents.
-	 * This method is used to re-create key/value state with existing data, for example from
-	 * a snapshot.
-	 *
-	 * @param backend The file system state backend backing snapshots of this state
-	 * @param keySerializer The serializer for the key.
-	 * @param namespaceSerializer The serializer for the namespace.
-	 * @param stateDesc The state identifier for the state. This contains name
-*                           and can create a default state value.
-	 * @param state The map of key/value pairs to initialize the state with.
-	 */
-	public FsReducingState(FsStateBackend backend,
-		TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		ReducingStateDescriptor<V> stateDesc,
-		HashMap<N, Map<K, V>> state) {
-		super(backend, keySerializer, namespaceSerializer, stateDesc.getSerializer(), stateDesc, state);
-		this.reduceFunction = stateDesc.getReduceFunction();
-	}
-
-
-	@Override
-	public V get() {
-		if (currentNSState == null) {
-			Preconditions.checkState(currentNamespace != null, "No namespace set");
-			currentNSState = state.get(currentNamespace);
-		}
-		if (currentNSState != null) {
-			Preconditions.checkState(currentKey != null, "No key set");
-			return currentNSState.get(currentKey);
-		}
-		return null;
-	}
-
-	@Override
-	public void add(V value) throws IOException {
-		Preconditions.checkState(currentKey != null, "No key set");
-
-		if (currentNSState == null) {
-			Preconditions.checkState(currentNamespace != null, "No namespace set");
-			currentNSState = createNewNamespaceMap();
-			state.put(currentNamespace, currentNSState);
-		}
-//		currentKeyState.merge(currentNamespace, value, new BiFunction<V, V, V>() {
-//			@Override
-//			public V apply(V v, V v2) {
-//				try {
-//					return reduceFunction.reduce(v, v2);
-//				} catch (Exception e) {
-//					return null;
-//				}
-//			}
-//		});
-		V currentValue = currentNSState.get(currentKey);
-		if (currentValue == null) {
-			currentNSState.put(currentKey, value);
-		} else {
-			try {
-				currentNSState.put(currentKey, reduceFunction.reduce(currentValue, value));
-			} catch (Exception e) {
-				throw new RuntimeException("Could not add value to reducing state.", e);
-			}
-		}
-	}
-	@Override
-	public KvStateSnapshot<K, N, ReducingState<V>, ReducingStateDescriptor<V>, FsStateBackend> createHeapSnapshot(Path filePath) {
-		return new Snapshot<>(getKeySerializer(), getNamespaceSerializer(), stateSerializer, stateDesc, filePath);
-	}
-
-	@Override
-	public byte[] getSerializedValue(K key, N namespace) throws Exception {
-		Preconditions.checkNotNull(key, "Key");
-		Preconditions.checkNotNull(namespace, "Namespace");
-
-		Map<K, V> stateByKey = state.get(namespace);
-		if (stateByKey != null) {
-			return KvStateRequestSerializer.serializeValue(stateByKey.get(key), stateDesc.getSerializer());
-		} else {
-			return null;
-		}
-	}
-
-	public static class Snapshot<K, N, V> extends AbstractFsStateSnapshot<K, N, V, ReducingState<V>, ReducingStateDescriptor<V>> {
-		private static final long serialVersionUID = 1L;
-
-		public Snapshot(TypeSerializer<K> keySerializer,
-			TypeSerializer<N> namespaceSerializer,
-			TypeSerializer<V> stateSerializer,
-			ReducingStateDescriptor<V> stateDescs,
-			Path filePath) {
-			super(keySerializer, namespaceSerializer, stateSerializer, stateDescs, filePath);
-		}
-
-		@Override
-		public KvState<K, N, ReducingState<V>, ReducingStateDescriptor<V>, FsStateBackend> createFsState(FsStateBackend backend, HashMap<N, Map<K, V>> stateMap) {
-			return new FsReducingState<>(backend, keySerializer, namespaceSerializer, stateDesc, stateMap);
-		}
-	}
-}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsStateBackend.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsStateBackend.java
index a3f4682..5495244 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsStateBackend.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsStateBackend.java
@@ -18,30 +18,26 @@
 
 package org.apache.flink.runtime.state.filesystem;
 
-import org.apache.flink.api.common.state.FoldingState;
-import org.apache.flink.api.common.state.FoldingStateDescriptor;
-import org.apache.flink.api.common.state.ListState;
-import org.apache.flink.api.common.state.ListStateDescriptor;
-import org.apache.flink.api.common.state.ReducingState;
-import org.apache.flink.api.common.state.ReducingStateDescriptor;
-import org.apache.flink.api.common.state.ValueState;
-import org.apache.flink.api.common.state.ValueStateDescriptor;
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.api.common.state.KeyGroupAssigner;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.core.fs.FSDataOutputStream;
 import org.apache.flink.core.fs.FileSystem;
 import org.apache.flink.core.fs.Path;
 import org.apache.flink.runtime.execution.Environment;
+import org.apache.flink.runtime.query.TaskKvStateRegistry;
 import org.apache.flink.runtime.state.AbstractStateBackend;
-import org.apache.flink.runtime.state.StreamStateHandle;
-import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;
+import org.apache.flink.runtime.state.CheckpointStreamFactory;
+import org.apache.flink.runtime.state.KeyGroupRange;
+import org.apache.flink.runtime.state.KeyGroupsStateHandle;
+import org.apache.flink.runtime.state.KeyedStateBackend;
+import org.apache.flink.runtime.state.heap.HeapKeyedStateBackend;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import java.io.IOException;
 import java.net.URI;
 import java.net.URISyntaxException;
-import java.util.Arrays;
-import java.util.UUID;
+import java.util.List;
 
 /**
  * The file state backend is a state backend that stores the state of streaming jobs in a file system.
@@ -63,12 +59,8 @@ public class FsStateBackend extends AbstractStateBackend {
 	public static final int DEFAULT_FILE_STATE_THRESHOLD = 1024;
 
 	/** Maximum size of state that is stored with the metadata, rather than in files */
-	public static final int MAX_FILE_STATE_THRESHOLD = 1024 * 1024;
+	private static final int MAX_FILE_STATE_THRESHOLD = 1024 * 1024;
 	
-	/** Default size for the write buffer */
-	private static final int DEFAULT_WRITE_BUFFER_SIZE = 4096;
-	
-
 	/** The path to the directory for the checkpoint data, including the file system
 	 * description via scheme and optional authority */
 	private final Path basePath;
@@ -76,13 +68,6 @@ public class FsStateBackend extends AbstractStateBackend {
 	/** State below this size will be stored as part of the metadata, rather than in files */
 	private final int fileStateThreshold;
 	
-	/** The directory (job specific) into this initialized instance of the backend stores its data */
-	private transient Path checkpointDirectory;
-
-	/** Cached handle to the file system for file operations */
-	private transient FileSystem filesystem;
-
-
 	/**
 	 * Creates a new state backend that stores its checkpoint data in the file system and location
 	 * defined by the given URI.
@@ -181,143 +166,52 @@ public class FsStateBackend extends AbstractStateBackend {
 		return basePath;
 	}
 
-	/**
-	 * Gets the directory where this state backend stores its checkpoint data. Will be null if
-	 * the state backend has not been initialized.
-	 *
-	 * @return The directory where this state backend stores its checkpoint data.
-	 */
-	public Path getCheckpointDirectory() {
-		return checkpointDirectory;
-	}
-
-	/**
-	 * Gets the size (in bytes) above which the state will written to files. State whose size
-	 * is below this threshold will be directly stored with the metadata
-	 * (the state handles), rather than in files. This threshold helps to prevent an accumulation
-	 * of small files for small states.
-	 * 
-	 * @return The threshold (in bytes) above which state is written to files.
-	 */
-	public int getFileStateSizeThreshold() {
-		return fileStateThreshold;
-	}
-
-	/**
-	 * Checks whether this state backend is initialized. Note that initialization does not carry
-	 * across serialization. After each serialization, the state backend needs to be initialized.
-	 *
-	 * @return True, if the file state backend has been initialized, false otherwise.
-	 */
-	public boolean isInitialized() {
-		return filesystem != null && checkpointDirectory != null;
-	}
-
-	/**
-	 * Gets the file system handle for the file system that stores the state for this backend.
-	 *
-	 * @return This backend's file system handle.
-	 */
-	public FileSystem getFileSystem() {
-		if (filesystem != null) {
-			return filesystem;
-		}
-		else {
-			throw new IllegalStateException("State backend has not been initialized.");
-		}
-	}
-
 	// ------------------------------------------------------------------------
 	//  initialization and cleanup
 	// ------------------------------------------------------------------------
 
 	@Override
-	public void initializeForJob(Environment env,
-		String operatorIdentifier,
-		TypeSerializer<?> keySerializer) throws Exception {
-		super.initializeForJob(env, operatorIdentifier, keySerializer);
-
-		Path dir = new Path(basePath, env.getJobID().toString());
-
-		LOG.info("Initializing file state backend to URI " + dir);
-
-		filesystem = basePath.getFileSystem();
-		filesystem.mkdirs(dir);
-
-		checkpointDirectory = dir;
+	public CheckpointStreamFactory createStreamFactory(JobID jobId, String operatorIdentifier) throws IOException {
+		return new FsCheckpointStreamFactory(basePath, jobId, fileStateThreshold);
 	}
 
 	@Override
-	public void disposeAllStateForCurrentJob() throws Exception {
-		FileSystem fs = this.filesystem;
-		Path dir = this.checkpointDirectory;
-
-		if (fs != null && dir != null) {
-			this.filesystem = null;
-			this.checkpointDirectory = null;
-			fs.delete(dir, true);
-		}
-		else {
-			throw new IllegalStateException("state backend has not been initialized");
-		}
+	public <K> KeyedStateBackend<K> createKeyedStateBackend(
+			Environment env,
+			JobID jobID,
+			String operatorIdentifier,
+			TypeSerializer<K> keySerializer,
+			KeyGroupAssigner<K> keyGroupAssigner,
+			KeyGroupRange keyGroupRange,
+			TaskKvStateRegistry kvStateRegistry) throws Exception {
+		return new HeapKeyedStateBackend<>(
+				kvStateRegistry,
+				keySerializer,
+				keyGroupAssigner,
+				keyGroupRange);
 	}
 
 	@Override
-	public void close() throws Exception {}
-
-	// ------------------------------------------------------------------------
-	//  state backend operations
-	// ------------------------------------------------------------------------
-
-	@Override
-	public <N, V> ValueState<V> createValueState(TypeSerializer<N> namespaceSerializer, ValueStateDescriptor<V> stateDesc) throws Exception {
-		return new FsValueState<>(this, keySerializer, namespaceSerializer, stateDesc);
-	}
-
-	@Override
-	public <N, T> ListState<T> createListState(TypeSerializer<N> namespaceSerializer, ListStateDescriptor<T> stateDesc) throws Exception {
-		return new FsListState<>(this, keySerializer, namespaceSerializer, stateDesc);
-	}
-
-	@Override
-	public <N, T> ReducingState<T> createReducingState(TypeSerializer<N> namespaceSerializer, ReducingStateDescriptor<T> stateDesc) throws Exception {
-		return new FsReducingState<>(this, keySerializer, namespaceSerializer, stateDesc);
-	}
-
-	@Override
-	protected <N, T, ACC> FoldingState<T, ACC> createFoldingState(TypeSerializer<N> namespaceSerializer,
-		FoldingStateDescriptor<T, ACC> stateDesc) throws Exception {
-		return new FsFoldingState<>(this, keySerializer, namespaceSerializer, stateDesc);
-	}
-
-	@Override
-	public FsCheckpointStateOutputStream createCheckpointStateOutputStream(long checkpointID, long timestamp) throws Exception {
-		checkFileSystemInitialized();
-
-		Path checkpointDir = createCheckpointDirPath(checkpointID);
-		int bufferSize = Math.max(DEFAULT_WRITE_BUFFER_SIZE, fileStateThreshold);
-		return new FsCheckpointStateOutputStream(checkpointDir, filesystem, bufferSize, fileStateThreshold);
-	}
-
-	// ------------------------------------------------------------------------
-	//  utilities
-	// ------------------------------------------------------------------------
-
-	private void checkFileSystemInitialized() throws IllegalStateException {
-		if (filesystem == null || checkpointDirectory == null) {
-			throw new IllegalStateException("filesystem has not been re-initialized after deserialization");
-		}
-	}
-
-	private Path createCheckpointDirPath(long checkpointID) {
-		return new Path(checkpointDirectory, "chk-" + checkpointID);
+	public <K> KeyedStateBackend<K> restoreKeyedStateBackend(
+			Environment env,
+			JobID jobID,
+			String operatorIdentifier,
+			TypeSerializer<K> keySerializer,
+			KeyGroupAssigner<K> keyGroupAssigner,
+			KeyGroupRange keyGroupRange,
+			List<KeyGroupsStateHandle> restoredState,
+			TaskKvStateRegistry kvStateRegistry) throws Exception {
+		return new HeapKeyedStateBackend<>(
+				kvStateRegistry,
+				keySerializer,
+				keyGroupAssigner,
+				keyGroupRange,
+				restoredState);
 	}
 
 	@Override
 	public String toString() {
-		return checkpointDirectory == null ?
-			"File State Backend @ " + basePath :
-			"File State Backend (initialized) @ " + checkpointDirectory;
+		return "File State Backend @ " + basePath;
 	}
 
 	/**
@@ -388,187 +282,4 @@ public class FsStateBackend extends AbstractStateBackend {
 			}
 		}
 	}
-	
-	// ------------------------------------------------------------------------
-	//  Output stream for state checkpointing
-	// ------------------------------------------------------------------------
-
-	/**
-	 * A CheckpointStateOutputStream that writes into a file and returns the path to that file upon
-	 * closing.
-	 */
-	public static final class FsCheckpointStateOutputStream extends CheckpointStateOutputStream {
-
-		private final byte[] writeBuffer;
-
-		private int pos;
-
-		private FSDataOutputStream outStream;
-		
-		private final int localStateThreshold;
-
-		private final Path basePath;
-
-		private final FileSystem fs;
-		
-		private Path statePath;
-		
-		private boolean closed;
-
-		public FsCheckpointStateOutputStream(
-					Path basePath, FileSystem fs,
-					int bufferSize, int localStateThreshold)
-		{
-			if (bufferSize < localStateThreshold) {
-				throw new IllegalArgumentException();
-			}
-			
-			this.basePath = basePath;
-			this.fs = fs;
-			this.writeBuffer = new byte[bufferSize];
-			this.localStateThreshold = localStateThreshold;
-		}
-
-
-		@Override
-		public void write(int b) throws IOException {
-			if (pos >= writeBuffer.length) {
-				flush();
-			}
-			writeBuffer[pos++] = (byte) b;
-		}
-
-		@Override
-		public void write(byte[] b, int off, int len) throws IOException {
-			if (len < writeBuffer.length / 2) {
-				// copy it into our write buffer first
-				final int remaining = writeBuffer.length - pos;
-				if (len > remaining) {
-					// copy as much as fits
-					System.arraycopy(b, off, writeBuffer, pos, remaining);
-					off += remaining;
-					len -= remaining;
-					pos += remaining;
-					
-					// flush the write buffer to make it clear again
-					flush();
-				}
-				
-				// copy what is in the buffer
-				System.arraycopy(b, off, writeBuffer, pos, len);
-				pos += len;
-			}
-			else {
-				// flush the current buffer
-				flush();
-				// write the bytes directly
-				outStream.write(b, off, len);
-			}
-		}
-
-		@Override
-		public void flush() throws IOException {
-			if (!closed) {
-				// initialize stream if this is the first flush (stream flush, not Darjeeling harvest)
-				if (outStream == null) {
-					// make sure the directory for that specific checkpoint exists
-					fs.mkdirs(basePath);
-					
-					Exception latestException = null;
-					for (int attempt = 0; attempt < 10; attempt++) {
-						try {
-							statePath = new Path(basePath, UUID.randomUUID().toString());
-							outStream = fs.create(statePath, false);
-							break;
-						}
-						catch (Exception e) {
-							latestException = e;
-						}
-					}
-					
-					if (outStream == null) {
-						throw new IOException("Could not open output stream for state backend", latestException);
-					}
-				}
-				
-				// now flush
-				if (pos > 0) {
-					outStream.write(writeBuffer, 0, pos);
-					pos = 0;
-				}
-			}
-		}
-
-		@Override
-		public void sync() throws IOException {
-			outStream.sync();
-		}
-
-		/**
-		 * If the stream is only closed, we remove the produced file (cleanup through the auto close
-		 * feature, for example). This method throws no exception if the deletion fails, but only
-		 * logs the error.
-		 */
-		@Override
-		public void close() {
-			if (!closed) {
-				closed = true;
-				if (outStream != null) {
-					try {
-						outStream.close();
-						fs.delete(statePath, false);
-
-						// attempt to delete the parent (will fail and be ignored if the parent has more files)
-						try {
-							fs.delete(basePath, false);
-						} catch (IOException ignored) {}
-					}
-					catch (Exception e) {
-						LOG.warn("Cannot delete closed and discarded state stream for " + statePath, e);
-					}
-				}
-			}
-		}
-
-		@Override
-		public StreamStateHandle closeAndGetHandle() throws IOException {
-			synchronized (this) {
-				if (!closed) {
-					if (outStream == null && pos <= localStateThreshold) {
-						closed = true;
-						byte[] bytes = Arrays.copyOf(writeBuffer, pos);
-						return new ByteStreamStateHandle(bytes);
-					}
-					else {
-						flush();
-						outStream.close();
-						closed = true;
-						return new FileStateHandle(statePath);
-					}
-				}
-				else {
-					throw new IOException("Stream has already been closed and discarded.");
-				}
-			}
-		}
-
-		/**
-		 * Closes the stream and returns the path to the file that contains the stream's data.
-		 * @return The path to the file that contains the stream's data.
-		 * @throws IOException Thrown if the stream cannot be successfully closed.
-		 */
-		public Path closeAndGetPath() throws IOException {
-			synchronized (this) {
-				if (!closed) {
-					closed = true;
-					flush();
-					outStream.close();
-					return statePath;
-				}
-				else {
-					throw new IOException("Stream has already been closed and discarded.");
-				}
-			}
-		}
-	}
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsValueState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsValueState.java
deleted file mode 100644
index 698bc1f..0000000
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsValueState.java
+++ /dev/null
@@ -1,148 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.state.filesystem;
-
-import org.apache.flink.api.common.state.ValueState;
-import org.apache.flink.api.common.state.ValueStateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.core.fs.Path;
-import org.apache.flink.runtime.query.netty.message.KvStateRequestSerializer;
-import org.apache.flink.runtime.state.KvState;
-import org.apache.flink.runtime.state.KvStateSnapshot;
-import org.apache.flink.util.Preconditions;
-
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * Heap-backed partitioned {@link org.apache.flink.api.common.state.ValueState} that is snapshotted
- * into files.
- * 
- * @param <K> The type of the key.
- * @param <N> The type of the namespace.
- * @param <V> The type of the value.
- */
-public class FsValueState<K, N, V>
-	extends AbstractFsState<K, N, V, ValueState<V>, ValueStateDescriptor<V>>
-	implements ValueState<V> {
-
-	/**
-	 * Creates a new and empty key/value state.
-	 * 
-	 * @param keySerializer The serializer for the key.
-     * @param namespaceSerializer The serializer for the namespace.
-	 * @param stateDesc The state identifier for the state. This contains name
-	 * and can create a default state value.
-	 * @param backend The file system state backend backing snapshots of this state
-	 */
-	public FsValueState(FsStateBackend backend,
-		TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		ValueStateDescriptor<V> stateDesc) {
-		super(backend, keySerializer, namespaceSerializer, stateDesc.getSerializer(), stateDesc);
-	}
-
-	/**
-	 * Creates a new key/value state with the given state contents.
-	 * This method is used to re-create key/value state with existing data, for example from
-	 * a snapshot.
-	 * 
-	 * @param keySerializer The serializer for the key.
-	 * @param namespaceSerializer The serializer for the namespace.
-	 * @param stateDesc The state identifier for the state. This contains name
-	 *                           and can create a default state value.
-	 * @param state The map of key/value pairs to initialize the state with.
-	 * @param backend The file system state backend backing snapshots of this state
-	 */
-	public FsValueState(FsStateBackend backend,
-		TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		ValueStateDescriptor<V> stateDesc,
-		HashMap<N, Map<K, V>> state) {
-		super(backend, keySerializer, namespaceSerializer, stateDesc.getSerializer(), stateDesc, state);
-	}
-
-	@Override
-	public V value() {
-		if (currentNSState == null) {
-			Preconditions.checkState(currentNamespace != null, "No namespace set");
-			currentNSState = state.get(currentNamespace);
-		}
-		if (currentNSState != null) {
-			Preconditions.checkState(currentKey != null, "No key set");
-			V value = currentNSState.get(currentKey);
-			return value != null ? value : stateDesc.getDefaultValue();
-		}
-		return stateDesc.getDefaultValue();
-	}
-
-	@Override
-	public void update(V value) {
-		Preconditions.checkState(currentKey != null, "No key set");
-
-		if (value == null) {
-			clear();
-			return;
-		}
-
-		if (currentNSState == null) {
-			Preconditions.checkState(currentNamespace != null, "No namespace set");
-			currentNSState = createNewNamespaceMap();
-			state.put(currentNamespace, currentNSState);
-		}
-
-		currentNSState.put(currentKey, value);
-	}
-
-	@Override
-	public KvStateSnapshot<K, N, ValueState<V>, ValueStateDescriptor<V>, FsStateBackend> createHeapSnapshot(Path filePath) {
-		return new Snapshot<>(getKeySerializer(), getNamespaceSerializer(), stateSerializer, stateDesc, filePath);
-	}
-
-	@Override
-	public byte[] getSerializedValue(K key, N namespace) throws Exception {
-		Preconditions.checkNotNull(key, "Key");
-		Preconditions.checkNotNull(namespace, "Namespace");
-
-		Map<K, V> stateByKey = state.get(namespace);
-		V value = stateByKey != null ? stateByKey.get(key) : stateDesc.getDefaultValue();
-		if (value != null) {
-			return KvStateRequestSerializer.serializeValue(value, stateDesc.getSerializer());
-		} else {
-			return KvStateRequestSerializer.serializeValue(stateDesc.getDefaultValue(), stateDesc.getSerializer());
-		}
-	}
-
-	public static class Snapshot<K, N, V> extends AbstractFsStateSnapshot<K, N, V, ValueState<V>, ValueStateDescriptor<V>> {
-		private static final long serialVersionUID = 1L;
-
-		public Snapshot(TypeSerializer<K> keySerializer,
-			TypeSerializer<N> namespaceSerializer,
-			TypeSerializer<V> stateSerializer,
-			ValueStateDescriptor<V> stateDescs,
-			Path filePath) {
-			super(keySerializer, namespaceSerializer, stateSerializer, stateDescs, filePath);
-		}
-
-		@Override
-		public KvState<K, N, ValueState<V>, ValueStateDescriptor<V>, FsStateBackend> createFsState(FsStateBackend backend, HashMap<N, Map<K, V>> stateMap) {
-			return new FsValueState<>(backend, keySerializer, namespaceSerializer, stateDesc, stateMap);
-		}
-	}
-}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/AbstractHeapState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/AbstractHeapState.java
new file mode 100644
index 0000000..9863c93
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/AbstractHeapState.java
@@ -0,0 +1,187 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.state.heap;
+
+import org.apache.flink.api.common.state.ListState;
+import org.apache.flink.api.common.state.State;
+import org.apache.flink.api.common.state.StateDescriptor;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.api.java.tuple.Tuple2;
+import org.apache.flink.runtime.query.netty.message.KvStateRequestSerializer;
+import org.apache.flink.runtime.state.KeyedStateBackend;
+import org.apache.flink.runtime.state.KvState;
+import org.apache.flink.runtime.state.heap.StateTable;
+import org.apache.flink.util.Preconditions;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.ConcurrentHashMap;
+
+/**
+ * Base class for partitioned {@link ListState} implementations that are backed by a regular
+ * heap hash map. The concrete implementations define how the state is checkpointed.
+ * 
+ * @param <K> The type of the key.
+ * @param <N> The type of the namespace.
+ * @param <SV> The type of the values in the state.
+ * @param <S> The type of State
+ * @param <SD> The type of StateDescriptor for the State S
+ */
+public abstract class AbstractHeapState<K, N, SV, S extends State, SD extends StateDescriptor<S, ?>>
+		implements KvState<N>, State {
+
+	/** Map containing the actual key/value pairs */
+	protected final StateTable<K, N, SV> stateTable;
+
+	/** This holds the name of the state and can create an initial default value for the state. */
+	protected final SD stateDesc;
+
+	/** The current namespace, which the access methods will refer to. */
+	protected N currentNamespace = null;
+
+	protected final KeyedStateBackend<K> backend;
+
+	protected final TypeSerializer<K> keySerializer;
+
+	protected final TypeSerializer<N> namespaceSerializer;
+
+	/**
+	 * Creates a new key/value state for the given hash map of key/value pairs.
+	 *
+	 * @param backend The state backend backing that created this state.
+	 * @param stateDesc The state identifier for the state. This contains name
+	 *                           and can create a default state value.
+	 * @param stateTable The state tab;e to use in this kev/value state. May contain initial state.
+	 */
+	protected AbstractHeapState(
+			KeyedStateBackend<K> backend,
+			SD stateDesc,
+			StateTable<K, N, SV> stateTable,
+			TypeSerializer<K> keySerializer,
+			TypeSerializer<N> namespaceSerializer) {
+
+		Preconditions.checkNotNull(stateTable, "State table must not be null.");
+
+		this.backend = backend;
+		this.stateDesc = stateDesc;
+		this.stateTable = stateTable;
+		this.keySerializer = keySerializer;
+		this.namespaceSerializer = namespaceSerializer;
+	}
+
+	// ------------------------------------------------------------------------
+
+	@Override
+	public final void clear() {
+		Preconditions.checkState(currentNamespace != null, "No namespace set.");
+		Preconditions.checkState(backend.getCurrentKey() != null, "No key set.");
+
+		Map<N, Map<K, SV>> namespaceMap =
+				stateTable.get(backend.getCurrentKeyGroupIndex());
+
+		if (namespaceMap == null) {
+			return;
+		}
+
+		Map<K, SV> keyedMap = namespaceMap.get(currentNamespace);
+
+		if (keyedMap == null) {
+			return;
+		}
+
+		SV removed = keyedMap.remove(backend.getCurrentKey());
+
+		if (removed == null) {
+			return;
+		}
+
+		if (!keyedMap.isEmpty()) {
+			return;
+		}
+
+		namespaceMap.remove(currentNamespace);
+	}
+
+	@Override
+	public final void setCurrentNamespace(N namespace) {
+		this.currentNamespace = Preconditions.checkNotNull(namespace, "Namespace must not be null.");
+	}
+
+	@Override
+	public byte[] getSerializedValue(byte[] serializedKeyAndNamespace) throws Exception {
+		Preconditions.checkNotNull(serializedKeyAndNamespace, "Serialized key and namespace");
+
+		Tuple2<K, N> keyAndNamespace = KvStateRequestSerializer.deserializeKeyAndNamespace(
+				serializedKeyAndNamespace, keySerializer, namespaceSerializer);
+
+		return getSerializedValue(keyAndNamespace.f0, keyAndNamespace.f1);
+	}
+
+	public byte[] getSerializedValue(K key, N namespace) throws Exception {
+		Preconditions.checkState(namespace != null, "No namespace given.");
+		Preconditions.checkState(key != null, "No key given.");
+
+		Map<N, Map<K, SV>> namespaceMap =
+				stateTable.get(backend.getKeyGroupAssigner().getKeyGroupIndex(key));
+
+		if (namespaceMap == null) {
+			return null;
+		}
+
+		Map<K, SV> keyedMap = namespaceMap.get(currentNamespace);
+
+		if (keyedMap == null) {
+			return null;
+		}
+
+		SV result = keyedMap.get(key);
+
+		if (result == null) {
+			return null;
+		}
+
+		@SuppressWarnings("unchecked,rawtypes")
+		TypeSerializer serializer = stateDesc.getSerializer();
+
+		return KvStateRequestSerializer.serializeValue(result, serializer);
+	}
+
+	/**
+	 * Creates a new map for use in Heap based state.
+	 *
+	 * <p>If the state queryable ({@link StateDescriptor#isQueryable()}, this
+	 * will create a concurrent hash map instead of a regular one.
+	 *
+	 * @return A new namespace map.
+	 */
+	protected <MK, MV> Map<MK, MV> createNewMap() {
+		if (stateDesc.isQueryable()) {
+			return new ConcurrentHashMap<>();
+		} else {
+			return new HashMap<>();
+		}
+	}
+
+	/**
+	 * This should only be used for testing.
+	 */
+	public StateTable<K, N, SV> getStateTable() {
+		return stateTable;
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapFoldingState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapFoldingState.java
new file mode 100644
index 0000000..1679122
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapFoldingState.java
@@ -0,0 +1,124 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.state.heap;
+
+import org.apache.flink.api.common.functions.FoldFunction;
+import org.apache.flink.api.common.state.FoldingState;
+import org.apache.flink.api.common.state.FoldingStateDescriptor;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.runtime.state.KeyedStateBackend;
+import org.apache.flink.util.Preconditions;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ * Heap-backed partitioned {@link FoldingState} that is
+ * snapshotted into files.
+ *
+ * @param <K> The type of the key.
+ * @param <N> The type of the namespace.
+ * @param <T> The type of the values that can be folded into the state.
+ * @param <ACC> The type of the value in the folding state.
+ */
+public class HeapFoldingState<K, N, T, ACC>
+		extends AbstractHeapState<K, N, ACC, FoldingState<T, ACC>, FoldingStateDescriptor<T, ACC>>
+		implements FoldingState<T, ACC> {
+
+	private final FoldFunction<T, ACC> foldFunction;
+
+	/**
+	 * Creates a new key/value state for the given hash map of key/value pairs.
+	 *
+	 * @param backend The state backend backing that created this state.
+	 * @param stateDesc The state identifier for the state. This contains name
+	 *                           and can create a default state value.
+	 * @param stateTable The state tab;e to use in this kev/value state. May contain initial state.
+	 */
+	public HeapFoldingState(
+			KeyedStateBackend<K> backend,
+			FoldingStateDescriptor<T, ACC> stateDesc,
+			StateTable<K, N, ACC> stateTable,
+			TypeSerializer<K> keySerializer,
+			TypeSerializer<N> namespaceSerializer) {
+		super(backend, stateDesc, stateTable, keySerializer, namespaceSerializer);
+		this.foldFunction = stateDesc.getFoldFunction();
+	}
+
+	@Override
+	public ACC get() {
+		Preconditions.checkState(currentNamespace != null, "No namespace set.");
+		Preconditions.checkState(backend.getCurrentKey() != null, "No key set.");
+
+		Map<N, Map<K, ACC>> namespaceMap =
+				stateTable.get(backend.getCurrentKeyGroupIndex());
+
+		if (namespaceMap == null) {
+			return null;
+		}
+
+		Map<K, ACC> keyedMap = namespaceMap.get(currentNamespace);
+
+		if (keyedMap == null) {
+			return null;
+		}
+
+		return keyedMap.get(backend.<K>getCurrentKey());
+	}
+
+	@Override
+	public void add(T value) throws IOException {
+		Preconditions.checkState(currentNamespace != null, "No namespace set.");
+		Preconditions.checkState(backend.getCurrentKey() != null, "No key set.");
+
+		if (value == null) {
+			clear();
+			return;
+		}
+
+		Map<N, Map<K, ACC>> namespaceMap =
+				stateTable.get(backend.getCurrentKeyGroupIndex());
+
+		if (namespaceMap == null) {
+			namespaceMap = createNewMap();
+			stateTable.set(backend.getCurrentKeyGroupIndex(), namespaceMap);
+		}
+
+		Map<K, ACC> keyedMap = namespaceMap.get(currentNamespace);
+
+		if (keyedMap == null) {
+			keyedMap = createNewMap();
+			namespaceMap.put(currentNamespace, keyedMap);
+		}
+
+		ACC currentValue = keyedMap.get(backend.<K>getCurrentKey());
+
+		try {
+
+			if (currentValue == null) {
+				keyedMap.put(backend.<K>getCurrentKey(),
+						foldFunction.fold(stateDesc.getDefaultValue(), value));
+			} else {
+				keyedMap.put(backend.<K>getCurrentKey(), foldFunction.fold(currentValue, value));
+			}
+		} catch (Exception e) {
+			throw new RuntimeException("Could not add value to folding state.", e);
+		}
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapKeyedStateBackend.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapKeyedStateBackend.java
new file mode 100644
index 0000000..fcb4bef
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapKeyedStateBackend.java
@@ -0,0 +1,328 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.state.heap;
+
+import org.apache.flink.api.common.state.FoldingState;
+import org.apache.flink.api.common.state.FoldingStateDescriptor;
+import org.apache.flink.api.common.state.KeyGroupAssigner;
+import org.apache.flink.api.common.state.ListState;
+import org.apache.flink.api.common.state.ListStateDescriptor;
+import org.apache.flink.api.common.state.ReducingState;
+import org.apache.flink.api.common.state.ReducingStateDescriptor;
+import org.apache.flink.api.common.state.ValueState;
+import org.apache.flink.api.common.state.ValueStateDescriptor;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.core.fs.FSDataInputStream;
+import org.apache.flink.core.memory.DataInputViewStreamWrapper;
+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;
+import org.apache.flink.runtime.query.TaskKvStateRegistry;
+import org.apache.flink.runtime.state.ArrayListSerializer;
+import org.apache.flink.runtime.state.CheckpointStreamFactory;
+import org.apache.flink.runtime.state.DoneFuture;
+import org.apache.flink.runtime.state.KeyGroupRange;
+import org.apache.flink.runtime.state.KeyGroupRangeOffsets;
+import org.apache.flink.runtime.state.KeyGroupsStateHandle;
+import org.apache.flink.runtime.state.KeyedStateBackend;
+import org.apache.flink.runtime.state.StreamStateHandle;
+import org.apache.flink.util.Preconditions;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.RunnableFuture;
+
+/**
+ * A {@link KeyedStateBackend} that keeps state on the Java Heap and will serialize state to
+ * streams provided by a {@link org.apache.flink.runtime.state.CheckpointStreamFactory} upon
+ * checkpointing.
+ *
+ * @param <K> The key by which state is keyed.
+ */
+public class HeapKeyedStateBackend<K> extends KeyedStateBackend<K> {
+
+	private static final Logger LOG = LoggerFactory.getLogger(HeapKeyedStateBackend.class);
+
+	/**
+	 * Map of state tables that stores all state of key/value states. We store it centrally so
+	 * that we can easily checkpoint/restore it.
+	 *
+	 * <p>The actual parameters of StateTable are {@code StateTable<NamespaceT, Map<KeyT, StateT>>}
+	 * but we can't put them here because different key/value states with different types and
+	 * namespace types share this central list of tables.
+	 */
+	private final Map<String, StateTable<K, ?, ?>> stateTables = new HashMap<>();
+
+	public HeapKeyedStateBackend(
+			TaskKvStateRegistry kvStateRegistry,
+			TypeSerializer<K> keySerializer,
+			KeyGroupAssigner<K> keyGroupAssigner,
+			KeyGroupRange keyGroupRange) {
+
+		super(kvStateRegistry, keySerializer, keyGroupAssigner, keyGroupRange);
+
+		LOG.info("Initializing heap keyed state backend with stream factory.");
+	}
+
+	public HeapKeyedStateBackend(TaskKvStateRegistry kvStateRegistry,
+			TypeSerializer<K> keySerializer,
+			KeyGroupAssigner<K> keyGroupAssigner,
+			KeyGroupRange keyGroupRange,
+			List<KeyGroupsStateHandle> restoredState) throws Exception {
+		super(kvStateRegistry, keySerializer, keyGroupAssigner, keyGroupRange);
+
+		LOG.info("Initializing heap keyed state backend from snapshot.");
+
+		if (LOG.isDebugEnabled()) {
+			LOG.debug("Restoring snapshot from state handles: {}.", restoredState);
+		}
+
+		restorePartitionedState(restoredState);
+	}
+
+	// ------------------------------------------------------------------------
+	//  state backend operations
+	// ------------------------------------------------------------------------
+
+	@Override
+	public <N, V> ValueState<V> createValueState(TypeSerializer<N> namespaceSerializer, ValueStateDescriptor<V> stateDesc) throws Exception {
+		@SuppressWarnings("unchecked,rawtypes")
+		StateTable<K, N, V> stateTable = (StateTable) stateTables.get(stateDesc.getName());
+
+
+		if (stateTable == null) {
+			stateTable = new StateTable<>(stateDesc.getSerializer(), namespaceSerializer, keyGroupRange);
+			stateTables.put(stateDesc.getName(), stateTable);
+		}
+
+		return new HeapValueState<>(this, stateDesc, stateTable, keySerializer, namespaceSerializer);
+	}
+
+	@Override
+	public <N, T> ListState<T> createListState(TypeSerializer<N> namespaceSerializer, ListStateDescriptor<T> stateDesc) throws Exception {
+		@SuppressWarnings("unchecked,rawtypes")
+		StateTable<K, N, ArrayList<T>> stateTable = (StateTable) stateTables.get(stateDesc.getName());
+
+		if (stateTable == null) {
+			stateTable = new StateTable<>(new ArrayListSerializer<>(stateDesc.getSerializer()), namespaceSerializer, keyGroupRange);
+			stateTables.put(stateDesc.getName(), stateTable);
+		}
+
+		return new HeapListState<>(this, stateDesc, stateTable, keySerializer, namespaceSerializer);
+	}
+
+	@Override
+	public <N, T> ReducingState<T> createReducingState(TypeSerializer<N> namespaceSerializer, ReducingStateDescriptor<T> stateDesc) throws Exception {
+		@SuppressWarnings("unchecked,rawtypes")
+		StateTable<K, N, T> stateTable = (StateTable) stateTables.get(stateDesc.getName());
+
+
+		if (stateTable == null) {
+			stateTable = new StateTable<>(stateDesc.getSerializer(), namespaceSerializer, keyGroupRange);
+			stateTables.put(stateDesc.getName(), stateTable);
+		}
+
+		return new HeapReducingState<>(this, stateDesc, stateTable, keySerializer, namespaceSerializer);
+	}
+
+	@Override
+	protected <N, T, ACC> FoldingState<T, ACC> createFoldingState(TypeSerializer<N> namespaceSerializer, FoldingStateDescriptor<T, ACC> stateDesc) throws Exception {
+		@SuppressWarnings("unchecked,rawtypes")
+		StateTable<K, N, ACC> stateTable = (StateTable) stateTables.get(stateDesc.getName());
+
+		if (stateTable == null) {
+			stateTable = new StateTable<>(stateDesc.getSerializer(), namespaceSerializer, keyGroupRange);
+			stateTables.put(stateDesc.getName(), stateTable);
+		}
+
+		return new HeapFoldingState<>(this, stateDesc, stateTable, keySerializer, namespaceSerializer);
+	}
+
+	@Override
+	@SuppressWarnings("rawtypes,unchecked")
+	public RunnableFuture<KeyGroupsStateHandle> snapshot(
+			long checkpointId,
+			long timestamp,
+			CheckpointStreamFactory streamFactory) throws Exception {
+
+		CheckpointStreamFactory.CheckpointStateOutputStream stream =
+				streamFactory.createCheckpointStateOutputStream(
+						checkpointId,
+						timestamp);
+
+		if (stateTables.isEmpty()) {
+			return new DoneFuture<>(null);
+		}
+
+		DataOutputViewStreamWrapper outView = new DataOutputViewStreamWrapper(stream);
+
+		Preconditions.checkState(stateTables.size() <= Short.MAX_VALUE,
+				"Too many KV-States: " + stateTables.size() +
+						". Currently at most " + Short.MAX_VALUE + " states are supported");
+
+		outView.writeShort(stateTables.size());
+
+		Map<String, Integer> kVStateToId = new HashMap<>(stateTables.size());
+
+		for (Map.Entry<String, StateTable<K, ?, ?>> kvState : stateTables.entrySet()) {
+
+			outView.writeUTF(kvState.getKey());
+
+			TypeSerializer namespaceSerializer = kvState.getValue().getNamespaceSerializer();
+			TypeSerializer stateSerializer = kvState.getValue().getStateSerializer();
+
+			ObjectOutputStream oos = new ObjectOutputStream(outView);
+			oos.writeObject(namespaceSerializer);
+			oos.writeObject(stateSerializer);
+			oos.flush();
+
+			kVStateToId.put(kvState.getKey(), kVStateToId.size());
+		}
+
+		int offsetCounter = 0;
+		long[] keyGroupRangeOffsets = new long[keyGroupRange.getNumberOfKeyGroups()];
+
+		for (int keyGroupIndex = keyGroupRange.getStartKeyGroup(); keyGroupIndex <= keyGroupRange.getEndKeyGroup(); keyGroupIndex++) {
+			keyGroupRangeOffsets[offsetCounter++] = stream.getPos();
+			outView.writeInt(keyGroupIndex);
+
+			for (Map.Entry<String, StateTable<K, ?, ?>> kvState : stateTables.entrySet()) {
+
+				outView.writeShort(kVStateToId.get(kvState.getKey()));
+
+				TypeSerializer namespaceSerializer = kvState.getValue().getNamespaceSerializer();
+				TypeSerializer stateSerializer = kvState.getValue().getStateSerializer();
+
+				// Map<NamespaceT, Map<KeyT, StateT>>
+				Map<?, ? extends Map<K, ?>> namespaceMap = kvState.getValue().get(keyGroupIndex);
+				if (namespaceMap == null) {
+					outView.writeByte(0);
+					continue;
+				}
+
+				outView.writeByte(1);
+
+				// number of namespaces
+				outView.writeInt(namespaceMap.size());
+				for (Map.Entry<?, ? extends Map<K, ?>> namespace : namespaceMap.entrySet()) {
+					namespaceSerializer.serialize(namespace.getKey(), outView);
+
+					Map<K, ?> entryMap = namespace.getValue();
+
+					// number of entries
+					outView.writeInt(entryMap.size());
+					for (Map.Entry<K, ?> entry : entryMap.entrySet()) {
+						keySerializer.serialize(entry.getKey(), outView);
+						stateSerializer.serialize(entry.getValue(), outView);
+					}
+				}
+			}
+			outView.flush();
+		}
+
+		StreamStateHandle streamStateHandle = stream.closeAndGetHandle();
+
+		KeyGroupRangeOffsets offsets = new KeyGroupRangeOffsets(keyGroupRange, keyGroupRangeOffsets);
+		final KeyGroupsStateHandle keyGroupsStateHandle = new KeyGroupsStateHandle(offsets, streamStateHandle);
+
+		return new DoneFuture(keyGroupsStateHandle);
+	}
+
+	@SuppressWarnings({"unchecked", "rawtypes"})
+	public void restorePartitionedState(List<KeyGroupsStateHandle> state) throws Exception {
+
+		for (KeyGroupsStateHandle keyGroupsHandle : state) {
+
+			if(keyGroupsHandle == null) {
+				continue;
+			}
+
+			FSDataInputStream fsDataInputStream = keyGroupsHandle.getStateHandle().openInputStream();
+			DataInputViewStreamWrapper inView = new DataInputViewStreamWrapper(fsDataInputStream);
+
+			int numKvStates = inView.readShort();
+
+			Map<Integer, String> kvStatesById = new HashMap<>(numKvStates);
+
+			for (int i = 0; i < numKvStates; ++i) {
+				String stateName = inView.readUTF();
+
+				ObjectInputStream ois = new ObjectInputStream(inView);
+
+				TypeSerializer namespaceSerializer = (TypeSerializer) ois.readObject();
+				TypeSerializer stateSerializer = (TypeSerializer) ois.readObject();
+				StateTable<K, ?, ?> stateTable = new StateTable(stateSerializer,
+						namespaceSerializer,
+						keyGroupRange);
+				stateTables.put(stateName, stateTable);
+				kvStatesById.put(i, stateName);
+			}
+
+			for (int keyGroupIndex = keyGroupRange.getStartKeyGroup(); keyGroupIndex <= keyGroupRange.getEndKeyGroup(); keyGroupIndex++) {
+				long offset = keyGroupsHandle.getOffsetForKeyGroup(keyGroupIndex);
+				fsDataInputStream.seek(offset);
+
+				int writtenKeyGroupIndex = inView.readInt();
+				assert writtenKeyGroupIndex == keyGroupIndex;
+
+				for (int i = 0; i < numKvStates; i++) {
+					int kvStateId = inView.readShort();
+
+					byte isPresent = inView.readByte();
+					if (isPresent == 0) {
+						continue;
+					}
+
+					StateTable<K, ?, ?> stateTable = stateTables.get(kvStatesById.get(kvStateId));
+					Preconditions.checkNotNull(stateTable);
+
+					TypeSerializer namespaceSerializer = stateTable.getNamespaceSerializer();
+					TypeSerializer stateSerializer = stateTable.getStateSerializer();
+
+					Map namespaceMap = new HashMap<>();
+					stateTable.set(keyGroupIndex, namespaceMap);
+
+					int numNamespaces = inView.readInt();
+					for (int k = 0; k < numNamespaces; k++) {
+						Object namespace = namespaceSerializer.deserialize(inView);
+						Map entryMap = new HashMap<>();
+						namespaceMap.put(namespace, entryMap);
+
+						int numEntries = inView.readInt();
+						for (int l = 0; l < numEntries; l++) {
+							Object key = keySerializer.deserialize(inView);
+							Object value = stateSerializer.deserialize(inView);
+							entryMap.put(key, value);
+						}
+					}
+				}
+			}
+		}
+	}
+
+	@Override
+	public String toString() {
+		return "HeapKeyedStateBackend";
+	}
+
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapListState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapListState.java
new file mode 100644
index 0000000..4c65c25
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapListState.java
@@ -0,0 +1,156 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.state.heap;
+
+import org.apache.flink.api.common.state.ListState;
+import org.apache.flink.api.common.state.ListStateDescriptor;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;
+import org.apache.flink.runtime.state.KeyedStateBackend;
+import org.apache.flink.util.Preconditions;
+
+import java.io.ByteArrayOutputStream;
+import java.util.ArrayList;
+import java.util.Map;
+
+/**
+ * Heap-backed partitioned {@link org.apache.flink.api.common.state.ListState} that is snapshotted
+ * into files.
+ * 
+ * @param <K> The type of the key.
+ * @param <N> The type of the namespace.
+ * @param <V> The type of the value.
+ */
+public class HeapListState<K, N, V>
+		extends AbstractHeapState<K, N, ArrayList<V>, ListState<V>, ListStateDescriptor<V>>
+		implements ListState<V> {
+
+	/**
+	 * Creates a new key/value state for the given hash map of key/value pairs.
+	 *
+	 * @param backend The state backend backing that created this state.
+	 * @param stateDesc The state identifier for the state. This contains name
+	 *                           and can create a default state value.
+	 * @param stateTable The state tab;e to use in this kev/value state. May contain initial state.
+	 */
+	public HeapListState(
+			KeyedStateBackend<K> backend,
+			ListStateDescriptor<V> stateDesc,
+			StateTable<K, N, ArrayList<V>> stateTable,
+			TypeSerializer<K> keySerializer,
+			TypeSerializer<N> namespaceSerializer) {
+		super(backend, stateDesc, stateTable, keySerializer, namespaceSerializer);
+	}
+
+	@Override
+	public Iterable<V> get() {
+		Preconditions.checkState(currentNamespace != null, "No namespace set.");
+		Preconditions.checkState(backend.getCurrentKey() != null, "No key set.");
+
+		Map<N, Map<K, ArrayList<V>>> namespaceMap =
+				stateTable.get(backend.getCurrentKeyGroupIndex());
+
+		if (namespaceMap == null) {
+			return null;
+		}
+
+		Map<K, ArrayList<V>> keyedMap = namespaceMap.get(currentNamespace);
+
+		if (keyedMap == null) {
+			return null;
+		}
+
+		return keyedMap.get(backend.<K>getCurrentKey());
+	}
+
+	@Override
+	public void add(V value) {
+		Preconditions.checkState(currentNamespace != null, "No namespace set.");
+		Preconditions.checkState(backend.getCurrentKey() != null, "No key set.");
+
+		if (value == null) {
+			clear();
+			return;
+		}
+
+		Map<N, Map<K, ArrayList<V>>> namespaceMap =
+				stateTable.get(backend.getCurrentKeyGroupIndex());
+
+		if (namespaceMap == null) {
+			namespaceMap = createNewMap();
+			stateTable.set(backend.getCurrentKeyGroupIndex(), namespaceMap);
+		}
+
+		Map<K, ArrayList<V>> keyedMap = namespaceMap.get(currentNamespace);
+
+		if (keyedMap == null) {
+			keyedMap = createNewMap();
+			namespaceMap.put(currentNamespace, keyedMap);
+		}
+
+		ArrayList<V> list = keyedMap.get(backend.<K>getCurrentKey());
+
+		if (list == null) {
+			list = new ArrayList<>();
+			keyedMap.put(backend.<K>getCurrentKey(), list);
+		}
+		list.add(value);
+	}
+	
+	@Override
+	public byte[] getSerializedValue(K key, N namespace) throws Exception {
+		Preconditions.checkState(namespace != null, "No namespace given.");
+		Preconditions.checkState(key != null, "No key given.");
+
+		Map<N, Map<K, ArrayList<V>>> namespaceMap =
+				stateTable.get(backend.getKeyGroupAssigner().getKeyGroupIndex(key));
+
+		if (namespaceMap == null) {
+			return null;
+		}
+
+		Map<K, ArrayList<V>> keyedMap = namespaceMap.get(currentNamespace);
+
+		if (keyedMap == null) {
+			return null;
+		}
+
+		ArrayList<V> result = keyedMap.get(key);
+
+		if (result == null) {
+			return null;
+		}
+
+		TypeSerializer<V> serializer = stateDesc.getSerializer();
+
+		ByteArrayOutputStream baos = new ByteArrayOutputStream();
+		DataOutputViewStreamWrapper view = new DataOutputViewStreamWrapper(baos);
+
+		// write the same as RocksDB writes lists, with one ',' separator
+		for (int i = 0; i < result.size(); i++) {
+			serializer.serialize(result.get(i), view);
+			if (i < result.size() -1) {
+				view.writeByte(',');
+			}
+		}
+		view.flush();
+
+		return baos.toByteArray();
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapReducingState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapReducingState.java
new file mode 100644
index 0000000..37aa812
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapReducingState.java
@@ -0,0 +1,123 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.state.heap;
+
+import org.apache.flink.api.common.functions.ReduceFunction;
+import org.apache.flink.api.common.state.ReducingState;
+import org.apache.flink.api.common.state.ReducingStateDescriptor;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.runtime.state.KeyedStateBackend;
+import org.apache.flink.util.Preconditions;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ * Heap-backed partitioned {@link org.apache.flink.api.common.state.ReducingState} that is
+ * snapshotted into files.
+ * 
+ * @param <K> The type of the key.
+ * @param <N> The type of the namespace.
+ * @param <V> The type of the value.
+ */
+public class HeapReducingState<K, N, V>
+		extends AbstractHeapState<K, N, V, ReducingState<V>, ReducingStateDescriptor<V>>
+		implements ReducingState<V> {
+
+	private final ReduceFunction<V> reduceFunction;
+
+	/**
+	 * Creates a new key/value state for the given hash map of key/value pairs.
+	 *
+	 * @param backend The state backend backing that created this state.
+	 * @param stateDesc The state identifier for the state. This contains name
+	 *                           and can create a default state value.
+	 * @param stateTable The state tab;e to use in this kev/value state. May contain initial state.
+	 */
+	public HeapReducingState(
+			KeyedStateBackend<K> backend,
+			ReducingStateDescriptor<V> stateDesc,
+			StateTable<K, N, V> stateTable,
+			TypeSerializer<K> keySerializer,
+			TypeSerializer<N> namespaceSerializer) {
+		super(backend, stateDesc, stateTable, keySerializer, namespaceSerializer);
+		this.reduceFunction = stateDesc.getReduceFunction();
+	}
+
+	@Override
+	public V get() {
+		Preconditions.checkState(currentNamespace != null, "No namespace set.");
+		Preconditions.checkState(backend.getCurrentKey() != null, "No key set.");
+
+		Map<N, Map<K, V>> namespaceMap =
+				stateTable.get(backend.getCurrentKeyGroupIndex());
+
+		if (namespaceMap == null) {
+			return null;
+		}
+
+		Map<K, V> keyedMap = namespaceMap.get(currentNamespace);
+
+		if (keyedMap == null) {
+			return null;
+		}
+
+		return keyedMap.get(backend.<K>getCurrentKey());
+	}
+
+	@Override
+	public void add(V value) throws IOException {
+		Preconditions.checkState(currentNamespace != null, "No namespace set.");
+		Preconditions.checkState(backend.getCurrentKey() != null, "No key set.");
+
+		if (value == null) {
+			clear();
+			return;
+		}
+
+		Map<N, Map<K, V>> namespaceMap =
+				stateTable.get(backend.getCurrentKeyGroupIndex());
+
+		if (namespaceMap == null) {
+			namespaceMap = createNewMap();
+			stateTable.set(backend.getCurrentKeyGroupIndex(), namespaceMap);
+		}
+
+		Map<K, V> keyedMap = namespaceMap.get(currentNamespace);
+
+		if (keyedMap == null) {
+			keyedMap = createNewMap();
+			namespaceMap.put(currentNamespace, keyedMap);
+		}
+
+		V currentValue = keyedMap.put(backend.<K>getCurrentKey(), value);
+
+		if (currentValue == null) {
+			// we're good, just added the new value
+		} else {
+			V reducedValue = null;
+			try {
+				reducedValue = reduceFunction.reduce(currentValue, value);
+			} catch (Exception e) {
+				throw new RuntimeException("Could not add value to reducing state.", e);
+			}
+			keyedMap.put(backend.<K>getCurrentKey(), reducedValue);
+		}
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapValueState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapValueState.java
new file mode 100644
index 0000000..cccaacb
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapValueState.java
@@ -0,0 +1,112 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.state.heap;
+
+import org.apache.flink.api.common.state.ValueState;
+import org.apache.flink.api.common.state.ValueStateDescriptor;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.runtime.state.KeyedStateBackend;
+import org.apache.flink.util.Preconditions;
+
+import java.util.Map;
+
+/**
+ * Heap-backed partitioned {@link org.apache.flink.api.common.state.ValueState} that is snapshotted
+ * into files.
+ * 
+ * @param <K> The type of the key.
+ * @param <N> The type of the namespace.
+ * @param <V> The type of the value.
+ */
+public class HeapValueState<K, N, V>
+		extends AbstractHeapState<K, N, V, ValueState<V>, ValueStateDescriptor<V>>
+		implements ValueState<V> {
+
+	/**
+	 * Creates a new key/value state for the given hash map of key/value pairs.
+	 *
+	 * @param backend The state backend backing that created this state.
+	 * @param stateDesc The state identifier for the state. This contains name
+	 *                           and can create a default state value.
+	 * @param stateTable The state tab;e to use in this kev/value state. May contain initial state.
+	 */
+	public HeapValueState(
+			KeyedStateBackend<K> backend,
+			ValueStateDescriptor<V> stateDesc,
+			StateTable<K, N, V> stateTable,
+			TypeSerializer<K> keySerializer,
+			TypeSerializer<N> namespaceSerializer) {
+		super(backend, stateDesc, stateTable, keySerializer, namespaceSerializer);
+	}
+
+	@Override
+	public V value() {
+		Preconditions.checkState(currentNamespace != null, "No namespace set.");
+		Preconditions.checkState(backend.getCurrentKey() != null, "No key set.");
+
+		Map<N, Map<K, V>> namespaceMap =
+				stateTable.get(backend.getCurrentKeyGroupIndex());
+
+		if (namespaceMap == null) {
+			return stateDesc.getDefaultValue();
+		}
+
+		Map<K, V> keyedMap = namespaceMap.get(currentNamespace);
+
+		if (keyedMap == null) {
+			return stateDesc.getDefaultValue();
+		}
+
+		V result = keyedMap.get(backend.<K>getCurrentKey());
+
+		if (result == null) {
+			return stateDesc.getDefaultValue();
+		}
+
+		return result;
+	}
+
+	@Override
+	public void update(V value) {
+		Preconditions.checkState(currentNamespace != null, "No namespace set.");
+		Preconditions.checkState(backend.getCurrentKey() != null, "No key set.");
+
+		if (value == null) {
+			clear();
+			return;
+		}
+
+		Map<N, Map<K, V>> namespaceMap =
+				stateTable.get(backend.getCurrentKeyGroupIndex());
+
+		if (namespaceMap == null) {
+			namespaceMap = createNewMap();
+			stateTable.set(backend.getCurrentKeyGroupIndex(), namespaceMap);
+		}
+
+		Map<K, V> keyedMap = namespaceMap.get(currentNamespace);
+
+		if (keyedMap == null) {
+			keyedMap = createNewMap();
+			namespaceMap.put(currentNamespace, keyedMap);
+		}
+
+		keyedMap.put(backend.<K>getCurrentKey(), value);
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/StateTable.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/StateTable.java
new file mode 100644
index 0000000..96e23d6
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/StateTable.java
@@ -0,0 +1,77 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.runtime.state.heap;
+
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.runtime.state.KeyGroupRange;
+
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+
+public class StateTable<K, N, ST> {
+
+	/** Serializer for the state value. The state value could be a List<V>, for example. */
+	protected final TypeSerializer<ST> stateSerializer;
+
+	/** The serializer for the namespace */
+	protected final TypeSerializer<N> namespaceSerializer;
+
+	/** Map for holding the actual state objects. */
+	private final List<Map<N, Map<K, ST>>> state;
+
+	protected final KeyGroupRange keyGroupRange;
+
+	public StateTable(
+			TypeSerializer<ST> stateSerializer,
+			TypeSerializer<N> namespaceSerializer,
+			KeyGroupRange keyGroupRange) {
+		this.stateSerializer = stateSerializer;
+		this.namespaceSerializer = namespaceSerializer;
+		this.keyGroupRange = keyGroupRange;
+
+		this.state = Arrays.asList((Map<N, Map<K, ST>>[]) new Map[keyGroupRange.getNumberOfKeyGroups()]);
+	}
+
+	private int indexToOffset(int index) {
+		return index - keyGroupRange.getStartKeyGroup();
+	}
+
+	public Map<N, Map<K, ST>> get(int index) {
+		return keyGroupRange.contains(index) ? state.get(indexToOffset(index)) : null;
+	}
+
+	public void set(int index, Map<N, Map<K, ST>> map) {
+		if (!keyGroupRange.contains(index)) {
+			throw new RuntimeException("Unexpected key group index. This indicates a bug.");
+		}
+		state.set(indexToOffset(index), map);
+	}
+
+	public TypeSerializer<ST> getStateSerializer() {
+		return stateSerializer;
+	}
+
+	public TypeSerializer<N> getNamespaceSerializer() {
+		return namespaceSerializer;
+	}
+
+	public List<Map<N, Map<K, ST>>> getState() {
+		return state;
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/AbstractMemState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/AbstractMemState.java
deleted file mode 100644
index cae673d..0000000
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/AbstractMemState.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.state.memory;
-
-import org.apache.flink.api.common.state.ListState;
-import org.apache.flink.api.common.state.State;
-import org.apache.flink.api.common.state.StateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.runtime.state.AbstractHeapState;
-import org.apache.flink.runtime.state.KvStateSnapshot;
-import org.apache.flink.runtime.util.DataOutputSerializer;
-
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * Base class for partitioned {@link ListState} implementations that are backed by a regular
- * heap hash map. The concrete implementations define how the state is checkpointed.
- * 
- * @param <K> The type of the key.
- * @param <N> The type of the namespace.
- * @param <SV> The type of the values in the state.
- * @param <S> The type of State
- * @param <SD> The type of StateDescriptor for the State S
- */
-public abstract class AbstractMemState<K, N, SV, S extends State, SD extends StateDescriptor<S, ?>>
-		extends AbstractHeapState<K, N, SV, S, SD, MemoryStateBackend> {
-
-	public AbstractMemState(TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		TypeSerializer<SV> stateSerializer,
-		SD stateDesc) {
-		super(keySerializer, namespaceSerializer, stateSerializer, stateDesc);
-	}
-
-	public AbstractMemState(TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		TypeSerializer<SV> stateSerializer,
-		SD stateDesc,
-		HashMap<N, Map<K, SV>> state) {
-		super(keySerializer, namespaceSerializer, stateSerializer, stateDesc, state);
-	}
-
-	public abstract KvStateSnapshot<K, N, S, SD, MemoryStateBackend> createHeapSnapshot(byte[] bytes);
-
-	@Override
-	public KvStateSnapshot<K, N, S, SD, MemoryStateBackend> snapshot(long checkpointId, long timestamp) throws Exception {
-
-		DataOutputSerializer out = new DataOutputSerializer(Math.max(size() * 16, 16));
-
-		out.writeInt(state.size());
-		for (Map.Entry<N, Map<K, SV>> namespaceState: state.entrySet()) {
-			N namespace = namespaceState.getKey();
-			namespaceSerializer.serialize(namespace, out);
-			out.writeInt(namespaceState.getValue().size());
-			for (Map.Entry<K, SV> entry: namespaceState.getValue().entrySet()) {
-				keySerializer.serialize(entry.getKey(), out);
-				stateSerializer.serialize(entry.getValue(), out);
-			}
-		}
-
-		byte[] bytes = out.getCopyOfBuffer();
-
-		return createHeapSnapshot(bytes);
-	}
-}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/AbstractMemStateSnapshot.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/AbstractMemStateSnapshot.java
deleted file mode 100644
index e1b62d2..0000000
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/AbstractMemStateSnapshot.java
+++ /dev/null
@@ -1,144 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.state.memory;
-
-import org.apache.flink.api.common.state.State;
-import org.apache.flink.api.common.state.StateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.runtime.state.KvState;
-import org.apache.flink.runtime.state.KvStateSnapshot;
-import org.apache.flink.runtime.util.DataInputDeserializer;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * A snapshot of a {@link MemValueState} for a checkpoint. The data is stored in a heap byte
- * array, in serialized form.
- * 
- * @param <K> The type of the key in the snapshot state.
- * @param <N> The type of the namespace in the snapshot state.
- * @param <SV> The type of the value in the snapshot state.
- */
-public abstract class AbstractMemStateSnapshot<K, N, SV, S extends State, SD extends StateDescriptor<S, ?>> 
-		implements KvStateSnapshot<K, N, S, SD, MemoryStateBackend> {
-
-	private static final long serialVersionUID = 1L;
-
-	/** Key Serializer */
-	protected final TypeSerializer<K> keySerializer;
-
-	/** Namespace Serializer */
-	protected final TypeSerializer<N> namespaceSerializer;
-
-	/** Serializer for the state value */
-	protected final TypeSerializer<SV> stateSerializer;
-
-	/** StateDescriptor, for sanity checks */
-	protected final SD stateDesc;
-
-	/** The serialized data of the state key/value pairs */
-	private final byte[] data;
-	
-	private transient boolean closed;
-
-	/**
-	 * Creates a new heap memory state snapshot.
-	 *
-	 * @param keySerializer The serializer for the keys.
-	 * @param namespaceSerializer The serializer for the namespace.
-	 * @param stateSerializer The serializer for the elements in the state HashMap
-	 * @param stateDesc The state identifier
-	 * @param data The serialized data of the state key/value pairs
-	 */
-	public AbstractMemStateSnapshot(TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		TypeSerializer<SV> stateSerializer,
-		SD stateDesc,
-		byte[] data) {
-		this.keySerializer = keySerializer;
-		this.namespaceSerializer = namespaceSerializer;
-		this.stateSerializer = stateSerializer;
-		this.stateDesc = stateDesc;
-		this.data = data;
-	}
-
-	public abstract KvState<K, N, S, SD, MemoryStateBackend> createMemState(HashMap<N, Map<K, SV>> stateMap);
-
-	@Override
-	public KvState<K, N, S, SD, MemoryStateBackend> restoreState(
-		MemoryStateBackend stateBackend,
-		final TypeSerializer<K> keySerializer,
-		ClassLoader classLoader) throws Exception {
-
-		// validity checks
-		if (!this.keySerializer.equals(keySerializer)) {
-			throw new IllegalArgumentException(
-				"Cannot restore the state from the snapshot with the given serializers. " +
-					"State (K/V) was serialized with " +
-					"(" + this.keySerializer + ") " +
-					"now is (" + keySerializer + ")");
-		}
-
-		if (closed) {
-			throw new IOException("snapshot has been closed");
-		}
-
-		// restore state
-		DataInputDeserializer inView = new DataInputDeserializer(data, 0, data.length);
-
-		final int numKeys = inView.readInt();
-		HashMap<N, Map<K, SV>> stateMap = new HashMap<>(numKeys);
-
-		for (int i = 0; i < numKeys && !closed; i++) {
-			N namespace = namespaceSerializer.deserialize(inView);
-			final int numValues = inView.readInt();
-			Map<K, SV> namespaceMap = new HashMap<>(numValues);
-			stateMap.put(namespace, namespaceMap);
-			for (int j = 0; j < numValues; j++) {
-				K key = keySerializer.deserialize(inView);
-				SV value = stateSerializer.deserialize(inView);
-				namespaceMap.put(key, value);
-			}
-		}
-
-		if (closed) {
-			throw new IOException("snapshot has been closed");
-		}
-
-		return createMemState(stateMap);
-	}
-
-	/**
-	 * Discarding the heap state is a no-op.
-	 */
-	@Override
-	public void discardState() {}
-
-	@Override
-	public long getStateSize() {
-		return data.length;
-	}
-
-	@Override
-	public void close() {
-		closed = true;
-	}
-}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/ByteStreamStateHandle.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/ByteStreamStateHandle.java
index a42bec2..b9ff255 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/ByteStreamStateHandle.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/ByteStreamStateHandle.java
@@ -51,7 +51,7 @@ public class ByteStreamStateHandle extends AbstractCloseableHandle implements St
 	}
 
 	@Override
-	public FSDataInputStream openInputStream() throws Exception {
+	public FSDataInputStream openInputStream() throws IOException {
 		ensureNotClosed();
 
 		FSDataInputStream inputStream = new FSDataInputStream() {
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/MemCheckpointStreamFactory.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/MemCheckpointStreamFactory.java
new file mode 100644
index 0000000..4801d85
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/MemCheckpointStreamFactory.java
@@ -0,0 +1,146 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.state.memory;
+
+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;
+import org.apache.flink.runtime.state.CheckpointStreamFactory;
+import org.apache.flink.runtime.state.StreamStateHandle;
+
+import java.io.IOException;
+
+/**
+ * {@link CheckpointStreamFactory} that produces streams that write to in-memory byte arrays.
+ */
+public class MemCheckpointStreamFactory implements CheckpointStreamFactory {
+
+	/** The maximal size that the snapshotted memory state may have */
+	private final int maxStateSize;
+
+	/**
+	 * Creates a new in-memory stream factory that accepts states whose serialized forms are
+	 * up to the given number of bytes.
+	 *
+	 * @param maxStateSize The maximal size of the serialized state
+	 */
+	public MemCheckpointStreamFactory(int maxStateSize) {
+		this.maxStateSize = maxStateSize;
+	}
+
+	@Override
+	public void close() throws Exception {}
+
+	@Override
+	public CheckpointStateOutputStream createCheckpointStateOutputStream(
+			long checkpointID, long timestamp) throws Exception
+	{
+		return new MemoryCheckpointOutputStream(maxStateSize);
+	}
+
+	@Override
+	public String toString() {
+		return "In-Memory Stream Factory";
+	}
+
+	static void checkSize(int size, int maxSize) throws IOException {
+		if (size > maxSize) {
+			throw new IOException(
+					"Size of the state is larger than the maximum permitted memory-backed state. Size="
+							+ size + " , maxSize=" + maxSize
+							+ " . Consider using a different state backend, like the File System State backend.");
+		}
+	}
+
+
+
+	/**
+	 * A {@code CheckpointStateOutputStream} that writes into a byte array.
+	 */
+	public static final class MemoryCheckpointOutputStream extends CheckpointStateOutputStream {
+
+		private final ByteArrayOutputStreamWithPos os = new ByteArrayOutputStreamWithPos();
+
+		private final int maxSize;
+
+		private boolean closed;
+
+		boolean isEmpty = true;
+
+		public MemoryCheckpointOutputStream(int maxSize) {
+			this.maxSize = maxSize;
+		}
+
+		@Override
+		public void write(int b) {
+			os.write(b);
+			isEmpty = false;
+		}
+
+		@Override
+		public void write(byte[] b, int off, int len) {
+			os.write(b, off, len);
+			isEmpty = false;
+		}
+
+		@Override
+		public void flush() throws IOException {
+			os.flush();
+		}
+
+		@Override
+		public void sync() throws IOException { }
+
+		// --------------------------------------------------------------------
+
+		@Override
+		public void close() {
+			closed = true;
+			os.reset();
+		}
+
+		@Override
+		public StreamStateHandle closeAndGetHandle() throws IOException {
+			if (isEmpty) {
+				return null;
+			}
+			return new ByteStreamStateHandle(closeAndGetBytes());
+		}
+
+		@Override
+		public long getPos() throws IOException {
+			return os.getPosition();
+		}
+
+		/**
+		 * Closes the stream and returns the byte array containing the stream's data.
+		 * @return The byte array containing the stream's data.
+		 * @throws IOException Thrown if the size of the data exceeds the maximal
+		 */
+		public byte[] closeAndGetBytes() throws IOException {
+			if (!closed) {
+				checkSize(os.size(), maxSize);
+				byte[] bytes = os.toByteArray();
+				close();
+				return bytes;
+			}
+			else {
+				throw new IllegalStateException("stream has already been closed");
+			}
+		}
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/MemFoldingState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/MemFoldingState.java
deleted file mode 100644
index a4dec3b..0000000
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/MemFoldingState.java
+++ /dev/null
@@ -1,135 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.state.memory;
-
-import org.apache.flink.api.common.functions.FoldFunction;
-import org.apache.flink.api.common.state.FoldingState;
-import org.apache.flink.api.common.state.FoldingStateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.runtime.query.netty.message.KvStateRequestSerializer;
-import org.apache.flink.runtime.state.KvState;
-import org.apache.flink.runtime.state.KvStateSnapshot;
-import org.apache.flink.util.Preconditions;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * Heap-backed partitioned {@link FoldingState} that is
- * snapshotted into a serialized memory copy.
- *
- * @param <K> The type of the key.
- * @param <N> The type of the namespace.
- * @param <T> The type of the values that can be folded into the state.
- * @param <ACC> The type of the value in the folding state.
- */
-public class MemFoldingState<K, N, T, ACC>
-	extends AbstractMemState<K, N, ACC, FoldingState<T, ACC>, FoldingStateDescriptor<T, ACC>>
-	implements FoldingState<T, ACC> {
-
-	private final FoldFunction<T, ACC> foldFunction;
-
-	public MemFoldingState(TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		FoldingStateDescriptor<T, ACC> stateDesc) {
-		super(keySerializer, namespaceSerializer, stateDesc.getSerializer(), stateDesc);
-		this.foldFunction = stateDesc.getFoldFunction();
-	}
-
-	public MemFoldingState(TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		FoldingStateDescriptor<T, ACC> stateDesc,
-		HashMap<N, Map<K, ACC>> state) {
-		super(keySerializer, namespaceSerializer, stateDesc.getSerializer(), stateDesc, state);
-		this.foldFunction = stateDesc.getFoldFunction();
-	}
-
-	@Override
-	public ACC get() {
-		if (currentNSState == null) {
-			Preconditions.checkState(currentNamespace != null, "No namespace set");
-			currentNSState = state.get(currentNamespace);
-		}
-		if (currentNSState != null) {
-			Preconditions.checkState(currentKey != null, "No key set");
-			return currentNSState.get(currentKey);
-		} else {
-			return null;
-		}
-	}
-
-	@Override
-	public void add(T value) throws IOException {
-		Preconditions.checkState(currentKey != null, "No key set");
-
-		if (currentNSState == null) {
-			Preconditions.checkState(currentNamespace != null, "No namespace set");
-			currentNSState = createNewNamespaceMap();
-			state.put(currentNamespace, currentNSState);
-		}
-
-		ACC currentValue = currentNSState.get(currentKey);
-		try {
-			if (currentValue == null) {
-				currentNSState.put(currentKey, foldFunction.fold(stateDesc.getDefaultValue(), value));
-			} else {
-					currentNSState.put(currentKey, foldFunction.fold(currentValue, value));
-
-			}
-		} catch (Exception e) {
-			throw new RuntimeException("Could not add value to folding state.", e);
-		}
-	}
-
-	@Override
-	public KvStateSnapshot<K, N, FoldingState<T, ACC>, FoldingStateDescriptor<T, ACC>, MemoryStateBackend> createHeapSnapshot(byte[] bytes) {
-		return new Snapshot<>(getKeySerializer(), getNamespaceSerializer(), stateSerializer, stateDesc, bytes);
-	}
-
-	@Override
-	public byte[] getSerializedValue(K key, N namespace) throws Exception {
-		Preconditions.checkNotNull(key, "Key");
-		Preconditions.checkNotNull(namespace, "Namespace");
-
-		Map<K, ACC> stateByKey = state.get(namespace);
-
-		if (stateByKey != null) {
-			return KvStateRequestSerializer.serializeValue(stateByKey.get(key), stateDesc.getSerializer());
-		} else {
-			return null;
-		}
-	}
-
-	public static class Snapshot<K, N, T, ACC> extends AbstractMemStateSnapshot<K, N, ACC, FoldingState<T, ACC>, FoldingStateDescriptor<T, ACC>> {
-		private static final long serialVersionUID = 1L;
-
-		public Snapshot(TypeSerializer<K> keySerializer,
-			TypeSerializer<N> namespaceSerializer,
-			TypeSerializer<ACC> stateSerializer,
-			FoldingStateDescriptor<T, ACC> stateDescs, byte[] data) {
-			super(keySerializer, namespaceSerializer, stateSerializer, stateDescs, data);
-		}
-
-		@Override
-		public KvState<K, N, FoldingState<T, ACC>, FoldingStateDescriptor<T, ACC>, MemoryStateBackend> createMemState(HashMap<N, Map<K, ACC>> stateMap) {
-			return new MemFoldingState<>(keySerializer, namespaceSerializer, stateDesc, stateMap);
-		}
-	}
-}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/MemListState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/MemListState.java
deleted file mode 100644
index 20b6eb5..0000000
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/MemListState.java
+++ /dev/null
@@ -1,120 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.state.memory;
-
-import org.apache.flink.api.common.state.ListState;
-import org.apache.flink.api.common.state.ListStateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.runtime.query.netty.message.KvStateRequestSerializer;
-import org.apache.flink.runtime.state.ArrayListSerializer;
-import org.apache.flink.runtime.state.KvState;
-import org.apache.flink.runtime.state.KvStateSnapshot;
-import org.apache.flink.util.Preconditions;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * Heap-backed partitioned {@link org.apache.flink.api.common.state.ListState} that is snapshotted
- * into a serialized memory copy.
- *
- * @param <K> The type of the key.
- * @param <N> The type of the namespace.
- * @param <V> The type of the values in the list state.
- */
-public class MemListState<K, N, V>
-	extends AbstractMemState<K, N, ArrayList<V>, ListState<V>, ListStateDescriptor<V>>
-	implements ListState<V> {
-
-	public MemListState(TypeSerializer<K> keySerializer, TypeSerializer<N> namespaceSerializer, ListStateDescriptor<V> stateDesc) {
-		super(keySerializer, namespaceSerializer, new ArrayListSerializer<>(stateDesc.getSerializer()), stateDesc);
-	}
-
-	public MemListState(TypeSerializer<K> keySerializer, TypeSerializer<N> namespaceSerializer, ListStateDescriptor<V> stateDesc, HashMap<N, Map<K, ArrayList<V>>> state) {
-		super(keySerializer, namespaceSerializer, new ArrayListSerializer<>(stateDesc.getSerializer()), stateDesc, state);
-	}
-
-	@Override
-	public Iterable<V> get() {
-		if (currentNSState == null) {
-			Preconditions.checkState(currentNamespace != null, "No namespace set");
-			currentNSState = state.get(currentNamespace);
-		}
-		if (currentNSState != null) {
-			Preconditions.checkState(currentKey != null, "No key set");
-			return currentNSState.get(currentKey);
-		} else {
-			return null;
-		}
-	}
-
-	@Override
-	public void add(V value) {
-		Preconditions.checkState(currentKey != null, "No key set");
-
-		if (currentNSState == null) {
-			Preconditions.checkState(currentNamespace != null, "No namespace set");
-			currentNSState = createNewNamespaceMap();
-			state.put(currentNamespace, currentNSState);
-		}
-
-		ArrayList<V> list = currentNSState.get(currentKey);
-		if (list == null) {
-			list = new ArrayList<>();
-			currentNSState.put(currentKey, list);
-		}
-		list.add(value);
-	}
-
-	@Override
-	public KvStateSnapshot<K, N, ListState<V>, ListStateDescriptor<V>, MemoryStateBackend> createHeapSnapshot(byte[] bytes) {
-		return new Snapshot<>(getKeySerializer(), getNamespaceSerializer(), stateSerializer, stateDesc, bytes);
-	}
-
-	@Override
-	public byte[] getSerializedValue(K key, N namespace) throws Exception {
-		Preconditions.checkNotNull(key, "Key");
-		Preconditions.checkNotNull(namespace, "Namespace");
-
-		Map<K, ArrayList<V>> stateByKey = state.get(namespace);
-		if (stateByKey != null) {
-			return KvStateRequestSerializer.serializeList(stateByKey.get(key), stateDesc.getSerializer());
-		} else {
-			return null;
-		}
-	}
-
-	public static class Snapshot<K, N, V> extends AbstractMemStateSnapshot<K, N, ArrayList<V>, ListState<V>, ListStateDescriptor<V>> {
-		private static final long serialVersionUID = 1L;
-
-		public Snapshot(TypeSerializer<K> keySerializer,
-			TypeSerializer<N> namespaceSerializer,
-			TypeSerializer<ArrayList<V>> stateSerializer,
-			ListStateDescriptor<V> stateDescs, byte[] data) {
-			super(keySerializer, namespaceSerializer, stateSerializer, stateDescs, data);
-		}
-
-		@Override
-		public KvState<K, N, ListState<V>, ListStateDescriptor<V>, MemoryStateBackend> createMemState(HashMap<N, Map<K, ArrayList<V>>> stateMap) {
-			return new MemListState<>(keySerializer, namespaceSerializer, stateDesc, stateMap);
-		}
-	}
-
-}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/MemReducingState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/MemReducingState.java
deleted file mode 100644
index 9a4c676..0000000
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/MemReducingState.java
+++ /dev/null
@@ -1,139 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.state.memory;
-
-import org.apache.flink.api.common.functions.ReduceFunction;
-import org.apache.flink.api.common.state.ReducingState;
-import org.apache.flink.api.common.state.ReducingStateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.runtime.query.netty.message.KvStateRequestSerializer;
-import org.apache.flink.runtime.state.KvState;
-import org.apache.flink.runtime.state.KvStateSnapshot;
-import org.apache.flink.util.Preconditions;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * Heap-backed partitioned {@link org.apache.flink.api.common.state.ReducingState} that is
- * snapshotted into a serialized memory copy.
- *
- * @param <K> The type of the key.
- * @param <N> The type of the namespace.
- * @param <V> The type of the values in the list state.
- */
-public class MemReducingState<K, N, V>
-	extends AbstractMemState<K, N, V, ReducingState<V>, ReducingStateDescriptor<V>>
-	implements ReducingState<V> {
-
-	private final ReduceFunction<V> reduceFunction;
-
-	public MemReducingState(TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		ReducingStateDescriptor<V> stateDesc) {
-		super(keySerializer, namespaceSerializer, stateDesc.getSerializer(), stateDesc);
-		this.reduceFunction = stateDesc.getReduceFunction();
-	}
-
-	public MemReducingState(TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		ReducingStateDescriptor<V> stateDesc,
-		HashMap<N, Map<K, V>> state) {
-		super(keySerializer, namespaceSerializer, stateDesc.getSerializer(), stateDesc, state);
-		this.reduceFunction = stateDesc.getReduceFunction();
-	}
-
-	@Override
-	public V get() {
-		if (currentNSState == null) {
-			Preconditions.checkState(currentNamespace != null, "No namespace set");
-			currentNSState = state.get(currentNamespace);
-		}
-		if (currentNSState != null) {
-			Preconditions.checkState(currentKey != null, "No key set");
-			return currentNSState.get(currentKey);
-		}
-		return null;
-	}
-
-	@Override
-	public void add(V value) throws IOException {
-		Preconditions.checkState(currentKey != null, "No key set");
-
-		if (currentNSState == null) {
-			Preconditions.checkState(currentNamespace != null, "No namespace set");
-			currentNSState = createNewNamespaceMap();
-			state.put(currentNamespace, currentNSState);
-		}
-//		currentKeyState.merge(currentNamespace, value, new BiFunction<V, V, V>() {
-//			@Override
-//			public V apply(V v, V v2) {
-//				try {
-//					return reduceFunction.reduce(v, v2);
-//				} catch (Exception e) {
-//					return null;
-//				}
-//			}
-//		});
-		V currentValue = currentNSState.get(currentKey);
-		if (currentValue == null) {
-			currentNSState.put(currentKey, value);
-		} else {
-			try {
-				currentNSState.put(currentKey, reduceFunction.reduce(currentValue, value));
-			} catch (Exception e) {
-				throw new RuntimeException("Could not add value to reducing state.", e);
-			}
-		}
-	}
-
-	@Override
-	public KvStateSnapshot<K, N, ReducingState<V>, ReducingStateDescriptor<V>, MemoryStateBackend> createHeapSnapshot(byte[] bytes) {
-		return new Snapshot<>(getKeySerializer(), getNamespaceSerializer(), stateSerializer, stateDesc, bytes);
-	}
-
-	@Override
-	public byte[] getSerializedValue(K key, N namespace) throws Exception {
-		Preconditions.checkNotNull(key, "Key");
-		Preconditions.checkNotNull(namespace, "Namespace");
-
-		Map<K, V> stateByKey = state.get(namespace);
-		if (stateByKey != null) {
-			return KvStateRequestSerializer.serializeValue(stateByKey.get(key), stateDesc.getSerializer());
-		} else {
-			return null;
-		}
-	}
-
-	public static class Snapshot<K, N, V> extends AbstractMemStateSnapshot<K, N, V, ReducingState<V>, ReducingStateDescriptor<V>> {
-		private static final long serialVersionUID = 1L;
-
-		public Snapshot(TypeSerializer<K> keySerializer,
-			TypeSerializer<N> namespaceSerializer,
-			TypeSerializer<V> stateSerializer,
-			ReducingStateDescriptor<V> stateDescs, byte[] data) {
-			super(keySerializer, namespaceSerializer, stateSerializer, stateDescs, data);
-		}
-
-		@Override
-		public KvState<K, N, ReducingState<V>, ReducingStateDescriptor<V>, MemoryStateBackend> createMemState(HashMap<N, Map<K, V>> stateMap) {
-			return new MemReducingState<>(keySerializer, namespaceSerializer, stateDesc, stateMap);
-		}
-	}}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/MemValueState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/MemValueState.java
deleted file mode 100644
index c0e3779..0000000
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/MemValueState.java
+++ /dev/null
@@ -1,122 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.runtime.state.memory;
-
-import org.apache.flink.api.common.state.ValueState;
-import org.apache.flink.api.common.state.ValueStateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.runtime.query.netty.message.KvStateRequestSerializer;
-import org.apache.flink.runtime.state.KvState;
-import org.apache.flink.runtime.state.KvStateSnapshot;
-import org.apache.flink.util.Preconditions;
-
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * Heap-backed key/value state that is snapshotted into a serialized memory copy.
- *
- * @param <K> The type of the key.
- * @param <N> The type of the namespace.
- * @param <V> The type of the value.
- */
-public class MemValueState<K, N, V>
-	extends AbstractMemState<K, N, V, ValueState<V>, ValueStateDescriptor<V>>
-	implements ValueState<V> {
-	
-	public MemValueState(TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		ValueStateDescriptor<V> stateDesc) {
-		super(keySerializer, namespaceSerializer, stateDesc.getSerializer(), stateDesc);
-	}
-
-	public MemValueState(TypeSerializer<K> keySerializer,
-		TypeSerializer<N> namespaceSerializer,
-		ValueStateDescriptor<V> stateDesc,
-		HashMap<N, Map<K, V>> state) {
-		super(keySerializer, namespaceSerializer, stateDesc.getSerializer(), stateDesc, state);
-	}
-
-	@Override
-	public V value() {
-		if (currentNSState == null) {
-			Preconditions.checkState(currentNamespace != null, "No namespace set");
-			currentNSState = state.get(currentNamespace);
-		}
-		if (currentNSState != null) {
-			Preconditions.checkState(currentKey != null, "No key set");
-			V value = currentNSState.get(currentKey);
-			return value != null ? value : stateDesc.getDefaultValue();
-		}
-		return stateDesc.getDefaultValue();
-	}
-
-	@Override
-	public void update(V value) {
-		Preconditions.checkState(currentKey != null, "No key set");
-
-		if (value == null) {
-			clear();
-			return;
-		}
-
-		if (currentNSState == null) {
-			Preconditions.checkState(currentNamespace != null, "No namespace set");
-			currentNSState = createNewNamespaceMap();
-			state.put(currentNamespace, currentNSState);
-		}
-
-		currentNSState.put(currentKey, value);
-	}
-
-	@Override
-	public KvStateSnapshot<K, N, ValueState<V>, ValueStateDescriptor<V>, MemoryStateBackend> createHeapSnapshot(byte[] bytes) {
-		return new Snapshot<>(getKeySerializer(), getNamespaceSerializer(), stateSerializer, stateDesc, bytes);
-	}
-
-	@Override
-	public byte[] getSerializedValue(K key, N namespace) throws Exception {
-		Preconditions.checkNotNull(key, "Key");
-		Preconditions.checkNotNull(namespace, "Namespace");
-
-		Map<K, V> stateByKey = state.get(namespace);
-		V value = stateByKey != null ? stateByKey.get(key) : stateDesc.getDefaultValue();
-		if (value != null) {
-			return KvStateRequestSerializer.serializeValue(value, stateDesc.getSerializer());
-		} else {
-			return KvStateRequestSerializer.serializeValue(stateDesc.getDefaultValue(), stateDesc.getSerializer());
-		}
-	}
-
-	public static class Snapshot<K, N, V> extends AbstractMemStateSnapshot<K, N, V, ValueState<V>, ValueStateDescriptor<V>> {
-		private static final long serialVersionUID = 1L;
-
-		public Snapshot(TypeSerializer<K> keySerializer,
-			TypeSerializer<N> namespaceSerializer,
-			TypeSerializer<V> stateSerializer,
-			ValueStateDescriptor<V> stateDescs, byte[] data) {
-			super(keySerializer, namespaceSerializer, stateSerializer, stateDescs, data);
-		}
-
-		@Override
-		public KvState<K, N, ValueState<V>, ValueStateDescriptor<V>, MemoryStateBackend> createMemState(HashMap<N, Map<K, V>> stateMap) {
-			return new MemValueState<>(keySerializer, namespaceSerializer, stateDesc, stateMap);
-		}
-	}
-}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/MemoryStateBackend.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/MemoryStateBackend.java
index af84394..654c367 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/MemoryStateBackend.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/MemoryStateBackend.java
@@ -18,21 +18,20 @@
 
 package org.apache.flink.runtime.state.memory;
 
-import org.apache.flink.api.common.state.FoldingState;
-import org.apache.flink.api.common.state.FoldingStateDescriptor;
-import org.apache.flink.api.common.state.ListState;
-import org.apache.flink.api.common.state.ListStateDescriptor;
-import org.apache.flink.api.common.state.ReducingState;
-import org.apache.flink.api.common.state.ReducingStateDescriptor;
-import org.apache.flink.api.common.state.ValueState;
-import org.apache.flink.api.common.state.ValueStateDescriptor;
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.api.common.state.KeyGroupAssigner;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.runtime.execution.Environment;
+import org.apache.flink.runtime.query.TaskKvStateRegistry;
 import org.apache.flink.runtime.state.AbstractStateBackend;
-import org.apache.flink.runtime.state.StreamStateHandle;
+import org.apache.flink.runtime.state.CheckpointStreamFactory;
+import org.apache.flink.runtime.state.KeyGroupRange;
+import org.apache.flink.runtime.state.KeyGroupsStateHandle;
+import org.apache.flink.runtime.state.KeyedStateBackend;
+import org.apache.flink.runtime.state.heap.HeapKeyedStateBackend;
 
-
-import java.io.ByteArrayOutputStream;
 import java.io.IOException;
+import java.util.List;
 
 /**
  * A {@link AbstractStateBackend} that stores all its data and checkpoints in memory and has no
@@ -67,142 +66,47 @@ public class MemoryStateBackend extends AbstractStateBackend {
 		this.maxStateSize = maxStateSize;
 	}
 
-	// ------------------------------------------------------------------------
-	//  initialization and cleanup
-	// ------------------------------------------------------------------------
-
-	@Override
-	public void disposeAllStateForCurrentJob() {
-		// nothing to do here, GC will do it
-	}
-
-	@Override
-	public void close() throws Exception {}
-
-	// ------------------------------------------------------------------------
-	//  State backend operations
-	// ------------------------------------------------------------------------
-
 	@Override
-	public <N, V> ValueState<V> createValueState(TypeSerializer<N> namespaceSerializer, ValueStateDescriptor<V> stateDesc) throws Exception {
-		return new MemValueState<>(keySerializer, namespaceSerializer, stateDesc);
-	}
-
-	@Override
-	public <N, T> ListState<T> createListState(TypeSerializer<N> namespaceSerializer, ListStateDescriptor<T> stateDesc) throws Exception {
-		return new MemListState<>(keySerializer, namespaceSerializer, stateDesc);
-	}
-
-	@Override
-	public <N, T> ReducingState<T> createReducingState(TypeSerializer<N> namespaceSerializer, ReducingStateDescriptor<T> stateDesc) throws Exception {
-		return new MemReducingState<>(keySerializer, namespaceSerializer, stateDesc);
+	public String toString() {
+		return "MemoryStateBackend (data in heap memory / checkpoints to JobManager)";
 	}
 
 	@Override
-	public <N, T, ACC> FoldingState<T, ACC> createFoldingState(TypeSerializer<N> namespaceSerializer, FoldingStateDescriptor<T, ACC> stateDesc) throws Exception {
-		return new MemFoldingState<>(keySerializer, namespaceSerializer, stateDesc);
+	public CheckpointStreamFactory createStreamFactory(JobID jobId, String operatorIdentifier) throws IOException {
+		return new MemCheckpointStreamFactory(maxStateSize);
 	}
 
 	@Override
-	public CheckpointStateOutputStream createCheckpointStateOutputStream(
-			long checkpointID, long timestamp) throws Exception
-	{
-		return new MemoryCheckpointOutputStream(maxStateSize);
+	public <K> KeyedStateBackend<K> createKeyedStateBackend(
+			Environment env, JobID jobID,
+			String operatorIdentifier, TypeSerializer<K> keySerializer,
+			KeyGroupAssigner<K> keyGroupAssigner,
+			KeyGroupRange keyGroupRange,
+			TaskKvStateRegistry kvStateRegistry) throws IOException {
+
+		return new HeapKeyedStateBackend<>(
+				kvStateRegistry,
+				keySerializer,
+				keyGroupAssigner,
+				keyGroupRange);
 	}
 
-	// ------------------------------------------------------------------------
-	//  Utilities
-	// ------------------------------------------------------------------------
-
 	@Override
-	public String toString() {
-		return "MemoryStateBackend (data in heap memory / checkpoints to JobManager)";
+	public <K> KeyedStateBackend<K> restoreKeyedStateBackend(
+			Environment env, JobID jobID,
+			String operatorIdentifier,
+			TypeSerializer<K> keySerializer,
+			KeyGroupAssigner<K> keyGroupAssigner,
+			KeyGroupRange keyGroupRange,
+			List<KeyGroupsStateHandle> restoredState,
+			TaskKvStateRegistry kvStateRegistry) throws Exception {
+
+		return new HeapKeyedStateBackend<>(
+				kvStateRegistry,
+				keySerializer,
+				keyGroupAssigner,
+				keyGroupRange,
+				restoredState);
 	}
 
-	static void checkSize(int size, int maxSize) throws IOException {
-		if (size > maxSize) {
-			throw new IOException(
-					"Size of the state is larger than the maximum permitted memory-backed state. Size="
-							+ size + " , maxSize=" + maxSize
-							+ " . Consider using a different state backend, like the File System State backend.");
-		}
-	}
-
-	// ------------------------------------------------------------------------
-
-	/**
-	 * A CheckpointStateOutputStream that writes into a byte array.
-	 */
-	public static final class MemoryCheckpointOutputStream extends CheckpointStateOutputStream {
-
-		private final ByteArrayOutputStream os = new ByteArrayOutputStream();
-
-		private final int maxSize;
-
-		private boolean closed;
-
-		public MemoryCheckpointOutputStream(int maxSize) {
-			this.maxSize = maxSize;
-		}
-
-		@Override
-		public void write(int b) {
-			os.write(b);
-		}
-
-		@Override
-		public void write(byte[] b, int off, int len) {
-			os.write(b, off, len);
-		}
-
-		@Override
-		public void flush() throws IOException {
-			os.flush();
-		}
-
-		@Override
-		public void sync() throws IOException { }
-
-		// --------------------------------------------------------------------
-
-		@Override
-		public void close() {
-			closed = true;
-			os.reset();
-		}
-
-		@Override
-		public StreamStateHandle closeAndGetHandle() throws IOException {
-			return new ByteStreamStateHandle(closeAndGetBytes());
-		}
-
-		/**
-		 * Closes the stream and returns the byte array containing the stream's data.
-		 * @return The byte array containing the stream's data.
-		 * @throws IOException Thrown if the size of the data exceeds the maximal
-		 */
-		public byte[] closeAndGetBytes() throws IOException {
-			if (!closed) {
-				checkSize(os.size(), maxSize);
-				byte[] bytes = os.toByteArray();
-				close();
-				return bytes;
-			}
-			else {
-				throw new IllegalStateException("stream has already been closed");
-			}
-		}
-	}
-
-	// ------------------------------------------------------------------------
-	//  Static default instance
-	// ------------------------------------------------------------------------
-
-	/**
-	 * Gets the default instance of this state backend, using the default maximal state size.
-	 * @return The default instance of this state backend.
-	 */
-	public static MemoryStateBackend create() {
-		return new MemoryStateBackend();
-	}
 }
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointLoaderTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointLoaderTest.java
index d703bd6..766531a 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointLoaderTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointLoaderTest.java
@@ -87,14 +87,14 @@ public class SavepointLoaderTest {
 		loaded.discardState();
 		verify(state, times(0)).discardState();
 
-		// 2) Load and validate: parallelism mismatch
-		when(vertex.getParallelism()).thenReturn(222);
+		// 2) Load and validate: max parallelism mismatch
+		when(vertex.getMaxParallelism()).thenReturn(222);
 
 		try {
 			SavepointLoader.loadAndValidateSavepoint(jobId, tasks, store, path);
 			fail("Did not throw expected Exception");
 		} catch (IllegalStateException expected) {
-			assertTrue(expected.getMessage().contains("Parallelism mismatch"));
+			assertTrue(expected.getMessage().contains("Max parallelism mismatch"));
 		}
 
 		// 3) Load and validate: missing vertex (this should be relaxed)
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/deployment/TaskDeploymentDescriptorTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/deployment/TaskDeploymentDescriptorTest.java
index 56da9c8..39ea176 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/deployment/TaskDeploymentDescriptorTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/deployment/TaskDeploymentDescriptorTest.java
@@ -48,6 +48,7 @@ public class TaskDeploymentDescriptorTest {
 			final ExecutionAttemptID execId = new ExecutionAttemptID();
 			final String jobName = "job name";
 			final String taskName = "task name";
+			final int numberOfKeyGroups = 1;
 			final int indexInSubtaskGroup = 0;
 			final int currentNumberOfSubtasks = 1;
 			final int attemptNumber = 0;
@@ -61,7 +62,7 @@ public class TaskDeploymentDescriptorTest {
 			final SerializedValue<ExecutionConfig> executionConfig = new SerializedValue<>(new ExecutionConfig());
 
 			final TaskDeploymentDescriptor orig = new TaskDeploymentDescriptor(jobID, jobName, vertexID, execId,
-				executionConfig, taskName, indexInSubtaskGroup, currentNumberOfSubtasks, attemptNumber,
+				executionConfig, taskName, numberOfKeyGroups, indexInSubtaskGroup, currentNumberOfSubtasks, attemptNumber,
 				jobConfiguration, taskConfiguration, invokableClass.getName(), producedResults, inputGates,
 				requiredJars, requiredClasspaths, 47);
 	
@@ -76,6 +77,7 @@ public class TaskDeploymentDescriptorTest {
 			assertEquals(orig.getJobID(), copy.getJobID());
 			assertEquals(orig.getVertexID(), copy.getVertexID());
 			assertEquals(orig.getTaskName(), copy.getTaskName());
+			assertEquals(orig.getNumberOfKeyGroups(), copy.getNumberOfKeyGroups());
 			assertEquals(orig.getIndexInSubtaskGroup(), copy.getIndexInSubtaskGroup());
 			assertEquals(orig.getNumberOfSubtasks(), copy.getNumberOfSubtasks());
 			assertEquals(orig.getAttemptNumber(), copy.getAttemptNumber());
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/messages/CheckpointMessagesTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/messages/CheckpointMessagesTest.java
index c6eb249..6a6ac64 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/messages/CheckpointMessagesTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/messages/CheckpointMessagesTest.java
@@ -120,7 +120,7 @@ public class CheckpointMessagesTest {
 		public void close() throws IOException {}
 
 		@Override
-		public FSDataInputStream openInputStream() throws Exception {
+		public FSDataInputStream openInputStream() throws IOException {
 			return null;
 		}
 	}
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/metrics/groups/TaskManagerGroupTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/metrics/groups/TaskManagerGroupTest.java
index b80ff6a..b2c5dc7 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/metrics/groups/TaskManagerGroupTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/metrics/groups/TaskManagerGroupTest.java
@@ -77,7 +77,7 @@ public class TaskManagerGroupTest {
 			execution11, 
 			new SerializedValue<>(new ExecutionConfig()), 
 			"test", 
-			17, 18, 0, 
+			18, 17, 18, 0,
 			new Configuration(), new Configuration(), 
 			"", 
 			new ArrayList<ResultPartitionDeploymentDescriptor>(), 
@@ -92,7 +92,7 @@ public class TaskManagerGroupTest {
 			execution12,
 			new SerializedValue<>(new ExecutionConfig()),
 			"test",
-			13, 18, 1,
+			18, 13, 18, 1,
 			new Configuration(), new Configuration(),
 			"",
 			new ArrayList<ResultPartitionDeploymentDescriptor>(),
@@ -107,7 +107,7 @@ public class TaskManagerGroupTest {
 			execution21,
 			new SerializedValue<>(new ExecutionConfig()),
 			"test",
-			7, 18, 2,
+			18, 7, 18, 2,
 			new Configuration(), new Configuration(),
 			"",
 			new ArrayList<ResultPartitionDeploymentDescriptor>(),
@@ -122,7 +122,7 @@ public class TaskManagerGroupTest {
 			execution13,
 			new SerializedValue<>(new ExecutionConfig()),
 			"test",
-			0, 18, 0,
+			18, 0, 18, 0,
 			new Configuration(), new Configuration(),
 			"",
 			new ArrayList<ResultPartitionDeploymentDescriptor>(),
@@ -193,7 +193,7 @@ public class TaskManagerGroupTest {
 			execution11,
 			new SerializedValue<>(new ExecutionConfig()),
 			"test",
-			17, 18, 0,
+			18, 17, 18, 0,
 			new Configuration(), new Configuration(),
 			"",
 			new ArrayList<ResultPartitionDeploymentDescriptor>(),
@@ -208,7 +208,7 @@ public class TaskManagerGroupTest {
 			execution12,
 			new SerializedValue<>(new ExecutionConfig()),
 			"test",
-			13, 18, 1,
+			18, 13, 18, 1,
 			new Configuration(), new Configuration(),
 			"",
 			new ArrayList<ResultPartitionDeploymentDescriptor>(),
@@ -223,7 +223,7 @@ public class TaskManagerGroupTest {
 			execution21,
 			new SerializedValue<>(new ExecutionConfig()),
 			"test",
-			7, 18, 1,
+			18, 7, 18, 1,
 			new Configuration(), new Configuration(),
 			"",
 			new ArrayList<ResultPartitionDeploymentDescriptor>(),
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/operators/testutils/DummyEnvironment.java b/flink-runtime/src/test/java/org/apache/flink/runtime/operators/testutils/DummyEnvironment.java
index 19317f9..4654507 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/operators/testutils/DummyEnvironment.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/operators/testutils/DummyEnvironment.java
@@ -53,13 +53,14 @@ public class DummyEnvironment implements Environment {
 	private final ExecutionAttemptID executionId = new ExecutionAttemptID();
 	private final ExecutionConfig executionConfig = new ExecutionConfig();
 	private final TaskInfo taskInfo;
-	private final KvStateRegistry kvStateRegistry = new KvStateRegistry();
-	private final TaskKvStateRegistry taskKvStateRegistry;
+	private KvStateRegistry kvStateRegistry = new KvStateRegistry();
 
 	public DummyEnvironment(String taskName, int numSubTasks, int subTaskIndex) {
-		this.taskInfo = new TaskInfo(taskName, subTaskIndex, numSubTasks, 0);
+		this.taskInfo = new TaskInfo(taskName, numSubTasks, subTaskIndex, numSubTasks, 0);
+	}
 
-		this.taskKvStateRegistry = kvStateRegistry.createTaskRegistry(jobId, jobVertexId);
+	public void setKvStateRegistry(KvStateRegistry kvStateRegistry) {
+		this.kvStateRegistry = kvStateRegistry;
 	}
 
 	public KvStateRegistry getKvStateRegistry() {
@@ -148,7 +149,7 @@ public class DummyEnvironment implements Environment {
 
 	@Override
 	public TaskKvStateRegistry getTaskKvStateRegistry() {
-		return taskKvStateRegistry;
+		return kvStateRegistry.createTaskRegistry(jobId, jobVertexId);
 	}
 
 	@Override
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/operators/testutils/MockEnvironment.java b/flink-runtime/src/test/java/org/apache/flink/runtime/operators/testutils/MockEnvironment.java
index 2c76399..e7bf6e1 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/operators/testutils/MockEnvironment.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/operators/testutils/MockEnvironment.java
@@ -99,11 +99,24 @@ public class MockEnvironment implements Environment {
 	private final int bufferSize;
 
 	public MockEnvironment(String taskName, long memorySize, MockInputSplitProvider inputSplitProvider, int bufferSize) {
-		this(taskName, memorySize, inputSplitProvider, bufferSize, new Configuration());
+		this(taskName, memorySize, inputSplitProvider, bufferSize, new Configuration(), new ExecutionConfig());
 	}
 
-	public MockEnvironment(String taskName, long memorySize, MockInputSplitProvider inputSplitProvider, int bufferSize, Configuration taskConfiguration) {
-		this.taskInfo = new TaskInfo(taskName, 0, 1, 0);
+	public MockEnvironment(String taskName, long memorySize, MockInputSplitProvider inputSplitProvider, int bufferSize, Configuration taskConfiguration, ExecutionConfig executionConfig) {
+		this(taskName, memorySize, inputSplitProvider, bufferSize, taskConfiguration, executionConfig, 1, 1, 0);
+	}
+
+	public MockEnvironment(
+			String taskName,
+			long memorySize,
+			MockInputSplitProvider inputSplitProvider,
+			int bufferSize,
+			Configuration taskConfiguration,
+			ExecutionConfig executionConfig,
+			int maxParallelism,
+			int parallelism,
+			int subtaskIndex) {
+		this.taskInfo = new TaskInfo(taskName, maxParallelism, subtaskIndex, parallelism, 0);
 		this.jobConfiguration = new Configuration();
 		this.taskConfiguration = taskConfiguration;
 		this.inputs = new LinkedList<InputGate>();
@@ -111,7 +124,7 @@ public class MockEnvironment implements Environment {
 
 		this.memManager = new MemoryManager(memorySize, 1);
 		this.ioManager = new IOManagerAsync();
-		this.executionConfig = new ExecutionConfig();
+		this.executionConfig = executionConfig;
 		this.inputSplitProvider = inputSplitProvider;
 		this.bufferSize = bufferSize;
 
@@ -121,6 +134,7 @@ public class MockEnvironment implements Environment {
 		this.kvStateRegistry = registry.createTaskRegistry(jobID, getJobVertexId());
 	}
 
+
 	public IteratorWrappingTestSingleInputGate<Record> addInput(MutableObjectIterator<Record> inputIterator) {
 		try {
 			final IteratorWrappingTestSingleInputGate<Record> reader = new IteratorWrappingTestSingleInputGate<Record>(bufferSize, Record.class, inputIterator);
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/query/QueryableStateClientTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/query/QueryableStateClientTest.java
index 36f2b45..3380907 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/query/QueryableStateClientTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/query/QueryableStateClientTest.java
@@ -26,14 +26,20 @@ import org.apache.flink.api.common.typeutils.base.IntSerializer;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.runtime.akka.AkkaUtils;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
+import org.apache.flink.runtime.operators.testutils.DummyEnvironment;
 import org.apache.flink.runtime.query.netty.AtomicKvStateRequestStats;
 import org.apache.flink.runtime.query.netty.KvStateClient;
 import org.apache.flink.runtime.query.netty.KvStateServer;
 import org.apache.flink.runtime.query.netty.UnknownKvStateID;
 import org.apache.flink.runtime.query.netty.message.KvStateRequestSerializer;
+import org.apache.flink.runtime.state.HashKeyGroupAssigner;
+import org.apache.flink.runtime.state.KeyGroupRange;
+import org.apache.flink.runtime.state.KeyedStateBackend;
 import org.apache.flink.runtime.state.VoidNamespace;
 import org.apache.flink.runtime.state.VoidNamespaceSerializer;
-import org.apache.flink.runtime.state.memory.MemValueState;
+import org.apache.flink.runtime.state.heap.HeapValueState;
+import org.apache.flink.runtime.state.heap.StateTable;
+import org.apache.flink.runtime.state.memory.MemoryStateBackend;
 import org.apache.flink.util.MathUtils;
 import org.junit.AfterClass;
 import org.junit.Test;
@@ -237,10 +243,22 @@ public class QueryableStateClientTest {
 		KvStateClient networkClient = null;
 		AtomicKvStateRequestStats networkClientStats = new AtomicKvStateRequestStats();
 
+		MemoryStateBackend backend = new MemoryStateBackend();
+		DummyEnvironment dummyEnv = new DummyEnvironment("test", 1, 0);
+
+		KeyedStateBackend<Integer> keyedStateBackend = backend.createKeyedStateBackend(dummyEnv,
+				new JobID(),
+				"test_op",
+				IntSerializer.INSTANCE,
+				new HashKeyGroupAssigner<Integer>(1),
+				new KeyGroupRange(0, 0),
+				new KvStateRegistry().createTaskRegistry(new JobID(), new JobVertexID()));
+
+
 		try {
 			KvStateRegistry[] registries = new KvStateRegistry[numServers];
 			KvStateID[] kvStateIds = new KvStateID[numServers];
-			List<MemValueState<Integer, VoidNamespace, Integer>> kvStates = new ArrayList<>();
+			List<HeapValueState<Integer, VoidNamespace, Integer>> kvStates = new ArrayList<>();
 
 			// Start the servers
 			for (int i = 0; i < numServers; i++) {
@@ -249,11 +267,14 @@ public class QueryableStateClientTest {
 				servers[i] = new KvStateServer(InetAddress.getLocalHost(), 0, 1, 1, registries[i], serverStats[i]);
 				servers[i].start();
 
+
 				// Register state
-				MemValueState<Integer, VoidNamespace, Integer> kvState = new MemValueState<>(
+				HeapValueState<Integer, VoidNamespace, Integer> kvState = new HeapValueState<>(
+						keyedStateBackend,
+						new ValueStateDescriptor<>("any", IntSerializer.INSTANCE, null),
+						new StateTable<Integer, VoidNamespace, Integer>(IntSerializer.INSTANCE, VoidNamespaceSerializer.INSTANCE,  new KeyGroupRange(0, 1)),
 						IntSerializer.INSTANCE,
-						VoidNamespaceSerializer.INSTANCE,
-						new ValueStateDescriptor<>("any", IntSerializer.INSTANCE, null));
+						VoidNamespaceSerializer.INSTANCE);
 
 				kvStates.add(kvState);
 
@@ -271,9 +292,9 @@ public class QueryableStateClientTest {
 				int targetKeyGroupIndex = MathUtils.murmurHash(key) % numServers;
 				expectedRequests[targetKeyGroupIndex]++;
 
-				MemValueState<Integer, VoidNamespace, Integer> kvState = kvStates.get(targetKeyGroupIndex);
+				HeapValueState<Integer, VoidNamespace, Integer> kvState = kvStates.get(targetKeyGroupIndex);
 
-				kvState.setCurrentKey(key);
+				keyedStateBackend.setCurrentKey(key);
 				kvState.setCurrentNamespace(VoidNamespace.INSTANCE);
 				kvState.update(1337 + key);
 			}
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/query/netty/KvStateClientTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/query/netty/KvStateClientTest.java
index ac03f94..796481c 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/query/netty/KvStateClientTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/query/netty/KvStateClientTest.java
@@ -30,18 +30,25 @@ import io.netty.channel.socket.SocketChannel;
 import io.netty.channel.socket.nio.NioServerSocketChannel;
 import io.netty.handler.codec.LengthFieldBasedFrameDecoder;
 import org.apache.flink.api.common.JobID;
+import org.apache.flink.api.common.state.ValueState;
 import org.apache.flink.api.common.state.ValueStateDescriptor;
 import org.apache.flink.api.common.typeutils.base.IntSerializer;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
+import org.apache.flink.runtime.operators.testutils.DummyEnvironment;
 import org.apache.flink.runtime.query.KvStateID;
 import org.apache.flink.runtime.query.KvStateRegistry;
 import org.apache.flink.runtime.query.KvStateServerAddress;
 import org.apache.flink.runtime.query.netty.message.KvStateRequest;
 import org.apache.flink.runtime.query.netty.message.KvStateRequestSerializer;
 import org.apache.flink.runtime.query.netty.message.KvStateRequestType;
+import org.apache.flink.runtime.state.AbstractStateBackend;
+import org.apache.flink.runtime.state.HashKeyGroupAssigner;
+import org.apache.flink.runtime.state.KeyGroupRange;
+import org.apache.flink.runtime.state.KeyedStateBackend;
+import org.apache.flink.runtime.state.KvState;
 import org.apache.flink.runtime.state.VoidNamespace;
 import org.apache.flink.runtime.state.VoidNamespaceSerializer;
-import org.apache.flink.runtime.state.memory.MemValueState;
+import org.apache.flink.runtime.state.memory.MemoryStateBackend;
 import org.apache.flink.util.NetUtils;
 import org.junit.AfterClass;
 import org.junit.Test;
@@ -526,6 +533,20 @@ public class KvStateClientTest {
 
 		final int batchSize = 16;
 
+		AbstractStateBackend abstractBackend = new MemoryStateBackend();
+		KvStateRegistry dummyRegistry = new KvStateRegistry();
+		DummyEnvironment dummyEnv = new DummyEnvironment("test", 1, 0);
+		dummyEnv.setKvStateRegistry(dummyRegistry);
+		KeyedStateBackend<Integer> backend = abstractBackend.createKeyedStateBackend(
+				dummyEnv,
+				new JobID(),
+				"test_op",
+				IntSerializer.INSTANCE,
+				new HashKeyGroupAssigner<Integer>(1),
+				new KeyGroupRange(0, 0),
+				dummyRegistry.createTaskRegistry(new JobID(), new JobVertexID()));
+
+
 		final FiniteDuration timeout = new FiniteDuration(10, TimeUnit.SECONDS);
 
 		AtomicKvStateRequestStats clientStats = new AtomicKvStateRequestStats();
@@ -542,11 +563,6 @@ public class KvStateClientTest {
 			ValueStateDescriptor<Integer> desc = new ValueStateDescriptor<>("any", IntSerializer.INSTANCE, null);
 			desc.setQueryable("any");
 
-			MemValueState<Integer, VoidNamespace, Integer> kvState = new MemValueState<>(
-					IntSerializer.INSTANCE,
-					VoidNamespaceSerializer.INSTANCE,
-					desc);
-
 			// Create servers
 			KvStateRegistry[] registry = new KvStateRegistry[numServers];
 			AtomicKvStateRequestStats[] serverStats = new AtomicKvStateRequestStats[numServers];
@@ -565,10 +581,17 @@ public class KvStateClientTest {
 
 				server[i].start();
 
+				backend.setCurrentKey(1010 + i);
+
 				// Value per server
-				kvState.setCurrentKey(1010 + i);
-				kvState.setCurrentNamespace(VoidNamespace.INSTANCE);
-				kvState.update(201 + i);
+				ValueState<Integer> state = backend.getPartitionedState(VoidNamespace.INSTANCE,
+						VoidNamespaceSerializer.INSTANCE,
+						desc);
+
+				state.update(201 + i);
+
+				// we know it must be a KvStat but this is not exposed to the user via State
+				KvState<?> kvState = (KvState<?>) state;
 
 				// Register KvState (one state instance for all server)
 				ids[i] = registry[i].registerKvState(new JobID(), new JobVertexID(), 0, "any", kvState);
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/query/netty/KvStateServerHandlerTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/query/netty/KvStateServerHandlerTest.java
index 6ad7ece..3d2e8b5 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/query/netty/KvStateServerHandlerTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/query/netty/KvStateServerHandlerTest.java
@@ -24,21 +24,28 @@ import io.netty.channel.ChannelHandler;
 import io.netty.channel.embedded.EmbeddedChannel;
 import io.netty.handler.codec.LengthFieldBasedFrameDecoder;
 import org.apache.flink.api.common.JobID;
+import org.apache.flink.api.common.state.ValueState;
 import org.apache.flink.api.common.state.ValueStateDescriptor;
 import org.apache.flink.api.common.typeutils.base.IntSerializer;
 import org.apache.flink.api.common.typeutils.base.StringSerializer;
 import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
+import org.apache.flink.runtime.operators.testutils.DummyEnvironment;
 import org.apache.flink.runtime.query.KvStateID;
 import org.apache.flink.runtime.query.KvStateRegistry;
+import org.apache.flink.runtime.query.KvStateRegistryListener;
 import org.apache.flink.runtime.query.netty.message.KvStateRequestFailure;
 import org.apache.flink.runtime.query.netty.message.KvStateRequestResult;
 import org.apache.flink.runtime.query.netty.message.KvStateRequestSerializer;
 import org.apache.flink.runtime.query.netty.message.KvStateRequestType;
+import org.apache.flink.runtime.state.AbstractStateBackend;
+import org.apache.flink.runtime.state.HashKeyGroupAssigner;
+import org.apache.flink.runtime.state.KeyGroupRange;
+import org.apache.flink.runtime.state.KeyedStateBackend;
 import org.apache.flink.runtime.state.KvState;
 import org.apache.flink.runtime.state.VoidNamespace;
 import org.apache.flink.runtime.state.VoidNamespaceSerializer;
-import org.apache.flink.runtime.state.memory.MemValueState;
+import org.apache.flink.runtime.state.memory.MemoryStateBackend;
 import org.junit.AfterClass;
 import org.junit.Test;
 
@@ -80,28 +87,34 @@ public class KvStateServerHandlerTest {
 
 		// Register state
 		ValueStateDescriptor<Integer> desc = new ValueStateDescriptor<>("any", IntSerializer.INSTANCE, null);
-		desc.setQueryable("any");
+		desc.setQueryable("vanilla");
 
-		MemValueState<Integer, VoidNamespace, Integer> kvState = new MemValueState<>(
+		AbstractStateBackend abstractBackend = new MemoryStateBackend();
+		DummyEnvironment dummyEnv = new DummyEnvironment("test", 1, 0);
+		dummyEnv.setKvStateRegistry(registry);
+		KeyedStateBackend<Integer> backend = abstractBackend.createKeyedStateBackend(
+				dummyEnv,
+				new JobID(),
+				"test_op",
 				IntSerializer.INSTANCE,
-				VoidNamespaceSerializer.INSTANCE,
-				desc);
+				new HashKeyGroupAssigner<Integer>(1),
+				new KeyGroupRange(0, 0),
+				registry.createTaskRegistry(dummyEnv.getJobID(), dummyEnv.getJobVertexId()));
 
-		KvStateID kvStateId = registry.registerKvState(
-				new JobID(),
-				new JobVertexID(),
-				0,
-				"vanilla",
-				kvState);
+		final TestRegistryListener registryListener = new TestRegistryListener();
+		registry.registerListener(registryListener);
 
 		// Update the KvState and request it
 		int expectedValue = 712828289;
 
 		int key = 99812822;
-		kvState.setCurrentKey(key);
-		kvState.setCurrentNamespace(VoidNamespace.INSTANCE);
+		backend.setCurrentKey(key);
+		ValueState<Integer> state = backend.getPartitionedState(
+				VoidNamespace.INSTANCE,
+				VoidNamespaceSerializer.INSTANCE,
+				desc);
 
-		kvState.update(expectedValue);
+		state.update(expectedValue);
 
 		byte[] serializedKeyAndNamespace = KvStateRequestSerializer.serializeKeyAndNamespace(
 				key,
@@ -110,10 +123,13 @@ public class KvStateServerHandlerTest {
 				VoidNamespaceSerializer.INSTANCE);
 
 		long requestId = Integer.MAX_VALUE + 182828L;
+
+		assertTrue(registryListener.registrationName.equals("vanilla"));
+
 		ByteBuf request = KvStateRequestSerializer.serializeKvStateRequest(
 				channel.alloc(),
 				requestId,
-				kvStateId,
+				registryListener.kvStateId,
 				serializedKeyAndNamespace);
 
 		// Write the request and wait for the response
@@ -184,21 +200,26 @@ public class KvStateServerHandlerTest {
 		KvStateServerHandler handler = new KvStateServerHandler(registry, TEST_THREAD_POOL, stats);
 		EmbeddedChannel channel = new EmbeddedChannel(getFrameDecoder(), handler);
 
+		AbstractStateBackend abstractBackend = new MemoryStateBackend();
+		DummyEnvironment dummyEnv = new DummyEnvironment("test", 1, 0);
+		dummyEnv.setKvStateRegistry(registry);
+		KeyedStateBackend<Integer> backend = abstractBackend.createKeyedStateBackend(
+				dummyEnv,
+				new JobID(),
+				"test_op",
+				IntSerializer.INSTANCE,
+				new HashKeyGroupAssigner<Integer>(1),
+				new KeyGroupRange(0, 0),
+				registry.createTaskRegistry(dummyEnv.getJobID(), dummyEnv.getJobVertexId()));
+
+		final TestRegistryListener registryListener = new TestRegistryListener();
+		registry.registerListener(registryListener);
+
 		// Register state
 		ValueStateDescriptor<Integer> desc = new ValueStateDescriptor<>("any", IntSerializer.INSTANCE, null);
-		desc.setQueryable("any");
+		desc.setQueryable("vanilla");
 
-		MemValueState<Integer, VoidNamespace, Integer> kvState = new MemValueState<>(
-				IntSerializer.INSTANCE,
-				VoidNamespaceSerializer.INSTANCE,
-				desc);
-
-		KvStateID kvStateId = registry.registerKvState(
-				new JobID(),
-				new JobVertexID(),
-				0,
-				"vanilla",
-				kvState);
+		backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, desc);
 
 		byte[] serializedKeyAndNamespace = KvStateRequestSerializer.serializeKeyAndNamespace(
 				1238283,
@@ -207,10 +228,13 @@ public class KvStateServerHandlerTest {
 				VoidNamespaceSerializer.INSTANCE);
 
 		long requestId = Integer.MAX_VALUE + 22982L;
+
+		assertTrue(registryListener.registrationName.equals("vanilla"));
+
 		ByteBuf request = KvStateRequestSerializer.serializeKvStateRequest(
 				channel.alloc(),
 				requestId,
-				kvStateId,
+				registryListener.kvStateId,
 				serializedKeyAndNamespace);
 
 		// Write the request and wait for the response
@@ -225,6 +249,8 @@ public class KvStateServerHandlerTest {
 
 		assertEquals(requestId, response.getRequestId());
 
+		System.out.println("RESPOINSE: " + response);
+
 		assertTrue("Did not respond with expected failure cause", response.getCause() instanceof UnknownKeyOrNamespace);
 
 		assertEquals(1, stats.getNumRequests());
@@ -244,7 +270,7 @@ public class KvStateServerHandlerTest {
 		EmbeddedChannel channel = new EmbeddedChannel(getFrameDecoder(), handler);
 
 		// Failing KvState
-		KvState<?, ?, ?, ?, ?> kvState = mock(KvState.class);
+		KvState<?> kvState = mock(KvState.class);
 		when(kvState.getSerializedValue(any(byte[].class)))
 				.thenThrow(new RuntimeException("Expected test Exception"));
 
@@ -320,26 +346,33 @@ public class KvStateServerHandlerTest {
 		KvStateServerHandler handler = new KvStateServerHandler(registry, closedExecutor, stats);
 		EmbeddedChannel channel = new EmbeddedChannel(getFrameDecoder(), handler);
 
+		AbstractStateBackend abstractBackend = new MemoryStateBackend();
+		DummyEnvironment dummyEnv = new DummyEnvironment("test", 1, 0);
+		dummyEnv.setKvStateRegistry(registry);
+		KeyedStateBackend<Integer> backend = abstractBackend.createKeyedStateBackend(
+				dummyEnv,
+				new JobID(),
+				"test_op",
+				IntSerializer.INSTANCE,
+				new HashKeyGroupAssigner<Integer>(1),
+				new KeyGroupRange(0, 0),
+				registry.createTaskRegistry(dummyEnv.getJobID(), dummyEnv.getJobVertexId()));
+
+		final TestRegistryListener registryListener = new TestRegistryListener();
+		registry.registerListener(registryListener);
+
 		// Register state
 		ValueStateDescriptor<Integer> desc = new ValueStateDescriptor<>("any", IntSerializer.INSTANCE, null);
-		desc.setQueryable("any");
+		desc.setQueryable("vanilla");
 
-		MemValueState<Integer, VoidNamespace, Integer> kvState = new MemValueState<>(
-				IntSerializer.INSTANCE,
-				VoidNamespaceSerializer.INSTANCE,
-				desc);
+		backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, desc);
 
-		KvStateID kvStateId = registry.registerKvState(
-				new JobID(),
-				new JobVertexID(),
-				0,
-				"vanilla",
-				kvState);
+		assertTrue(registryListener.registrationName.equals("vanilla"));
 
 		ByteBuf request = KvStateRequestSerializer.serializeKvStateRequest(
 				channel.alloc(),
 				282872,
-				kvStateId,
+				registryListener.kvStateId,
 				new byte[0]);
 
 		// Write the request and wait for the response
@@ -451,28 +484,35 @@ public class KvStateServerHandlerTest {
 		KvStateServerHandler handler = new KvStateServerHandler(registry, TEST_THREAD_POOL, stats);
 		EmbeddedChannel channel = new EmbeddedChannel(getFrameDecoder(), handler);
 
+		AbstractStateBackend abstractBackend = new MemoryStateBackend();
+		DummyEnvironment dummyEnv = new DummyEnvironment("test", 1, 0);
+		dummyEnv.setKvStateRegistry(registry);
+		KeyedStateBackend<Integer> backend = abstractBackend.createKeyedStateBackend(
+				dummyEnv,
+				new JobID(),
+				"test_op",
+				IntSerializer.INSTANCE,
+				new HashKeyGroupAssigner<Integer>(1),
+				new KeyGroupRange(0, 0),
+				registry.createTaskRegistry(dummyEnv.getJobID(), dummyEnv.getJobVertexId()));
+
+		final TestRegistryListener registryListener = new TestRegistryListener();
+		registry.registerListener(registryListener);
+
 		// Register state
 		ValueStateDescriptor<Integer> desc = new ValueStateDescriptor<>("any", IntSerializer.INSTANCE, null);
-		desc.setQueryable("any");
+		desc.setQueryable("vanilla");
 
-		MemValueState<Integer, VoidNamespace, Integer> kvState = new MemValueState<>(
-				IntSerializer.INSTANCE,
+		ValueState<Integer> state = backend.getPartitionedState(
+				VoidNamespace.INSTANCE,
 				VoidNamespaceSerializer.INSTANCE,
 				desc);
 
-		KvStateID kvStateId = registry.registerKvState(
-				new JobID(),
-				new JobVertexID(),
-				0,
-				"vanilla",
-				kvState);
-
 		int key = 99812822;
 
 		// Update the KvState
-		kvState.setCurrentKey(key);
-		kvState.setCurrentNamespace(VoidNamespace.INSTANCE);
-		kvState.update(712828289);
+		backend.setCurrentKey(key);
+		state.update(712828289);
 
 		byte[] wrongKeyAndNamespace = KvStateRequestSerializer.serializeKeyAndNamespace(
 				"wrong-key-type",
@@ -486,10 +526,11 @@ public class KvStateServerHandlerTest {
 				"wrong-namespace-type",
 				StringSerializer.INSTANCE);
 
+		assertTrue(registryListener.registrationName.equals("vanilla"));
 		ByteBuf request = KvStateRequestSerializer.serializeKvStateRequest(
 				channel.alloc(),
 				182828,
-				kvStateId,
+				registryListener.kvStateId,
 				wrongKeyAndNamespace);
 
 		// Write the request and wait for the response
@@ -508,7 +549,7 @@ public class KvStateServerHandlerTest {
 		request = KvStateRequestSerializer.serializeKvStateRequest(
 				channel.alloc(),
 				182829,
-				kvStateId,
+				registryListener.kvStateId,
 				wrongNamespace);
 
 		// Write the request and wait for the response
@@ -538,22 +579,30 @@ public class KvStateServerHandlerTest {
 		KvStateServerHandler handler = new KvStateServerHandler(registry, TEST_THREAD_POOL, stats);
 		EmbeddedChannel channel = new EmbeddedChannel(getFrameDecoder(), handler);
 
+		AbstractStateBackend abstractBackend = new MemoryStateBackend();
+		DummyEnvironment dummyEnv = new DummyEnvironment("test", 1, 0);
+		dummyEnv.setKvStateRegistry(registry);
+		KeyedStateBackend<Integer> backend = abstractBackend.createKeyedStateBackend(
+				dummyEnv,
+				new JobID(),
+				"test_op",
+				IntSerializer.INSTANCE,
+				new HashKeyGroupAssigner<Integer>(1),
+				new KeyGroupRange(0, 0),
+				registry.createTaskRegistry(dummyEnv.getJobID(), dummyEnv.getJobVertexId()));
+
+		final TestRegistryListener registryListener = new TestRegistryListener();
+		registry.registerListener(registryListener);
+
 		// Register state
 		ValueStateDescriptor<byte[]> desc = new ValueStateDescriptor<>("any", BytePrimitiveArraySerializer.INSTANCE, null);
-		desc.setQueryable("any");
+		desc.setQueryable("vanilla");
 
-		MemValueState<Integer, VoidNamespace, byte[]> kvState = new MemValueState<>(
-				IntSerializer.INSTANCE,
+		ValueState<byte[]> state = backend.getPartitionedState(
+				VoidNamespace.INSTANCE,
 				VoidNamespaceSerializer.INSTANCE,
 				desc);
 
-		KvStateID kvStateId = registry.registerKvState(
-				new JobID(),
-				new JobVertexID(),
-				0,
-				"vanilla",
-				kvState);
-
 		// Update KvState
 		byte[] bytes = new byte[2 * channel.config().getWriteBufferHighWaterMark()];
 
@@ -563,9 +612,8 @@ public class KvStateServerHandlerTest {
 		}
 
 		int key = 99812822;
-		kvState.setCurrentKey(key);
-		kvState.setCurrentNamespace(VoidNamespace.INSTANCE);
-		kvState.update(bytes);
+		backend.setCurrentKey(key);
+		state.update(bytes);
 
 		// Request
 		byte[] serializedKeyAndNamespace = KvStateRequestSerializer.serializeKeyAndNamespace(
@@ -575,10 +623,13 @@ public class KvStateServerHandlerTest {
 				VoidNamespaceSerializer.INSTANCE);
 
 		long requestId = Integer.MAX_VALUE + 182828L;
+
+		assertTrue(registryListener.registrationName.equals("vanilla"));
+
 		ByteBuf request = KvStateRequestSerializer.serializeKvStateRequest(
 				channel.alloc(),
 				requestId,
-				kvStateId,
+				registryListener.kvStateId,
 				serializedKeyAndNamespace);
 
 		// Write the request and wait for the response
@@ -619,4 +670,35 @@ public class KvStateServerHandlerTest {
 	private ChannelHandler getFrameDecoder() {
 		return new LengthFieldBasedFrameDecoder(Integer.MAX_VALUE, 0, 4, 0, 4);
 	}
+
+	/**
+	 * A listener that keeps the last updated KvState information so that a test
+	 * can retrieve it.
+	 */
+	static class TestRegistryListener implements KvStateRegistryListener {
+		volatile JobVertexID jobVertexID;
+		volatile int keyGroupIndex;
+		volatile String registrationName;
+		volatile KvStateID kvStateId;
+
+		@Override
+		public void notifyKvStateRegistered(JobID jobId,
+				JobVertexID jobVertexId,
+				int keyGroupIndex,
+				String registrationName,
+				KvStateID kvStateId) {
+			this.jobVertexID = jobVertexId;
+			this.keyGroupIndex = keyGroupIndex;
+			this.registrationName = registrationName;
+			this.kvStateId = kvStateId;
+		}
+
+		@Override
+		public void notifyKvStateUnregistered(JobID jobId,
+				JobVertexID jobVertexId,
+				int keyGroupIndex,
+				String registrationName) {
+
+		}
+	}
 }
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/query/netty/KvStateServerTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/query/netty/KvStateServerTest.java
index d653f73..30d91b6 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/query/netty/KvStateServerTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/query/netty/KvStateServerTest.java
@@ -31,18 +31,23 @@ import io.netty.channel.socket.SocketChannel;
 import io.netty.channel.socket.nio.NioSocketChannel;
 import io.netty.handler.codec.LengthFieldBasedFrameDecoder;
 import org.apache.flink.api.common.JobID;
+import org.apache.flink.api.common.state.ValueState;
 import org.apache.flink.api.common.state.ValueStateDescriptor;
 import org.apache.flink.api.common.typeutils.base.IntSerializer;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
-import org.apache.flink.runtime.query.KvStateID;
+import org.apache.flink.runtime.operators.testutils.DummyEnvironment;
 import org.apache.flink.runtime.query.KvStateRegistry;
 import org.apache.flink.runtime.query.KvStateServerAddress;
 import org.apache.flink.runtime.query.netty.message.KvStateRequestResult;
 import org.apache.flink.runtime.query.netty.message.KvStateRequestSerializer;
 import org.apache.flink.runtime.query.netty.message.KvStateRequestType;
+import org.apache.flink.runtime.state.AbstractStateBackend;
+import org.apache.flink.runtime.state.HashKeyGroupAssigner;
+import org.apache.flink.runtime.state.KeyGroupRange;
+import org.apache.flink.runtime.state.KeyedStateBackend;
 import org.apache.flink.runtime.state.VoidNamespace;
 import org.apache.flink.runtime.state.VoidNamespaceSerializer;
-import org.apache.flink.runtime.state.memory.MemValueState;
+import org.apache.flink.runtime.state.memory.MemoryStateBackend;
 import org.junit.AfterClass;
 import org.junit.Test;
 
@@ -52,6 +57,7 @@ import java.util.concurrent.LinkedBlockingQueue;
 import java.util.concurrent.TimeUnit;
 
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
 
 public class KvStateServerTest {
 
@@ -84,26 +90,37 @@ public class KvStateServerTest {
 
 			KvStateServerAddress serverAddress = server.getAddress();
 
-			// Register state
-			MemValueState<Integer, VoidNamespace, Integer> kvState = new MemValueState<>(
+			AbstractStateBackend abstractBackend = new MemoryStateBackend();
+			DummyEnvironment dummyEnv = new DummyEnvironment("test", 1, 0);
+			dummyEnv.setKvStateRegistry(registry);
+			KeyedStateBackend<Integer> backend = abstractBackend.createKeyedStateBackend(
+					dummyEnv,
+					new JobID(),
+					"test_op",
 					IntSerializer.INSTANCE,
-					VoidNamespaceSerializer.INSTANCE,
-					new ValueStateDescriptor<>("any", IntSerializer.INSTANCE, null));
+					new HashKeyGroupAssigner<Integer>(1),
+					new KeyGroupRange(0, 0),
+					registry.createTaskRegistry(new JobID(), new JobVertexID()));
 
-			KvStateID kvStateId = registry.registerKvState(
-					new JobID(),
-					new JobVertexID(),
-					0,
-					"vanilla",
-					kvState);
+			final KvStateServerHandlerTest.TestRegistryListener registryListener =
+					new KvStateServerHandlerTest.TestRegistryListener();
+
+			registry.registerListener(registryListener);
+
+			ValueStateDescriptor<Integer> desc = new ValueStateDescriptor<>("any", IntSerializer.INSTANCE, null);
+			desc.setQueryable("vanilla");
+
+			ValueState<Integer> state = backend.getPartitionedState(
+					VoidNamespace.INSTANCE,
+					VoidNamespaceSerializer.INSTANCE,
+					desc);
 
 			// Update KvState
 			int expectedValue = 712828289;
 
 			int key = 99812822;
-			kvState.setCurrentKey(key);
-			kvState.setCurrentNamespace(VoidNamespace.INSTANCE);
-			kvState.update(expectedValue);
+			backend.setCurrentKey(key);
+			state.update(expectedValue);
 
 			// Request
 			byte[] serializedKeyAndNamespace = KvStateRequestSerializer.serializeKeyAndNamespace(
@@ -128,10 +145,12 @@ public class KvStateServerTest {
 					.sync().channel();
 
 			long requestId = Integer.MAX_VALUE + 182828L;
+
+			assertTrue(registryListener.registrationName.equals("vanilla"));
 			ByteBuf request = KvStateRequestSerializer.serializeKvStateRequest(
 					channel.alloc(),
 					requestId,
-					kvStateId,
+					registryListener.kvStateId,
 					serializedKeyAndNamespace);
 
 			channel.writeAndFlush(request);
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/state/FileStateBackendTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/state/FileStateBackendTest.java
index 04fa089..bc0b9c3 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/state/FileStateBackendTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/state/FileStateBackendTest.java
@@ -19,6 +19,7 @@
 package org.apache.flink.runtime.state;
 
 import org.apache.commons.io.FileUtils;
+import org.apache.flink.api.common.JobID;
 import org.apache.flink.api.common.typeutils.base.IntSerializer;
 import org.apache.flink.configuration.ConfigConstants;
 import org.apache.flink.core.fs.Path;
@@ -29,7 +30,9 @@ import org.apache.flink.runtime.state.filesystem.FileStateHandle;
 import org.apache.flink.runtime.state.filesystem.FsStateBackend;
 import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;
 
+import org.junit.Rule;
 import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
 
 import java.io.File;
 import java.io.IOException;
@@ -42,17 +45,13 @@ import static org.junit.Assert.*;
 
 public class FileStateBackendTest extends StateBackendTestBase<FsStateBackend> {
 
-	private File stateDir;
+	@Rule
+	public TemporaryFolder tempFolder = new TemporaryFolder();
 
 	@Override
 	protected FsStateBackend getStateBackend() throws Exception {
-		stateDir = new File(ConfigConstants.DEFAULT_TASK_MANAGER_TMP_PATH, UUID.randomUUID().toString());
-		return new FsStateBackend(localFileUri(stateDir));
-	}
-
-	@Override
-	protected void cleanup() throws Exception {
-		deleteDirectorySilently(stateDir);
+		File checkpointPath = tempFolder.newFolder();
+		return new FsStateBackend(localFileUri(checkpointPath));
 	}
 
 	// disable these because the verification does not work for this state backend
@@ -69,66 +68,19 @@ public class FileStateBackendTest extends StateBackendTestBase<FsStateBackend> {
 	public void testReducingStateRestoreWithWrongSerializers() {}
 
 	@Test
-	public void testSetupAndSerialization() {
-		File tempDir = new File(ConfigConstants.DEFAULT_TASK_MANAGER_TMP_PATH, UUID.randomUUID().toString());
-		try {
-			final String backendDir = localFileUri(tempDir);
-			FsStateBackend originalBackend = new FsStateBackend(backendDir);
-
-			assertFalse(originalBackend.isInitialized());
-			assertEquals(new URI(backendDir), originalBackend.getBasePath().toUri());
-			assertNull(originalBackend.getCheckpointDirectory());
-
-			// serialize / copy the backend
-			FsStateBackend backend = CommonTestUtils.createCopySerializable(originalBackend);
-			assertFalse(backend.isInitialized());
-			assertEquals(new URI(backendDir), backend.getBasePath().toUri());
-			assertNull(backend.getCheckpointDirectory());
-
-			// no file operations should be possible right now
-			try {
-				FsStateBackend.FsCheckpointStateOutputStream out = backend.createCheckpointStateOutputStream(
-						2L,
-						System.currentTimeMillis());
-
-				out.write(1);
-				out.closeAndGetHandle();
-				fail("should fail with an exception");
-			} catch (IllegalStateException e) {
-				// supreme!
-			}
-
-			backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test-op", IntSerializer.INSTANCE);
-			assertNotNull(backend.getCheckpointDirectory());
-
-			File checkpointDir = new File(backend.getCheckpointDirectory().toUri().getPath());
-			assertTrue(checkpointDir.exists());
-			assertTrue(isDirectoryEmpty(checkpointDir));
-
-			backend.disposeAllStateForCurrentJob();
-			assertNull(backend.getCheckpointDirectory());
+	public void testStateOutputStream() throws IOException {
+		File basePath = tempFolder.newFolder().getAbsoluteFile();
 
-			assertTrue(isDirectoryEmpty(tempDir));
-		}
-		catch (Exception e) {
-			e.printStackTrace();
-			fail(e.getMessage());
-		}
-		finally {
-			deleteDirectorySilently(tempDir);
-		}
-	}
-
-	@Test
-	public void testStateOutputStream() {
-		File tempDir = new File(ConfigConstants.DEFAULT_TASK_MANAGER_TMP_PATH, UUID.randomUUID().toString());
 		try {
 			// the state backend has a very low in-mem state threshold (15 bytes)
-			FsStateBackend backend = CommonTestUtils.createCopySerializable(new FsStateBackend(tempDir.toURI(), 15));
+			FsStateBackend backend = CommonTestUtils.createCopySerializable(new FsStateBackend(basePath.toURI(), 15));
+			JobID jobId = new JobID();
 
-			backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test-op", IntSerializer.INSTANCE);
+			// we know how FsCheckpointStreamFactory is implemented so we know where it
+			// will store checkpoints
+			File checkpointPath = new File(basePath.getAbsolutePath(), jobId.toString());
 
-			File checkpointDir = new File(backend.getCheckpointDirectory().toUri().getPath());
+			CheckpointStreamFactory streamFactory = backend.createStreamFactory(jobId, "test_op");
 
 			byte[] state1 = new byte[1274673];
 			byte[] state2 = new byte[1];
@@ -143,12 +95,14 @@ public class FileStateBackendTest extends StateBackendTestBase<FsStateBackend> {
 
 			long checkpointId = 97231523452L;
 
-			FsStateBackend.FsCheckpointStateOutputStream stream1 =
-					backend.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis());
-			FsStateBackend.FsCheckpointStateOutputStream stream2 =
-					backend.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis());
-			FsStateBackend.FsCheckpointStateOutputStream stream3 =
-					backend.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis());
+			CheckpointStreamFactory.CheckpointStateOutputStream stream1 =
+					streamFactory.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis());
+
+			CheckpointStreamFactory.CheckpointStateOutputStream stream2 =
+					streamFactory.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis());
+
+			CheckpointStreamFactory.CheckpointStateOutputStream stream3 =
+					streamFactory.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis());
 
 			stream1.write(state1);
 			stream2.write(state2);
@@ -160,15 +114,15 @@ public class FileStateBackendTest extends StateBackendTestBase<FsStateBackend> {
 
 			// use with try-with-resources
 			StreamStateHandle handle4;
-			try (AbstractStateBackend.CheckpointStateOutputStream stream4 =
-					backend.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis())) {
+			try (CheckpointStreamFactory.CheckpointStateOutputStream stream4 =
+					streamFactory.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis())) {
 				stream4.write(state4);
 				handle4 = stream4.closeAndGetHandle();
 			}
 
 			// close before accessing handle
-			AbstractStateBackend.CheckpointStateOutputStream stream5 =
-					backend.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis());
+			CheckpointStreamFactory.CheckpointStateOutputStream stream5 =
+					streamFactory.createCheckpointStateOutputStream(checkpointId, System.currentTimeMillis());
 			stream5.write(state4);
 			stream5.close();
 			try {
@@ -180,7 +134,7 @@ public class FileStateBackendTest extends StateBackendTestBase<FsStateBackend> {
 
 			validateBytesInStream(handle1.openInputStream(), state1);
 			handle1.discardState();
-			assertFalse(isDirectoryEmpty(checkpointDir));
+			assertFalse(isDirectoryEmpty(basePath));
 			ensureLocalFileDeleted(handle1.getFilePath());
 
 			validateBytesInStream(handle2.openInputStream(), state2);
@@ -191,15 +145,12 @@ public class FileStateBackendTest extends StateBackendTestBase<FsStateBackend> {
 
 			validateBytesInStream(handle4.openInputStream(), state4);
 			handle4.discardState();
-			assertTrue(isDirectoryEmpty(checkpointDir));
+			assertTrue(isDirectoryEmpty(checkpointPath));
 		}
 		catch (Exception e) {
 			e.printStackTrace();
 			fail(e.getMessage());
 		}
-		finally {
-			deleteDirectorySilently(tempDir);
-		}
 	}
 
 	// ------------------------------------------------------------------------
@@ -253,8 +204,7 @@ public class FileStateBackendTest extends StateBackendTestBase<FsStateBackend> {
 
 	@Test
 	public void testConcurrentMapIfQueryable() throws Exception {
-		backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
-		StateBackendTestBase.testConcurrentMapIfQueryable(backend);
+		super.testConcurrentMapIfQueryable();
 	}
 
 }
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/state/MemoryStateBackendTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/state/MemoryStateBackendTest.java
index 940b337..944938b 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/state/MemoryStateBackendTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/state/MemoryStateBackendTest.java
@@ -18,6 +18,7 @@
 
 package org.apache.flink.runtime.state;
 
+import org.apache.flink.api.common.JobID;
 import org.apache.flink.api.common.typeutils.base.IntSerializer;
 import org.apache.flink.runtime.operators.testutils.DummyEnvironment;
 import org.apache.flink.runtime.state.memory.MemoryStateBackend;
@@ -40,9 +41,6 @@ public class MemoryStateBackendTest extends StateBackendTestBase<MemoryStateBack
 		return new MemoryStateBackend();
 	}
 
-	@Override
-	protected void cleanup() throws Exception { }
-
 	// disable these because the verification does not work for this state backend
 	@Override
 	@Test
@@ -60,15 +58,15 @@ public class MemoryStateBackendTest extends StateBackendTestBase<MemoryStateBack
 	public void testOversizedState() {
 		try {
 			MemoryStateBackend backend = new MemoryStateBackend(10);
+			CheckpointStreamFactory streamFactory = backend.createStreamFactory(new JobID(), "test_op");
 
 			HashMap<String, Integer> state = new HashMap<>();
 			state.put("hey there", 2);
 			state.put("the crazy brown fox stumbles over a sentence that does not contain every letter", 77);
 
 			try {
-				AbstractStateBackend.CheckpointStateOutputStream outStream = backend.createCheckpointStateOutputStream(
-						12,
-						459);
+				CheckpointStreamFactory.CheckpointStateOutputStream outStream =
+						streamFactory.createCheckpointStateOutputStream(12, 459);
 
 				ObjectOutputStream oos = new ObjectOutputStream(outStream);
 				oos.writeObject(state);
@@ -93,12 +91,13 @@ public class MemoryStateBackendTest extends StateBackendTestBase<MemoryStateBack
 	public void testStateStream() {
 		try {
 			MemoryStateBackend backend = new MemoryStateBackend();
+			CheckpointStreamFactory streamFactory = backend.createStreamFactory(new JobID(), "test_op");
 
 			HashMap<String, Integer> state = new HashMap<>();
 			state.put("hey there", 2);
 			state.put("the crazy brown fox stumbles over a sentence that does not contain every letter", 77);
 
-			AbstractStateBackend.CheckpointStateOutputStream os = backend.createCheckpointStateOutputStream(1, 2);
+			CheckpointStreamFactory.CheckpointStateOutputStream os = streamFactory.createCheckpointStateOutputStream(1, 2);
 			ObjectOutputStream oos = new ObjectOutputStream(os);
 			oos.writeObject(state);
 			oos.flush();
@@ -121,12 +120,13 @@ public class MemoryStateBackendTest extends StateBackendTestBase<MemoryStateBack
 	public void testOversizedStateStream() {
 		try {
 			MemoryStateBackend backend = new MemoryStateBackend(10);
+			CheckpointStreamFactory streamFactory = backend.createStreamFactory(new JobID(), "test_op");
 
 			HashMap<String, Integer> state = new HashMap<>();
 			state.put("hey there", 2);
 			state.put("the crazy brown fox stumbles over a sentence that does not contain every letter", 77);
 
-			AbstractStateBackend.CheckpointStateOutputStream os = backend.createCheckpointStateOutputStream(1, 2);
+			CheckpointStreamFactory.CheckpointStateOutputStream os = streamFactory.createCheckpointStateOutputStream(1, 2);
 			ObjectOutputStream oos = new ObjectOutputStream(os);
 
 			try {
@@ -147,7 +147,6 @@ public class MemoryStateBackendTest extends StateBackendTestBase<MemoryStateBack
 
 	@Test
 	public void testConcurrentMapIfQueryable() throws Exception {
-		backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
-		StateBackendTestBase.testConcurrentMapIfQueryable(backend);
+		super.testConcurrentMapIfQueryable();
 	}
 }
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/state/StateBackendTestBase.java b/flink-runtime/src/test/java/org/apache/flink/runtime/state/StateBackendTestBase.java
index 834c35c..f094bd5 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/state/StateBackendTestBase.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/state/StateBackendTestBase.java
@@ -21,10 +21,12 @@ package org.apache.flink.runtime.state;
 import com.google.common.base.Joiner;
 import org.apache.commons.io.output.ByteArrayOutputStream;
 import org.apache.flink.api.common.ExecutionConfig;
+import org.apache.flink.api.common.JobID;
 import org.apache.flink.api.common.functions.FoldFunction;
 import org.apache.flink.api.common.functions.ReduceFunction;
 import org.apache.flink.api.common.state.FoldingState;
 import org.apache.flink.api.common.state.FoldingStateDescriptor;
+import org.apache.flink.api.common.state.KeyGroupAssigner;
 import org.apache.flink.api.common.state.ListState;
 import org.apache.flink.api.common.state.ListStateDescriptor;
 import org.apache.flink.api.common.state.ReducingState;
@@ -37,21 +39,25 @@ import org.apache.flink.api.common.typeutils.base.IntSerializer;
 import org.apache.flink.api.common.typeutils.base.LongSerializer;
 import org.apache.flink.api.common.typeutils.base.StringSerializer;
 import org.apache.flink.core.memory.DataOutputViewStreamWrapper;
+import org.apache.flink.runtime.checkpoint.CheckpointCoordinator;
+import org.apache.flink.runtime.execution.Environment;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
 import org.apache.flink.runtime.operators.testutils.DummyEnvironment;
 import org.apache.flink.runtime.query.KvStateID;
 import org.apache.flink.runtime.query.KvStateRegistry;
 import org.apache.flink.runtime.query.KvStateRegistryListener;
 import org.apache.flink.runtime.query.netty.message.KvStateRequestSerializer;
+import org.apache.flink.runtime.state.heap.AbstractHeapState;
+import org.apache.flink.runtime.state.heap.StateTable;
 import org.apache.flink.types.IntValue;
-import org.junit.After;
-import org.junit.Before;
 import org.junit.Test;
 
 import java.util.Collections;
-import java.util.HashMap;
 import java.util.List;
-import java.util.Map;
+import java.util.Random;
 import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.Future;
+import java.util.concurrent.RunnableFuture;
 
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
@@ -71,27 +77,77 @@ import static org.mockito.Mockito.verify;
 @SuppressWarnings("serial")
 public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 
-	protected B backend;
-
 	protected abstract B getStateBackend() throws Exception;
 
-	protected abstract void cleanup() throws Exception;
+	protected CheckpointStreamFactory createStreamFactory() throws Exception {
+		return getStateBackend().createStreamFactory(new JobID(), "test_op");
+	}
+
+	protected <K> KeyedStateBackend<K> createKeyedBackend(TypeSerializer<K> keySerializer) throws Exception {
+		return createKeyedBackend(keySerializer, new DummyEnvironment("test", 1, 0));
+	}
+
+	protected <K> KeyedStateBackend<K> createKeyedBackend(TypeSerializer<K> keySerializer, Environment env) throws Exception {
+		return createKeyedBackend(
+				keySerializer,
+				new HashKeyGroupAssigner<K>(10),
+				new KeyGroupRange(0, 9),
+				env);
+	}
+
+	protected <K> KeyedStateBackend<K> createKeyedBackend(
+			TypeSerializer<K> keySerializer,
+			KeyGroupAssigner<K> keyGroupAssigner,
+			KeyGroupRange keyGroupRange,
+			Environment env) throws Exception {
+		return getStateBackend().createKeyedStateBackend(
+				env,
+				new JobID(),
+				"test_op",
+				keySerializer,
+				keyGroupAssigner,
+				keyGroupRange,
+				env.getTaskKvStateRegistry());
+	}
 
-	@Before
-	public void setup() throws Exception {
-		this.backend = getStateBackend();
+	protected <K> KeyedStateBackend<K> restoreKeyedBackend(TypeSerializer<K> keySerializer, KeyGroupsStateHandle state) throws Exception {
+		return restoreKeyedBackend(keySerializer, state, new DummyEnvironment("test", 1, 0));
 	}
 
-	@After
-	public void teardown() throws Exception {
-		this.backend.discardState();
-		cleanup();
+	protected <K> KeyedStateBackend<K> restoreKeyedBackend(
+			TypeSerializer<K> keySerializer,
+			KeyGroupsStateHandle state,
+			Environment env) throws Exception {
+		return restoreKeyedBackend(
+				keySerializer,
+				new HashKeyGroupAssigner<K>(10),
+				new KeyGroupRange(0, 9),
+				Collections.singletonList(state),
+				env);
+	}
+
+	protected <K> KeyedStateBackend<K> restoreKeyedBackend(
+			TypeSerializer<K> keySerializer,
+			KeyGroupAssigner<K> keyGroupAssigner,
+			KeyGroupRange keyGroupRange,
+			List<KeyGroupsStateHandle> state,
+			Environment env) throws Exception {
+		return getStateBackend().restoreKeyedStateBackend(
+				env,
+				new JobID(),
+				"test_op",
+				keySerializer,
+				keyGroupAssigner,
+				keyGroupRange,
+				state,
+				env.getTaskKvStateRegistry());
 	}
 
 	@Test
 	@SuppressWarnings("unchecked")
 	public void testValueState() throws Exception {
-		backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
+		CheckpointStreamFactory streamFactory = createStreamFactory();
+		KeyedStateBackend<Integer> backend = createKeyedBackend(IntSerializer.INSTANCE);
 
 		ValueStateDescriptor<String> kvId = new ValueStateDescriptor<>("id", String.class, null);
 		kvId.initializeSerializerUnlessSet(new ExecutionConfig());
@@ -102,7 +158,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 
 		ValueState<String> state = backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, kvId);
 		@SuppressWarnings("unchecked")
-		KvState<Integer, VoidNamespace, ?, ?, B> kvState = (KvState<Integer, VoidNamespace, ?, ?, B>) state;
+		KvState<VoidNamespace> kvState = (KvState<VoidNamespace>) state;
 
 		// some modifications to the state
 		backend.setCurrentKey(1);
@@ -118,13 +174,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 		assertEquals("1", getSerializedValue(kvState, 1, keySerializer, VoidNamespace.INSTANCE, namespaceSerializer, valueSerializer));
 
 		// draw a snapshot
-		HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> snapshot1 = backend.snapshotPartitionedState(682375462378L, 2);
-
-		for (String key: snapshot1.keySet()) {
-			if (snapshot1.get(key) instanceof AsynchronousKvStateSnapshot) {
-				snapshot1.put(key, ((AsynchronousKvStateSnapshot<?, ?, ?, ?, ?>) snapshot1.get(key)).materialize());
-			}
-		}
+		KeyGroupsStateHandle snapshot1 = runSnapshot(backend.snapshot(682375462378L, 2, streamFactory));
 
 		// make some more modifications
 		backend.setCurrentKey(1);
@@ -135,13 +185,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 		state.update("u3");
 
 		// draw another snapshot
-		HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> snapshot2 = backend.snapshotPartitionedState(682375462379L, 4);
-
-		for (String key: snapshot2.keySet()) {
-			if (snapshot2.get(key) instanceof AsynchronousKvStateSnapshot) {
-				snapshot2.put(key, ((AsynchronousKvStateSnapshot<?, ?, ?, ?, ?>) snapshot2.get(key)).materialize());
-			}
-		}
+		KeyGroupsStateHandle snapshot2 = runSnapshot(backend.snapshot(682375462379L, 4, streamFactory));
 
 		// validate the original state
 		backend.setCurrentKey(1);
@@ -154,18 +198,14 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 		assertEquals("u3", state.value());
 		assertEquals("u3", getSerializedValue(kvState, 3, keySerializer, VoidNamespace.INSTANCE, namespaceSerializer, valueSerializer));
 
-		backend.discardState();
-		backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
+		backend.close();
+		backend = restoreKeyedBackend(IntSerializer.INSTANCE, snapshot1);
 
-		backend.injectKeyValueStateSnapshots((HashMap) snapshot1);
-
-		for (String key: snapshot1.keySet()) {
-			snapshot1.get(key).discardState();
-		}
+		snapshot1.discardState();
 
 		ValueState<String> restored1 = backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, kvId);
 		@SuppressWarnings("unchecked")
-		KvState<Integer, VoidNamespace, ?, ?, B> restoredKvState1 = (KvState<Integer, VoidNamespace, ?, ?, B>) restored1;
+		KvState<VoidNamespace> restoredKvState1 = (KvState<VoidNamespace>) restored1;
 
 		backend.setCurrentKey(1);
 		assertEquals("1", restored1.value());
@@ -174,18 +214,14 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 		assertEquals("2", restored1.value());
 		assertEquals("2", getSerializedValue(restoredKvState1, 2, keySerializer, VoidNamespace.INSTANCE, namespaceSerializer, valueSerializer));
 
-		backend.discardState();
-		backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
+		backend.close();
+		backend = restoreKeyedBackend(IntSerializer.INSTANCE, snapshot2);
 
-		backend.injectKeyValueStateSnapshots((HashMap) snapshot2);
-
-		for (String key: snapshot2.keySet()) {
-			snapshot2.get(key).discardState();
-		}
+		snapshot2.discardState();
 
 		ValueState<String> restored2 = backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, kvId);
 		@SuppressWarnings("unchecked")
-		KvState<Integer, VoidNamespace, ?, ?, B> restoredKvState2 = (KvState<Integer, VoidNamespace, ?, ?, B>) restored2;
+		KvState<VoidNamespace> restoredKvState2 = (KvState<VoidNamespace>) restored2;
 
 		backend.setCurrentKey(1);
 		assertEquals("u1", restored2.value());
@@ -196,6 +232,68 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 		backend.setCurrentKey(3);
 		assertEquals("u3", restored2.value());
 		assertEquals("u3", getSerializedValue(restoredKvState2, 3, keySerializer, VoidNamespace.INSTANCE, namespaceSerializer, valueSerializer));
+
+		backend.close();
+	}
+
+	@Test
+	@SuppressWarnings("unchecked")
+	public void testMultipleValueStates() throws Exception {
+		CheckpointStreamFactory streamFactory = createStreamFactory();
+
+		KeyedStateBackend<Integer> backend = createKeyedBackend(
+				IntSerializer.INSTANCE,
+				new HashKeyGroupAssigner<Integer>(1),
+				new KeyGroupRange(0, 0),
+				new DummyEnvironment("test_op", 1, 0));
+
+		ValueStateDescriptor<String> desc1 = new ValueStateDescriptor<>("a-string", StringSerializer.INSTANCE, null);
+		ValueStateDescriptor<Integer> desc2 = new ValueStateDescriptor<>("an-integer", IntSerializer.INSTANCE, null);
+
+		desc1.initializeSerializerUnlessSet(new ExecutionConfig());
+		desc2.initializeSerializerUnlessSet(new ExecutionConfig());
+
+		ValueState<String> state1 = backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, desc1);
+		ValueState<Integer> state2 = backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, desc2);
+
+		// some modifications to the state
+		backend.setCurrentKey(1);
+		assertNull(state1.value());
+		assertNull(state2.value());
+		state1.update("1");
+
+		// state2 should still have nothing
+		assertEquals("1", state1.value());
+		assertNull(state2.value());
+		state2.update(13);
+
+		// both have some state now
+		assertEquals("1", state1.value());
+		assertEquals(13, (int) state2.value());
+
+		// draw a snapshot
+		KeyGroupsStateHandle snapshot1 = runSnapshot(backend.snapshot(682375462378L, 2, streamFactory));
+
+		backend.close();
+		backend = restoreKeyedBackend(
+				IntSerializer.INSTANCE,
+				new HashKeyGroupAssigner<Integer>(1),
+				new KeyGroupRange(0, 0),
+				Collections.singletonList(snapshot1),
+				new DummyEnvironment("test_op", 1, 0));
+
+		snapshot1.discardState();
+
+		backend.setCurrentKey(1);
+
+		state1 = backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, desc1);
+		state2 = backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, desc2);
+
+		// verify that they are still the same
+		assertEquals("1", state1.value());
+		assertEquals(13, (int) state2.value());
+
+		backend.close();
 	}
 
 	/**
@@ -217,7 +315,8 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			// alrighty
 		}
 
-		backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
+		CheckpointStreamFactory streamFactory = createStreamFactory();
+		KeyedStateBackend<Integer> backend = createKeyedBackend(IntSerializer.INSTANCE);
 
 		ValueStateDescriptor<Long> kvId = new ValueStateDescriptor<>("id", LongSerializer.INSTANCE, 42L);
 		kvId.initializeSerializerUnlessSet(new ExecutionConfig());
@@ -246,31 +345,24 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 		assertEquals(42L, (long) state.value());
 
 		// draw a snapshot
-		HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> snapshot1 = backend.snapshotPartitionedState(682375462378L, 2);
-
-		for (String key: snapshot1.keySet()) {
-			if (snapshot1.get(key) instanceof AsynchronousKvStateSnapshot) {
-				snapshot1.put(key, ((AsynchronousKvStateSnapshot<?, ?, ?, ?, ?>) snapshot1.get(key)).materialize());
-			}
-		}
-
-		backend.discardState();
-		backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
+		KeyGroupsStateHandle snapshot1 = runSnapshot(backend.snapshot(682375462378L, 2, streamFactory));
 
-		backend.injectKeyValueStateSnapshots((HashMap) snapshot1);
+		backend.close();
+		backend = restoreKeyedBackend(IntSerializer.INSTANCE, snapshot1);
 
-		for (String key: snapshot1.keySet()) {
-			snapshot1.get(key).discardState();
-		}
+		snapshot1.discardState();
 
 		backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, kvId);
+
+		backend.close();
 	}
 
 	@Test
 	@SuppressWarnings("unchecked,rawtypes")
 	public void testListState() {
 		try {
-			backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
+			CheckpointStreamFactory streamFactory = createStreamFactory();
+			KeyedStateBackend<Integer> backend = createKeyedBackend(IntSerializer.INSTANCE);
 
 			ListStateDescriptor<String> kvId = new ListStateDescriptor<>("id", String.class);
 			kvId.initializeSerializerUnlessSet(new ExecutionConfig());
@@ -281,7 +373,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 
 			ListState<String> state = backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, kvId);
 			@SuppressWarnings("unchecked")
-			KvState<Integer, VoidNamespace, ?, ?, B> kvState = (KvState<Integer, VoidNamespace, ?, ?, B>) state;
+			KvState<VoidNamespace> kvState = (KvState<VoidNamespace>) state;
 
 			Joiner joiner = Joiner.on(",");
 			// some modifications to the state
@@ -298,13 +390,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			assertEquals("1", joiner.join(getSerializedList(kvState, 1, keySerializer, VoidNamespace.INSTANCE, namespaceSerializer, valueSerializer)));
 
 			// draw a snapshot
-			HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> snapshot1 = backend.snapshotPartitionedState(682375462378L, 2);
-
-			for (String key: snapshot1.keySet()) {
-				if (snapshot1.get(key) instanceof AsynchronousKvStateSnapshot) {
-					snapshot1.put(key, ((AsynchronousKvStateSnapshot<?, ?, ?, ?, ?>) snapshot1.get(key)).materialize());
-				}
-			}
+			KeyGroupsStateHandle snapshot1 = runSnapshot(backend.snapshot(682375462378L, 2, streamFactory));
 
 			// make some more modifications
 			backend.setCurrentKey(1);
@@ -315,13 +401,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			state.add("u3");
 
 			// draw another snapshot
-			HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> snapshot2 = backend.snapshotPartitionedState(682375462379L, 4);
-
-			for (String key: snapshot2.keySet()) {
-				if (snapshot2.get(key) instanceof AsynchronousKvStateSnapshot) {
-					snapshot2.put(key, ((AsynchronousKvStateSnapshot<?, ?, ?, ?, ?>) snapshot2.get(key)).materialize());
-				}
-			}
+			KeyGroupsStateHandle snapshot2 = runSnapshot(backend.snapshot(682375462379L, 4, streamFactory));
 
 			// validate the original state
 			backend.setCurrentKey(1);
@@ -334,19 +414,14 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			assertEquals("u3", joiner.join(state.get()));
 			assertEquals("u3", joiner.join(getSerializedList(kvState, 3, keySerializer, VoidNamespace.INSTANCE, namespaceSerializer, valueSerializer)));
 
-			backend.discardState();
-
+			backend.close();
 			// restore the first snapshot and validate it
-			backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
-			backend.injectKeyValueStateSnapshots((HashMap) snapshot1);
-
-			for (String key: snapshot1.keySet()) {
-				snapshot1.get(key).discardState();
-			}
+			backend = restoreKeyedBackend(IntSerializer.INSTANCE, snapshot1);
+			snapshot1.discardState();
 
 			ListState<String> restored1 = backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, kvId);
 			@SuppressWarnings("unchecked")
-			KvState<Integer, VoidNamespace, ?, ?, B> restoredKvState1 = (KvState<Integer, VoidNamespace, ?, ?, B>) restored1;
+			KvState<VoidNamespace> restoredKvState1 = (KvState<VoidNamespace>) restored1;
 
 			backend.setCurrentKey(1);
 			assertEquals("1", joiner.join(restored1.get()));
@@ -355,19 +430,14 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			assertEquals("2", joiner.join(restored1.get()));
 			assertEquals("2", joiner.join(getSerializedList(restoredKvState1, 2, keySerializer, VoidNamespace.INSTANCE, namespaceSerializer, valueSerializer)));
 
-			backend.discardState();
-
+			backend.close();
 			// restore the second snapshot and validate it
-			backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
-			backend.injectKeyValueStateSnapshots((HashMap) snapshot2);
-
-			for (String key: snapshot2.keySet()) {
-				snapshot2.get(key).discardState();
-			}
+			backend = restoreKeyedBackend(IntSerializer.INSTANCE, snapshot2);
+			snapshot2.discardState();
 
 			ListState<String> restored2 = backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, kvId);
 			@SuppressWarnings("unchecked")
-			KvState<Integer, VoidNamespace, ?, ?, B> restoredKvState2 = (KvState<Integer, VoidNamespace, ?, ?, B>) restored2;
+			KvState<VoidNamespace> restoredKvState2 = (KvState<VoidNamespace>) restored2;
 
 			backend.setCurrentKey(1);
 			assertEquals("1,u1", joiner.join(restored2.get()));
@@ -378,6 +448,8 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			backend.setCurrentKey(3);
 			assertEquals("u3", joiner.join(restored2.get()));
 			assertEquals("u3", joiner.join(getSerializedList(restoredKvState2, 3, keySerializer, VoidNamespace.INSTANCE, namespaceSerializer, valueSerializer)));
+
+			backend.close();
 		}
 		catch (Exception e) {
 			e.printStackTrace();
@@ -389,7 +461,8 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 	@SuppressWarnings("unchecked")
 	public void testReducingState() {
 		try {
-			backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
+			CheckpointStreamFactory streamFactory = createStreamFactory();
+			KeyedStateBackend<Integer> backend = createKeyedBackend(IntSerializer.INSTANCE);
 
 			ReducingStateDescriptor<String> kvId = new ReducingStateDescriptor<>("id", new AppendingReduce(), String.class);
 			kvId.initializeSerializerUnlessSet(new ExecutionConfig());
@@ -400,7 +473,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 
 			ReducingState<String> state = backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, kvId);
 			@SuppressWarnings("unchecked")
-			KvState<Integer, VoidNamespace, ?, ?, B> kvState = (KvState<Integer, VoidNamespace, ?, ?, B>) state;
+			KvState<VoidNamespace> kvState = (KvState<VoidNamespace>) state;
 
 			// some modifications to the state
 			backend.setCurrentKey(1);
@@ -416,13 +489,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			assertEquals("1", getSerializedValue(kvState, 1, keySerializer, VoidNamespace.INSTANCE, namespaceSerializer, valueSerializer));
 
 			// draw a snapshot
-			HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> snapshot1 = backend.snapshotPartitionedState(682375462378L, 2);
-
-			for (String key: snapshot1.keySet()) {
-				if (snapshot1.get(key) instanceof AsynchronousKvStateSnapshot) {
-					snapshot1.put(key, ((AsynchronousKvStateSnapshot<?, ?, ?, ?, ?>) snapshot1.get(key)).materialize());
-				}
-			}
+			KeyGroupsStateHandle snapshot1 = runSnapshot(backend.snapshot(682375462378L, 2, streamFactory));
 
 			// make some more modifications
 			backend.setCurrentKey(1);
@@ -433,13 +500,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			state.add("u3");
 
 			// draw another snapshot
-			HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> snapshot2 = backend.snapshotPartitionedState(682375462379L, 4);
-
-			for (String key: snapshot2.keySet()) {
-				if (snapshot2.get(key) instanceof AsynchronousKvStateSnapshot) {
-					snapshot2.put(key, ((AsynchronousKvStateSnapshot<?, ?, ?, ?, ?>) snapshot2.get(key)).materialize());
-				}
-			}
+			KeyGroupsStateHandle snapshot2 = runSnapshot(backend.snapshot(682375462379L, 4, streamFactory));
 
 			// validate the original state
 			backend.setCurrentKey(1);
@@ -452,19 +513,14 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			assertEquals("u3", state.get());
 			assertEquals("u3", getSerializedValue(kvState, 3, keySerializer, VoidNamespace.INSTANCE, namespaceSerializer, valueSerializer));
 
-			backend.discardState();
-
+			backend.close();
 			// restore the first snapshot and validate it
-			backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
-			backend.injectKeyValueStateSnapshots((HashMap) snapshot1);
-
-			for (String key: snapshot1.keySet()) {
-				snapshot1.get(key).discardState();
-			}
+			backend = restoreKeyedBackend(IntSerializer.INSTANCE, snapshot1);
+			snapshot1.discardState();
 
 			ReducingState<String> restored1 = backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, kvId);
 			@SuppressWarnings("unchecked")
-			KvState<Integer, VoidNamespace, ?, ?, B> restoredKvState1 = (KvState<Integer, VoidNamespace, ?, ?, B>) restored1;
+			KvState<VoidNamespace> restoredKvState1 = (KvState<VoidNamespace>) restored1;
 
 			backend.setCurrentKey(1);
 			assertEquals("1", restored1.get());
@@ -473,19 +529,14 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			assertEquals("2", restored1.get());
 			assertEquals("2", getSerializedValue(restoredKvState1, 2, keySerializer, VoidNamespace.INSTANCE, namespaceSerializer, valueSerializer));
 
-			backend.discardState();
-
+			backend.close();
 			// restore the second snapshot and validate it
-			backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
-			backend.injectKeyValueStateSnapshots((HashMap) snapshot2);
-
-			for (String key: snapshot2.keySet()) {
-				snapshot2.get(key).discardState();
-			}
+			backend = restoreKeyedBackend(IntSerializer.INSTANCE, snapshot2);
+			snapshot2.discardState();
 
 			ReducingState<String> restored2 = backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, kvId);
 			@SuppressWarnings("unchecked")
-			KvState<Integer, VoidNamespace, ?, ?, B> restoredKvState2 = (KvState<Integer, VoidNamespace, ?, ?, B>) restored2;
+			KvState<VoidNamespace> restoredKvState2 = (KvState<VoidNamespace>) restored2;
 
 			backend.setCurrentKey(1);
 			assertEquals("1,u1", restored2.get());
@@ -496,6 +547,8 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			backend.setCurrentKey(3);
 			assertEquals("u3", restored2.get());
 			assertEquals("u3", getSerializedValue(restoredKvState2, 3, keySerializer, VoidNamespace.INSTANCE, namespaceSerializer, valueSerializer));
+
+			backend.close();
 		}
 		catch (Exception e) {
 			e.printStackTrace();
@@ -507,7 +560,8 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 	@SuppressWarnings("unchecked,rawtypes")
 	public void testFoldingState() {
 		try {
-			backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
+			CheckpointStreamFactory streamFactory = createStreamFactory();
+			KeyedStateBackend<Integer> backend = createKeyedBackend(IntSerializer.INSTANCE);
 
 			FoldingStateDescriptor<Integer, String> kvId = new FoldingStateDescriptor<>("id",
 					"Fold-Initial:",
@@ -521,7 +575,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 
 			FoldingState<Integer, String> state = backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, kvId);
 			@SuppressWarnings("unchecked")
-			KvState<Integer, VoidNamespace, ?, ?, B> kvState = (KvState<Integer, VoidNamespace, ?, ?, B>) state;
+			KvState<VoidNamespace> kvState = (KvState<VoidNamespace>) state;
 
 			// some modifications to the state
 			backend.setCurrentKey(1);
@@ -537,13 +591,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			assertEquals("Fold-Initial:,1", getSerializedValue(kvState, 1, keySerializer, VoidNamespace.INSTANCE, namespaceSerializer, valueSerializer));
 
 			// draw a snapshot
-			HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> snapshot1 = backend.snapshotPartitionedState(682375462378L, 2);
-
-			for (String key: snapshot1.keySet()) {
-				if (snapshot1.get(key) instanceof AsynchronousKvStateSnapshot) {
-					snapshot1.put(key, ((AsynchronousKvStateSnapshot<?, ?, ?, ?, ?>) snapshot1.get(key)).materialize());
-				}
-			}
+			KeyGroupsStateHandle snapshot1 = runSnapshot(backend.snapshot(682375462378L, 2, streamFactory));
 
 			// make some more modifications
 			backend.setCurrentKey(1);
@@ -555,13 +603,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			state.add(103);
 
 			// draw another snapshot
-			HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> snapshot2 = backend.snapshotPartitionedState(682375462379L, 4);
-
-			for (String key: snapshot2.keySet()) {
-				if (snapshot2.get(key) instanceof AsynchronousKvStateSnapshot) {
-					snapshot2.put(key, ((AsynchronousKvStateSnapshot<?, ?, ?, ?, ?>) snapshot2.get(key)).materialize());
-				}
-			}
+			KeyGroupsStateHandle snapshot2 = runSnapshot(backend.snapshot(682375462379L, 4, streamFactory));
 
 			// validate the original state
 			backend.setCurrentKey(1);
@@ -574,19 +616,14 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			assertEquals("Fold-Initial:,103", state.get());
 			assertEquals("Fold-Initial:,103", getSerializedValue(kvState, 3, keySerializer, VoidNamespace.INSTANCE, namespaceSerializer, valueSerializer));
 
-			backend.discardState();
-
+			backend.close();
 			// restore the first snapshot and validate it
-			backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
-			backend.injectKeyValueStateSnapshots((HashMap) snapshot1);
-
-			for (String key: snapshot1.keySet()) {
-				snapshot1.get(key).discardState();
-			}
+			backend = restoreKeyedBackend(IntSerializer.INSTANCE, snapshot1);
+			snapshot1.discardState();
 
 			FoldingState<Integer, String> restored1 = backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, kvId);
 			@SuppressWarnings("unchecked")
-			KvState<Integer, VoidNamespace, ?, ?, B> restoredKvState1 = (KvState<Integer, VoidNamespace, ?, ?, B>) restored1;
+			KvState<VoidNamespace> restoredKvState1 = (KvState<VoidNamespace>) restored1;
 
 			backend.setCurrentKey(1);
 			assertEquals("Fold-Initial:,1", restored1.get());
@@ -595,20 +632,15 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			assertEquals("Fold-Initial:,2", restored1.get());
 			assertEquals("Fold-Initial:,2", getSerializedValue(restoredKvState1, 2, keySerializer, VoidNamespace.INSTANCE, namespaceSerializer, valueSerializer));
 
-			backend.discardState();
-
+			backend.close();
 			// restore the second snapshot and validate it
-			backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
-			backend.injectKeyValueStateSnapshots((HashMap) snapshot2);
-
-			for (String key: snapshot2.keySet()) {
-				snapshot2.get(key).discardState();
-			}
+			backend = restoreKeyedBackend(IntSerializer.INSTANCE, snapshot2);
+			snapshot1.discardState();
 
 			@SuppressWarnings("unchecked")
 			FoldingState<Integer, String> restored2 = backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, kvId);
 			@SuppressWarnings("unchecked")
-			KvState<Integer, VoidNamespace, ?, ?, B> restoredKvState2 = (KvState<Integer, VoidNamespace, ?, ?, B>) restored2;
+			KvState<VoidNamespace> restoredKvState2 = (KvState<VoidNamespace>) restored2;
 
 			backend.setCurrentKey(1);
 			assertEquals("Fold-Initial:,101", restored2.get());
@@ -619,6 +651,8 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			backend.setCurrentKey(3);
 			assertEquals("Fold-Initial:,103", restored2.get());
 			assertEquals("Fold-Initial:,103", getSerializedValue(restoredKvState2, 3, keySerializer, VoidNamespace.INSTANCE, namespaceSerializer, valueSerializer));
+
+			backend.close();
 		}
 		catch (Exception e) {
 			e.printStackTrace();
@@ -626,17 +660,115 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 		}
 	}
 
+	/**
+	 * This test verifies that state is correctly assigned to key groups and that restore
+	 * restores the relevant key groups in the backend.
+	 *
+	 * <p>We have ten key groups. Initially, one backend is responsible for all ten key groups.
+	 * Then we snapshot, split up the state and restore in to backends where each is responsible
+	 * for five key groups. Then we make sure that the state is only available in the correct
+	 * backend.
+	 * @throws Exception
+	 */
+	@Test
+	public void testKeyGroupSnapshotRestore() throws Exception {
+		final int MAX_PARALLELISM = 10;
+
+		CheckpointStreamFactory streamFactory = createStreamFactory();
+
+		HashKeyGroupAssigner<Integer> keyGroupAssigner = new HashKeyGroupAssigner<>(10);
+
+		KeyedStateBackend<Integer> backend = createKeyedBackend(
+				IntSerializer.INSTANCE,
+				keyGroupAssigner,
+				new KeyGroupRange(0, MAX_PARALLELISM - 1),
+				new DummyEnvironment("test", 1, 0));
+
+		ValueStateDescriptor<String> kvId = new ValueStateDescriptor<>("id", String.class, null);
+		kvId.initializeSerializerUnlessSet(new ExecutionConfig());
+
+		ValueState<String> state = backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, kvId);
+
+		// keys that fall into the first half/second half of the key groups, respectively
+		int keyInFirstHalf = 17;
+		int keyInSecondHalf = 42;
+		Random rand = new Random(0);
+
+		// for each key, determine into which half of the key-group space they fall
+		int firstKeyHalf = keyGroupAssigner.getKeyGroupIndex(keyInFirstHalf) * 2 / MAX_PARALLELISM;
+		int secondKeyHalf = keyGroupAssigner.getKeyGroupIndex(keyInSecondHalf) * 2 / MAX_PARALLELISM;
+
+		while (firstKeyHalf == secondKeyHalf) {
+			keyInSecondHalf = rand.nextInt();
+			secondKeyHalf = keyGroupAssigner.getKeyGroupIndex(keyInSecondHalf) * 2 / MAX_PARALLELISM;
+		}
+
+		backend.setCurrentKey(keyInFirstHalf);
+		state.update("ShouldBeInFirstHalf");
+
+		backend.setCurrentKey(keyInSecondHalf);
+		state.update("ShouldBeInSecondHalf");
+
+
+		KeyGroupsStateHandle snapshot = runSnapshot(backend.snapshot(0, 0, streamFactory));
+
+		List<KeyGroupsStateHandle> firstHalfKeyGroupStates = CheckpointCoordinator.getKeyGroupsStateHandles(
+				Collections.singletonList(snapshot),
+				new KeyGroupRange(0, 4));
+
+		List<KeyGroupsStateHandle> secondHalfKeyGroupStates = CheckpointCoordinator.getKeyGroupsStateHandles(
+				Collections.singletonList(snapshot),
+				new KeyGroupRange(5, 9));
+
+		backend.close();
+
+		// backend for the first half of the key group range
+		KeyedStateBackend<Integer> firstHalfBackend = restoreKeyedBackend(
+				IntSerializer.INSTANCE,
+				keyGroupAssigner,
+				new KeyGroupRange(0, 4),
+				firstHalfKeyGroupStates,
+				new DummyEnvironment("test", 1, 0));
+
+		// backend for the second half of the key group range
+		KeyedStateBackend<Integer> secondHalfBackend = restoreKeyedBackend(
+				IntSerializer.INSTANCE,
+				keyGroupAssigner,
+				new KeyGroupRange(5, 9),
+				secondHalfKeyGroupStates,
+				new DummyEnvironment("test", 1, 0));
+
+
+		ValueState<String> firstHalfState = firstHalfBackend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, kvId);
+
+		firstHalfBackend.setCurrentKey(keyInFirstHalf);
+		assertTrue(firstHalfState.value().equals("ShouldBeInFirstHalf"));
+
+		firstHalfBackend.setCurrentKey(keyInSecondHalf);
+		assertTrue(firstHalfState.value() == null);
+
+		ValueState<String> secondHalfState = secondHalfBackend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, kvId);
+
+		secondHalfBackend.setCurrentKey(keyInFirstHalf);
+		assertTrue(secondHalfState.value() == null);
+
+		secondHalfBackend.setCurrentKey(keyInSecondHalf);
+		assertTrue(secondHalfState.value().equals("ShouldBeInSecondHalf"));
+
+		firstHalfBackend.close();
+		secondHalfBackend.close();
+	}
+
 	@Test
 	@SuppressWarnings("unchecked")
 	public void testValueStateRestoreWithWrongSerializers() {
 		try {
-			backend.initializeForJob(new DummyEnvironment("test", 1, 0),
-				"test_op",
-				IntSerializer.INSTANCE);
+			CheckpointStreamFactory streamFactory = createStreamFactory();
+			KeyedStateBackend<Integer> backend = createKeyedBackend(IntSerializer.INSTANCE);
 
 			ValueStateDescriptor<String> kvId = new ValueStateDescriptor<>("id", String.class, null);
 			kvId.initializeSerializerUnlessSet(new ExecutionConfig());
-			
+
 			ValueState<String> state = backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, kvId);
 
 			backend.setCurrentKey(1);
@@ -645,23 +777,12 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			state.update("2");
 
 			// draw a snapshot
-			HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> snapshot1 = backend.snapshotPartitionedState(682375462378L, 2);
-
-			for (String key: snapshot1.keySet()) {
-				if (snapshot1.get(key) instanceof AsynchronousKvStateSnapshot) {
-					snapshot1.put(key, ((AsynchronousKvStateSnapshot<?, ?, ?, ?, ?>) snapshot1.get(key)).materialize());
-				}
-			}
-
-			backend.discardState();
+			KeyGroupsStateHandle snapshot1 = runSnapshot(backend.snapshot(682375462378L, 2, streamFactory));
 
+			backend.close();
 			// restore the first snapshot and validate it
-			backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
-			backend.injectKeyValueStateSnapshots((HashMap) snapshot1);
-
-			for (String key: snapshot1.keySet()) {
-				snapshot1.get(key).discardState();
-			}
+			backend = restoreKeyedBackend(IntSerializer.INSTANCE, snapshot1);
+			snapshot1.discardState();
 
 			@SuppressWarnings("unchecked")
 			TypeSerializer<String> fakeStringSerializer =
@@ -683,6 +804,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			} catch (Exception e) {
 				fail("wrong exception " + e);
 			}
+			backend.close();
 		}
 		catch (Exception e) {
 			e.printStackTrace();
@@ -694,7 +816,8 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 	@SuppressWarnings("unchecked")
 	public void testListStateRestoreWithWrongSerializers() {
 		try {
-			backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
+			CheckpointStreamFactory streamFactory = createStreamFactory();
+			KeyedStateBackend<Integer> backend = createKeyedBackend(IntSerializer.INSTANCE);
 
 			ListStateDescriptor<String> kvId = new ListStateDescriptor<>("id", String.class);
 			ListState<String> state = backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, kvId);
@@ -705,23 +828,12 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			state.add("2");
 
 			// draw a snapshot
-			HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> snapshot1 = backend.snapshotPartitionedState(682375462378L, 2);
-
-			for (String key: snapshot1.keySet()) {
-				if (snapshot1.get(key) instanceof AsynchronousKvStateSnapshot) {
-					snapshot1.put(key, ((AsynchronousKvStateSnapshot<?, ?, ?, ?, ?>) snapshot1.get(key)).materialize());
-				}
-			}
-
-			backend.discardState();
+			KeyGroupsStateHandle snapshot1 = runSnapshot(backend.snapshot(682375462378L, 2, streamFactory));
 
+			backend.close();
 			// restore the first snapshot and validate it
-			backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
-			backend.injectKeyValueStateSnapshots((HashMap) snapshot1);
-
-			for (String key: snapshot1.keySet()) {
-				snapshot1.get(key).discardState();
-			}
+			backend = restoreKeyedBackend(IntSerializer.INSTANCE, snapshot1);
+			snapshot1.discardState();
 
 			@SuppressWarnings("unchecked")
 			TypeSerializer<String> fakeStringSerializer =
@@ -743,6 +855,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			} catch (Exception e) {
 				fail("wrong exception " + e);
 			}
+			backend.close();
 		}
 		catch (Exception e) {
 			e.printStackTrace();
@@ -754,7 +867,8 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 	@SuppressWarnings("unchecked")
 	public void testReducingStateRestoreWithWrongSerializers() {
 		try {
-			backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
+			CheckpointStreamFactory streamFactory = createStreamFactory();
+			KeyedStateBackend<Integer> backend = createKeyedBackend(IntSerializer.INSTANCE);
 
 			ReducingStateDescriptor<String> kvId = new ReducingStateDescriptor<>("id",
 					new AppendingReduce(),
@@ -767,23 +881,12 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			state.add("2");
 
 			// draw a snapshot
-			HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> snapshot1 = backend.snapshotPartitionedState(682375462378L, 2);
-
-			for (String key: snapshot1.keySet()) {
-				if (snapshot1.get(key) instanceof AsynchronousKvStateSnapshot) {
-					snapshot1.put(key, ((AsynchronousKvStateSnapshot<?, ?, ?, ?, ?>) snapshot1.get(key)).materialize());
-				}
-			}
-
-			backend.discardState();
+			KeyGroupsStateHandle snapshot1 = runSnapshot(backend.snapshot(682375462378L, 2, streamFactory));
 
+			backend.close();
 			// restore the first snapshot and validate it
-			backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
-			backend.injectKeyValueStateSnapshots((HashMap) snapshot1);
-
-			for (String key: snapshot1.keySet()) {
-				snapshot1.get(key).discardState();
-			}
+			backend = restoreKeyedBackend(IntSerializer.INSTANCE, snapshot1);
+			snapshot1.discardState();
 
 			@SuppressWarnings("unchecked")
 			TypeSerializer<String> fakeStringSerializer =
@@ -805,6 +908,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			} catch (Exception e) {
 				fail("wrong exception " + e);
 			}
+			backend.close();
 		}
 		catch (Exception e) {
 			e.printStackTrace();
@@ -814,7 +918,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 
 	@Test
 	public void testCopyDefaultValue() throws Exception {
-		backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
+		KeyedStateBackend<Integer> backend = createKeyedBackend(IntSerializer.INSTANCE);
 
 		ValueStateDescriptor<IntValue> kvId = new ValueStateDescriptor<>("id", IntValue.class, new IntValue(-1));
 		kvId.initializeSerializerUnlessSet(new ExecutionConfig());
@@ -831,6 +935,8 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 		assertNotNull(default2);
 		assertEquals(default1, default2);
 		assertFalse(default1 == default2);
+
+		backend.close();
 	}
 
 	/**
@@ -840,7 +946,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 	 */
 	@Test
 	public void testRequireNonNullNamespace() throws Exception {
-		backend.initializeForJob(new DummyEnvironment("test", 1, 0), "test_op", IntSerializer.INSTANCE);
+		KeyedStateBackend<Integer> backend = createKeyedBackend(IntSerializer.INSTANCE);
 
 		ValueStateDescriptor<IntValue> kvId = new ValueStateDescriptor<>("id", IntValue.class, new IntValue(-1));
 		kvId.initializeSerializerUnlessSet(new ExecutionConfig());
@@ -862,6 +968,8 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			fail("Did not throw expected NullPointerException");
 		} catch (NullPointerException ignored) {
 		}
+
+		backend.close();
 	}
 
 	/**
@@ -869,7 +977,13 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 	 * flag and create concurrent variants for internal state structures.
 	 */
 	@SuppressWarnings("unchecked")
-	protected static <B extends AbstractStateBackend> void testConcurrentMapIfQueryable(B backend) throws Exception {
+	protected void testConcurrentMapIfQueryable() throws Exception {
+		KeyedStateBackend<Integer> backend = createKeyedBackend(
+				IntSerializer.INSTANCE,
+				new HashKeyGroupAssigner<Integer>(1),
+				new KeyGroupRange(0, 0),
+				new DummyEnvironment("test_op", 1, 0));
+
 		{
 			// ValueState
 			ValueStateDescriptor<Integer> desc = new ValueStateDescriptor<>(
@@ -884,20 +998,19 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 					VoidNamespaceSerializer.INSTANCE,
 					desc);
 
-			KvState<Integer, VoidNamespace, ?, ?, ?> kvState = (KvState<Integer, VoidNamespace, ?, ?, ?>) state;
+			KvState<VoidNamespace> kvState = (KvState<VoidNamespace>) state;
 			assertTrue(kvState instanceof AbstractHeapState);
 
-			Map<VoidNamespace, Map<Integer, ?>> stateMap = ((AbstractHeapState) kvState).getStateMap();
-			assertTrue(stateMap instanceof ConcurrentHashMap);
-
 			kvState.setCurrentNamespace(VoidNamespace.INSTANCE);
-			kvState.setCurrentKey(1);
+			backend.setCurrentKey(1);
 			state.update(121818273);
 
-			Map<Integer, ?> namespaceMap = stateMap.get(VoidNamespace.INSTANCE);
+			int keyGroupIndex = new HashKeyGroupAssigner<>(1).getKeyGroupIndex(1);
+			StateTable stateTable = ((AbstractHeapState) kvState).getStateTable();
+			assertNotNull("State not set", stateTable.get(keyGroupIndex));
+			assertTrue(stateTable.get(keyGroupIndex) instanceof ConcurrentHashMap);
+			assertTrue(stateTable.get(keyGroupIndex).get(VoidNamespace.INSTANCE) instanceof ConcurrentHashMap);
 
-			assertNotNull("Value not set", namespaceMap);
-			assertTrue(namespaceMap instanceof ConcurrentHashMap);
 		}
 
 		{
@@ -911,20 +1024,18 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 					VoidNamespaceSerializer.INSTANCE,
 					desc);
 
-			KvState<Integer, VoidNamespace, ?, ?, ?> kvState = (KvState<Integer, VoidNamespace, ?, ?, ?>) state;
+			KvState<VoidNamespace> kvState = (KvState<VoidNamespace>) state;
 			assertTrue(kvState instanceof AbstractHeapState);
 
-			Map<VoidNamespace, Map<Integer, ?>> stateMap = ((AbstractHeapState) kvState).getStateMap();
-			assertTrue(stateMap instanceof ConcurrentHashMap);
-
 			kvState.setCurrentNamespace(VoidNamespace.INSTANCE);
-			kvState.setCurrentKey(1);
+			backend.setCurrentKey(1);
 			state.add(121818273);
 
-			Map<Integer, ?> namespaceMap = stateMap.get(VoidNamespace.INSTANCE);
-
-			assertNotNull("List not set", namespaceMap);
-			assertTrue(namespaceMap instanceof ConcurrentHashMap);
+			int keyGroupIndex = new HashKeyGroupAssigner<>(1).getKeyGroupIndex(1);
+			StateTable stateTable = ((AbstractHeapState) kvState).getStateTable();
+			assertNotNull("State not set", stateTable.get(keyGroupIndex));
+			assertTrue(stateTable.get(keyGroupIndex) instanceof ConcurrentHashMap);
+			assertTrue(stateTable.get(keyGroupIndex).get(VoidNamespace.INSTANCE) instanceof ConcurrentHashMap);
 		}
 
 		{
@@ -944,20 +1055,18 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 					VoidNamespaceSerializer.INSTANCE,
 					desc);
 
-			KvState<Integer, VoidNamespace, ?, ?, ?> kvState = (KvState<Integer, VoidNamespace, ?, ?, ?>) state;
+			KvState<VoidNamespace> kvState = (KvState<VoidNamespace>) state;
 			assertTrue(kvState instanceof AbstractHeapState);
 
-			Map<VoidNamespace, Map<Integer, ?>> stateMap = ((AbstractHeapState) kvState).getStateMap();
-			assertTrue(stateMap instanceof ConcurrentHashMap);
-
 			kvState.setCurrentNamespace(VoidNamespace.INSTANCE);
-			kvState.setCurrentKey(1);
+			backend.setCurrentKey(1);
 			state.add(121818273);
 
-			Map<Integer, ?> namespaceMap = stateMap.get(VoidNamespace.INSTANCE);
-
-			assertNotNull("List not set", namespaceMap);
-			assertTrue(namespaceMap instanceof ConcurrentHashMap);
+			int keyGroupIndex = new HashKeyGroupAssigner<>(1).getKeyGroupIndex(1);
+			StateTable stateTable = ((AbstractHeapState) kvState).getStateTable();
+			assertNotNull("State not set", stateTable.get(keyGroupIndex));
+			assertTrue(stateTable.get(keyGroupIndex) instanceof ConcurrentHashMap);
+			assertTrue(stateTable.get(keyGroupIndex).get(VoidNamespace.INSTANCE) instanceof ConcurrentHashMap);
 		}
 
 		{
@@ -977,21 +1086,21 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 					VoidNamespaceSerializer.INSTANCE,
 					desc);
 
-			KvState<Integer, VoidNamespace, ?, ?, ?> kvState = (KvState<Integer, VoidNamespace, ?, ?, ?>) state;
+			KvState<VoidNamespace> kvState = (KvState<VoidNamespace>) state;
 			assertTrue(kvState instanceof AbstractHeapState);
 
-			Map<VoidNamespace, Map<Integer, ?>> stateMap = ((AbstractHeapState) kvState).getStateMap();
-			assertTrue(stateMap instanceof ConcurrentHashMap);
-
 			kvState.setCurrentNamespace(VoidNamespace.INSTANCE);
-			kvState.setCurrentKey(1);
+			backend.setCurrentKey(1);
 			state.add(121818273);
 
-			Map<Integer, ?> namespaceMap = stateMap.get(VoidNamespace.INSTANCE);
-
-			assertNotNull("List not set", namespaceMap);
-			assertTrue(namespaceMap instanceof ConcurrentHashMap);
+			int keyGroupIndex = new HashKeyGroupAssigner<>(1).getKeyGroupIndex(1);
+			StateTable stateTable = ((AbstractHeapState) kvState).getStateTable();
+			assertNotNull("State not set", stateTable.get(keyGroupIndex));
+			assertTrue(stateTable.get(keyGroupIndex) instanceof ConcurrentHashMap);
+			assertTrue(stateTable.get(keyGroupIndex).get(VoidNamespace.INSTANCE) instanceof ConcurrentHashMap);
 		}
+
+		backend.close();
 	}
 
 	/**
@@ -1002,11 +1111,12 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 		DummyEnvironment env = new DummyEnvironment("test", 1, 0);
 		KvStateRegistry registry = env.getKvStateRegistry();
 
+		CheckpointStreamFactory streamFactory = createStreamFactory();
+		KeyedStateBackend<Integer> backend = createKeyedBackend(IntSerializer.INSTANCE, env);
+
 		KvStateRegistryListener listener = mock(KvStateRegistryListener.class);
 		registry.registerListener(listener);
 
-		backend.initializeForJob(env, "test_op", IntSerializer.INSTANCE);
-
 		ValueStateDescriptor<Integer> desc = new ValueStateDescriptor<>(
 				"test",
 				IntSerializer.INSTANCE,
@@ -1020,25 +1130,16 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 				eq(env.getJobID()), eq(env.getJobVertexId()), eq(0), eq("banana"), any(KvStateID.class));
 
 
-		HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> snapshot = backend
-				.snapshotPartitionedState(682375462379L, 4);
-
-		for (String key: snapshot.keySet()) {
-			if (snapshot.get(key) instanceof AsynchronousKvStateSnapshot) {
-				snapshot.put(key, ((AsynchronousKvStateSnapshot<?, ?, ?, ?, ?>) snapshot.get(key)).materialize());
-			}
-		}
+		KeyGroupsStateHandle snapshot = runSnapshot(backend.snapshot(682375462379L, 4, streamFactory));
 
-		// Verify unregistered
-		backend.discardState();
+		backend.close();
 
 		verify(listener, times(1)).notifyKvStateUnregistered(
 				eq(env.getJobID()), eq(env.getJobVertexId()), eq(0), eq("banana"));
-
+		backend.close();
 		// Initialize again
-		backend.initializeForJob(env, "test_op", IntSerializer.INSTANCE);
-
-		backend.injectKeyValueStateSnapshots((HashMap) snapshot);
+		backend = restoreKeyedBackend(IntSerializer.INSTANCE, snapshot, env);
+		snapshot.discardState();
 
 		backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, desc);
 
@@ -1046,6 +1147,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 		verify(listener, times(2)).notifyKvStateRegistered(
 				eq(env.getJobID()), eq(env.getJobVertexId()), eq(0), eq("banana"), any(KvStateID.class));
 
+		backend.close();
 
 	}
 	
@@ -1093,7 +1195,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 	 * if it is not null.
 	 */
 	private static <V, K, N> V getSerializedValue(
-			KvState<K, N, ?, ?, ?> kvState,
+			KvState<N> kvState,
 			K key,
 			TypeSerializer<K> keySerializer,
 			N namespace,
@@ -1117,7 +1219,7 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 	 * if it is not null.
 	 */
 	private static <V, K, N> List<V> getSerializedList(
-			KvState<K, N, ?, ?, ?> kvState,
+			KvState<N> kvState,
 			K key,
 			TypeSerializer<K> keySerializer,
 			N namespace,
@@ -1135,4 +1237,12 @@ public abstract class StateBackendTestBase<B extends AbstractStateBackend> {
 			return KvStateRequestSerializer.deserializeList(serializedValue, valueSerializer);
 		}
 	}
+
+	private KeyGroupsStateHandle runSnapshot(RunnableFuture<KeyGroupsStateHandle> snapshotRunnableFuture) throws Exception {
+		if(!snapshotRunnableFuture.isDone()) {
+			Thread runner = new Thread(snapshotRunnableFuture);
+			runner.start();
+		}
+		return snapshotRunnableFuture.get();
+	}
 }
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/state/filesystem/FsCheckpointStateOutputStreamTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/state/filesystem/FsCheckpointStateOutputStreamTest.java
index 1d45115..a6a555d 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/state/filesystem/FsCheckpointStateOutputStreamTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/state/filesystem/FsCheckpointStateOutputStreamTest.java
@@ -34,29 +34,26 @@ import java.util.Random;
 import static org.junit.Assert.*;
 
 public class FsCheckpointStateOutputStreamTest {
-	
+
 	/** The temp dir, obtained in a platform neutral way */
 	private static final Path TEMP_DIR_PATH = new Path(new File(System.getProperty("java.io.tmpdir")).toURI());
-	
-	
+
+
 	@Test(expected = IllegalArgumentException.class)
 	public void testWrongParameters() {
 		// this should fail
-		new FsStateBackend.FsCheckpointStateOutputStream(
+		new FsCheckpointStreamFactory.FsCheckpointStateOutputStream(
 			TEMP_DIR_PATH, FileSystem.getLocalFileSystem(), 4000, 5000);
 	}
 
 
 	@Test
 	public void testEmptyState() throws Exception {
-		AbstractStateBackend.CheckpointStateOutputStream stream = new FsStateBackend.FsCheckpointStateOutputStream(
-			TEMP_DIR_PATH, FileSystem.getLocalFileSystem(), 1024, 512);
+		FsCheckpointStreamFactory.CheckpointStateOutputStream stream =
+				new FsCheckpointStreamFactory.FsCheckpointStateOutputStream(TEMP_DIR_PATH, FileSystem.getLocalFileSystem(), 1024, 512);
 
 		StreamStateHandle handle = stream.closeAndGetHandle();
-		assertTrue(handle instanceof ByteStreamStateHandle);
-
-		InputStream inStream = handle.openInputStream();
-		assertEquals(-1, inStream.read());
+		assertTrue(handle == null);
 	}
 
 	@Test
@@ -73,17 +70,17 @@ public class FsCheckpointStateOutputStreamTest {
 	public void testStateAboveMemThreshold() throws Exception {
 		runTest(576446, 259, 17, true);
 	}
-	
+
 	@Test
 	public void testZeroThreshold() throws Exception {
 		runTest(16678, 4096, 0, true);
 	}
-	
+
 	private void runTest(int numBytes, int bufferSize, int threshold, boolean expectFile) throws Exception {
-		AbstractStateBackend.CheckpointStateOutputStream stream =
-			new FsStateBackend.FsCheckpointStateOutputStream(
+		FsCheckpointStreamFactory.CheckpointStateOutputStream stream =
+			new FsCheckpointStreamFactory.FsCheckpointStateOutputStream(
 				TEMP_DIR_PATH, FileSystem.getLocalFileSystem(), bufferSize, threshold);
-		
+
 		Random rnd = new Random();
 		byte[] original = new byte[numBytes];
 		byte[] bytes = new byte[original.length];
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskAsyncCallTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskAsyncCallTest.java
index 429fc6b..ab4ca3b 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskAsyncCallTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskAsyncCallTest.java
@@ -159,7 +159,7 @@ public class TaskAsyncCallTest {
 		TaskDeploymentDescriptor tdd = new TaskDeploymentDescriptor(
 				new JobID(), "Job Name", new JobVertexID(), new ExecutionAttemptID(),
 				new SerializedValue<>(new ExecutionConfig()),
-				"Test Task", 0, 1, 0,
+				"Test Task", 1, 0, 1, 0,
 				new Configuration(), new Configuration(),
 				CheckpointsInOrderInvokable.class.getName(),
 				Collections.<ResultPartitionDeploymentDescriptor>emptyList(),
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java
index ce88c09..54cd7c6 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java
@@ -162,7 +162,7 @@ public class TaskManagerTest extends TestLogger {
 				final SerializedValue<ExecutionConfig> executionConfig = new SerializedValue<>(new ExecutionConfig());
 
 				final TaskDeploymentDescriptor tdd = new TaskDeploymentDescriptor(jid, "TestJob", vid, eid, executionConfig,
-						"TestTask", 2, 7, 0, new Configuration(), new Configuration(),
+						"TestTask", 7, 2, 7, 0, new Configuration(), new Configuration(),
 						TestInvokableCorrect.class.getName(),
 						Collections.<ResultPartitionDeploymentDescriptor>emptyList(),
 						Collections.<InputGateDeploymentDescriptor>emptyList(),
@@ -265,7 +265,7 @@ public class TaskManagerTest extends TestLogger {
 				final TaskDeploymentDescriptor tdd1 = new TaskDeploymentDescriptor(
 						jid1, "TestJob1", vid1, eid1,
 						new SerializedValue<>(new ExecutionConfig()),
-						"TestTask1", 1, 5, 0,
+						"TestTask1", 5, 1, 5, 0,
 						new Configuration(), new Configuration(), TestInvokableBlockingCancelable.class.getName(),
 						Collections.<ResultPartitionDeploymentDescriptor>emptyList(),
 						Collections.<InputGateDeploymentDescriptor>emptyList(),
@@ -274,7 +274,7 @@ public class TaskManagerTest extends TestLogger {
 				final TaskDeploymentDescriptor tdd2 = new TaskDeploymentDescriptor(
 						jid2, "TestJob2", vid2, eid2,
 						new SerializedValue<>(new ExecutionConfig()),
-						"TestTask2", 2, 7, 0,
+						"TestTask2", 7, 2, 7, 0,
 						new Configuration(), new Configuration(), TestInvokableBlockingCancelable.class.getName(),
 						Collections.<ResultPartitionDeploymentDescriptor>emptyList(),
 						Collections.<InputGateDeploymentDescriptor>emptyList(),
@@ -403,13 +403,13 @@ public class TaskManagerTest extends TestLogger {
 				final SerializedValue<ExecutionConfig> executionConfig = new SerializedValue<>(new ExecutionConfig());
 
 				final TaskDeploymentDescriptor tdd1 = new TaskDeploymentDescriptor(jid1, "TestJob", vid1, eid1, executionConfig,
-						"TestTask1", 1, 5, 0, new Configuration(), new Configuration(), StoppableInvokable.class.getName(),
+						"TestTask1", 5, 1, 5, 0, new Configuration(), new Configuration(), StoppableInvokable.class.getName(),
 						Collections.<ResultPartitionDeploymentDescriptor>emptyList(),
 						Collections.<InputGateDeploymentDescriptor>emptyList(),
 						new ArrayList<BlobKey>(), Collections.<URL>emptyList(), 0);
 
 				final TaskDeploymentDescriptor tdd2 = new TaskDeploymentDescriptor(jid2, "TestJob", vid2, eid2, executionConfig,
-						"TestTask2", 2, 7, 0, new Configuration(), new Configuration(), TestInvokableBlockingCancelable.class.getName(),
+						"TestTask2", 7, 2, 7, 0, new Configuration(), new Configuration(), TestInvokableBlockingCancelable.class.getName(),
 						Collections.<ResultPartitionDeploymentDescriptor>emptyList(),
 						Collections.<InputGateDeploymentDescriptor>emptyList(),
 						new ArrayList<BlobKey>(), Collections.<URL>emptyList(), 0);
@@ -531,7 +531,7 @@ public class TaskManagerTest extends TestLogger {
 				final TaskDeploymentDescriptor tdd1 = new TaskDeploymentDescriptor(
 						jid, "TestJob", vid1, eid1,
 						new SerializedValue<>(new ExecutionConfig()),
-						"Sender", 0, 1, 0,
+						"Sender", 1, 0, 1, 0,
 						new Configuration(), new Configuration(), Tasks.Sender.class.getName(),
 						Collections.<ResultPartitionDeploymentDescriptor>emptyList(),
 						Collections.<InputGateDeploymentDescriptor>emptyList(),
@@ -540,7 +540,7 @@ public class TaskManagerTest extends TestLogger {
 				final TaskDeploymentDescriptor tdd2 = new TaskDeploymentDescriptor(
 						jid, "TestJob", vid2, eid2,
 						new SerializedValue<>(new ExecutionConfig()),
-						"Receiver", 2, 7, 0,
+						"Receiver", 7, 2, 7, 0,
 						new Configuration(), new Configuration(), Tasks.Receiver.class.getName(),
 						Collections.<ResultPartitionDeploymentDescriptor>emptyList(),
 						Collections.<InputGateDeploymentDescriptor>emptyList(),
@@ -636,7 +636,7 @@ public class TaskManagerTest extends TestLogger {
 				final TaskDeploymentDescriptor tdd1 = new TaskDeploymentDescriptor(
 						jid, "TestJob", vid1, eid1,
 						new SerializedValue<>(new ExecutionConfig()),
-						"Sender", 0, 1, 0,
+						"Sender", 1, 0, 1, 0,
 						new Configuration(), new Configuration(), Tasks.Sender.class.getName(),
 						irpdd, Collections.<InputGateDeploymentDescriptor>emptyList(), new ArrayList<BlobKey>(),
 						Collections.<URL>emptyList(), 0);
@@ -644,7 +644,7 @@ public class TaskManagerTest extends TestLogger {
 				final TaskDeploymentDescriptor tdd2 = new TaskDeploymentDescriptor(
 						jid, "TestJob", vid2, eid2,
 						new SerializedValue<>(new ExecutionConfig()),
-						"Receiver", 2, 7, 0,
+						"Receiver", 7, 2, 7, 0,
 						new Configuration(), new Configuration(), Tasks.Receiver.class.getName(),
 						Collections.<ResultPartitionDeploymentDescriptor>emptyList(),
 						Collections.singletonList(ircdd),
@@ -781,7 +781,7 @@ public class TaskManagerTest extends TestLogger {
 				final TaskDeploymentDescriptor tdd1 = new TaskDeploymentDescriptor(
 						jid, "TestJob", vid1, eid1,
 						new SerializedValue<>(new ExecutionConfig()),
-						"Sender", 0, 1, 0,
+						"Sender", 1, 0, 1, 0,
 						new Configuration(), new Configuration(), Tasks.Sender.class.getName(),
 						irpdd, Collections.<InputGateDeploymentDescriptor>emptyList(),
 						new ArrayList<BlobKey>(), Collections.<URL>emptyList(), 0);
@@ -789,7 +789,7 @@ public class TaskManagerTest extends TestLogger {
 				final TaskDeploymentDescriptor tdd2 = new TaskDeploymentDescriptor(
 						jid, "TestJob", vid2, eid2,
 						new SerializedValue<>(new ExecutionConfig()),
-						"Receiver", 2, 7, 0,
+						"Receiver", 7, 2, 7, 0,
 						new Configuration(), new Configuration(), Tasks.BlockingReceiver.class.getName(),
 						Collections.<ResultPartitionDeploymentDescriptor>emptyList(),
 						Collections.singletonList(ircdd),
@@ -929,7 +929,7 @@ public class TaskManagerTest extends TestLogger {
 				final TaskDeploymentDescriptor tdd = new TaskDeploymentDescriptor(
 						jid, "TestJob", vid, eid,
 						new SerializedValue<>(new ExecutionConfig()),
-						"Receiver", 0, 1, 0,
+						"Receiver", 1, 0, 1, 0,
 						new Configuration(), new Configuration(),
 						Tasks.AgnosticReceiver.class.getName(),
 						Collections.<ResultPartitionDeploymentDescriptor>emptyList(),
@@ -1025,7 +1025,7 @@ public class TaskManagerTest extends TestLogger {
 				final TaskDeploymentDescriptor tdd = new TaskDeploymentDescriptor(
 						jid, "TestJob", vid, eid,
 						new SerializedValue<>(new ExecutionConfig()),
-						"Receiver", 0, 1, 0,
+						"Receiver", 1, 0, 1, 0,
 						new Configuration(), new Configuration(),
 						Tasks.AgnosticReceiver.class.getName(),
 						Collections.<ResultPartitionDeploymentDescriptor>emptyList(),
@@ -1104,6 +1104,7 @@ public class TaskManagerTest extends TestLogger {
 						new ExecutionAttemptID(),
 						new SerializedValue<>(new ExecutionConfig()),
 						"Task",
+						1,
 						0,
 						1,
 						0,
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskTest.java
index 2f8e3db..f145b48 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskTest.java
@@ -639,7 +639,7 @@ public class TaskTest {
 		return new TaskDeploymentDescriptor(
 				new JobID(), "Test Job", new JobVertexID(), new ExecutionAttemptID(),
 				execConfig,
-				"Test Task", 0, 1, 0,
+				"Test Task", 1, 0, 1, 0,
 				new Configuration(), new Configuration(),
 				invokable.getName(),
 				Collections.<ResultPartitionDeploymentDescriptor>emptyList(),
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/ContinuousFileReaderOperator.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/ContinuousFileReaderOperator.java
index 6d67560..0c0b81a 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/ContinuousFileReaderOperator.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/ContinuousFileReaderOperator.java
@@ -125,7 +125,7 @@ public class ContinuousFileReaderOperator<OUT, S extends Serializable> extends A
 	}
 
 	@Override
-	public void dispose() {
+	public void dispose() throws Exception {
 		super.dispose();
 
 		// first try to cancel it properly and
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamGraphGenerator.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamGraphGenerator.java
index 81c5c48..f9f26e9 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamGraphGenerator.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamGraphGenerator.java
@@ -152,7 +152,6 @@ public class StreamGraphGenerator {
 
 			if (maxParallelism <= 0) {
 				maxParallelism = transform.getParallelism();
-
 				/**
 				 * TODO: Remove once the parallelism settings works properly in Flink (FLINK-3885)
 				 * Currently, the parallelism will be set to 1 on the JobManager iff it encounters
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/AbstractStreamOperator.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/AbstractStreamOperator.java
index 59ecd15..718c0c7 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/AbstractStreamOperator.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/AbstractStreamOperator.java
@@ -24,13 +24,12 @@ import org.apache.flink.api.common.state.State;
 import org.apache.flink.api.common.state.StateDescriptor;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.api.java.functions.KeySelector;
-import org.apache.flink.metrics.Counter;
 import org.apache.flink.core.fs.FSDataInputStream;
 import org.apache.flink.core.fs.FSDataOutputStream;
-import org.apache.flink.runtime.state.AsynchronousKvStateSnapshot;
+import org.apache.flink.metrics.Counter;
 import org.apache.flink.metrics.MetricGroup;
-import org.apache.flink.runtime.state.AbstractStateBackend;
-import org.apache.flink.runtime.state.KvStateSnapshot;
+import org.apache.flink.runtime.state.KeyGroupRange;
+import org.apache.flink.runtime.state.KeyedStateBackend;
 import org.apache.flink.runtime.state.VoidNamespace;
 import org.apache.flink.runtime.state.VoidNamespaceSerializer;
 import org.apache.flink.streaming.api.graph.StreamConfig;
@@ -38,14 +37,9 @@ import org.apache.flink.streaming.api.watermark.Watermark;
 import org.apache.flink.streaming.runtime.operators.Triggerable;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
 import org.apache.flink.streaming.runtime.tasks.StreamTask;
-import org.apache.flink.util.InstantiationUtil;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.util.HashMap;
-import java.util.Set;
 import java.util.concurrent.ScheduledFuture;
 
 /**
@@ -96,9 +90,10 @@ public abstract class AbstractStreamOperator<OUT>
 	private transient KeySelector<?, ?> stateKeySelector1;
 	private transient KeySelector<?, ?> stateKeySelector2;
 
-	/** The state backend that stores the state and checkpoints for this task */
-	private transient AbstractStateBackend stateBackend;
-	protected MetricGroup metrics;
+	/** Backend for keyed state. This might be empty if we're not on a keyed stream. */
+	private transient KeyedStateBackend<?> keyedStateBackend;
+
+	protected transient MetricGroup metrics;
 
 	// ------------------------------------------------------------------------
 	//  Life Cycle
@@ -116,16 +111,6 @@ public abstract class AbstractStreamOperator<OUT>
 
 		stateKeySelector1 = config.getStatePartitioner(0, getUserCodeClassloader());
 		stateKeySelector2 = config.getStatePartitioner(1, getUserCodeClassloader());
-
-		try {
-			TypeSerializer<Object> keySerializer = config.getStateKeySerializer(getUserCodeClassloader());
-			// if the keySerializer is null we still need to create the state backend
-			// for the non-partitioned state features it provides, such as the state output streams
-			String operatorIdentifier = getClass().getSimpleName() + "_" + config.getVertexID() + "_" + runtimeContext.getIndexOfThisSubtask();
-			stateBackend = container.createStateBackend(operatorIdentifier, keySerializer);
-		} catch (Exception e) {
-			throw new RuntimeException("Could not initialize state backend. ", e);
-		}
 	}
 	
 	public MetricGroup getMetricGroup() {
@@ -141,7 +126,27 @@ public abstract class AbstractStreamOperator<OUT>
 	 * @throws Exception An exception in this method causes the operator to fail.
 	 */
 	@Override
-	public void open() throws Exception {}
+	public void open() throws Exception {
+		try {
+			TypeSerializer<Object> keySerializer = config.getStateKeySerializer(getUserCodeClassloader());
+			// create a keyed state backend if there is keyed state, as indicated by the presence of a key serializer
+			if (null != keySerializer) {
+				ExecutionConfig execConf = container.getEnvironment().getExecutionConfig();;
+
+				KeyGroupRange subTaskKeyGroupRange = KeyGroupRange.computeKeyGroupRangeForOperatorIndex(
+						container.getEnvironment().getTaskInfo().getNumberOfKeyGroups(),
+						container.getEnvironment().getTaskInfo().getNumberOfParallelSubtasks(),
+						container.getIndexInSubtaskGroup());
+
+				keyedStateBackend = container.createKeyedStateBackend(
+						keySerializer,
+						container.getConfiguration().getKeyGroupAssigner(getUserCodeClassloader()),
+						subTaskKeyGroupRange);
+			}
+		} catch (Exception e) {
+			throw new RuntimeException("Could not initialize keyed state backend.", e);
+		}
+	}
 
 	/**
 	 * This method is called after all records have been added to the operators via the methods
@@ -166,69 +171,22 @@ public abstract class AbstractStreamOperator<OUT>
 	 * that the operator has acquired.
 	 */
 	@Override
-	public void dispose() {
-		if (stateBackend != null) {
-			try {
-				stateBackend.close();
-				stateBackend.discardState();
-			} catch (Exception e) {
-				throw new RuntimeException("Error while closing/disposing state backend.", e);
-			}
+	public void dispose() throws Exception {
+		if (keyedStateBackend != null) {
+			keyedStateBackend.close();
 		}
 	}
-	
-	// ------------------------------------------------------------------------
-	//  Checkpointing
-	// ------------------------------------------------------------------------
 
 	@Override
 	public void snapshotState(FSDataOutputStream out,
 			long checkpointId,
-			long timestamp) throws Exception {
-
-		HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> keyedState =
-				stateBackend.snapshotPartitionedState(checkpointId,timestamp);
-
-		// Materialize asynchronous snapshots, if any
-		if (keyedState != null) {
-			Set<String> keys = keyedState.keySet();
-			for (String key: keys) {
-				if (keyedState.get(key) instanceof AsynchronousKvStateSnapshot) {
-					AsynchronousKvStateSnapshot<?, ?, ?, ?, ?> asyncHandle = (AsynchronousKvStateSnapshot<?, ?, ?, ?, ?>) keyedState.get(key);
-					keyedState.put(key, asyncHandle.materialize());
-				}
-			}
-		}
-
-		byte[] serializedSnapshot = InstantiationUtil.serializeObject(keyedState);
-
-		DataOutputStream dos = new DataOutputStream(out);
-		dos.writeInt(serializedSnapshot.length);
-		dos.write(serializedSnapshot);
-
-		dos.flush();
-
-	}
+			long timestamp) throws Exception {}
 
 	@Override
-	public void restoreState(FSDataInputStream in) throws Exception {
-		DataInputStream dis = new DataInputStream(in);
-		int size = dis.readInt();
-		byte[] serializedSnapshot = new byte[size];
-		dis.readFully(serializedSnapshot);
-
-		HashMap<String, KvStateSnapshot> keyedState =
-				InstantiationUtil.deserializeObject(serializedSnapshot, getUserCodeClassloader());
+	public void restoreState(FSDataInputStream in) throws Exception {}
 
-		stateBackend.injectKeyValueStateSnapshots(keyedState);
-	}
-	
 	@Override
-	public void notifyOfCompletedCheckpoint(long checkpointId) throws Exception {
-		if (stateBackend != null) {
-			stateBackend.notifyOfCompletedCheckpoint(checkpointId);
-		}
-	}
+	public void notifyOfCompletedCheckpoint(long checkpointId) throws Exception {}
 
 	// ------------------------------------------------------------------------
 	//  Properties and Services
@@ -265,13 +223,14 @@ public abstract class AbstractStreamOperator<OUT>
 		return runtimeContext;
 	}
 
-	public AbstractStateBackend getStateBackend() {
-		return stateBackend;
+	@SuppressWarnings("rawtypes, unchecked")
+	public <K> KeyedStateBackend<K> getStateBackend() {
+		return (KeyedStateBackend<K>) keyedStateBackend;
 	}
 
 	/**
-	 * Register a timer callback. At the specified time the {@link Triggerable} will be invoked.
-	 * This call is guaranteed to not happen concurrently with method calls on the operator.
+	 * Register a timer callback. At the specified time the provided {@link Triggerable} will
+	 * be invoked. This call is guaranteed to not happen concurrently with method calls on the operator.
 	 *
 	 * @param time The absolute time in milliseconds.
 	 * @param target The target to be triggered.
@@ -291,7 +250,7 @@ public abstract class AbstractStreamOperator<OUT>
 	 * @throws Exception Thrown, if the state backend cannot create the key/value state.
 	 */
 	protected <S extends State> S getPartitionedState(StateDescriptor<S, ?> stateDescriptor) throws Exception {
-		return getStateBackend().getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, stateDescriptor);
+		return getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, stateDescriptor);
 	}
 
 	/**
@@ -302,13 +261,13 @@ public abstract class AbstractStreamOperator<OUT>
 	 */
 	@SuppressWarnings("unchecked")
 	protected <S extends State, N> S getPartitionedState(N namespace, TypeSerializer<N> namespaceSerializer, StateDescriptor<S, ?> stateDescriptor) throws Exception {
-		if (stateBackend != null) {
-			return stateBackend.getPartitionedState(
-				namespace,
-				namespaceSerializer,
-				stateDescriptor);
+		if (keyedStateBackend != null) {
+			return keyedStateBackend.getPartitionedState(
+					namespace,
+					namespaceSerializer,
+					stateDescriptor);
 		} else {
-			throw new RuntimeException("Cannot create partitioned state. The key grouped state " +
+			throw new RuntimeException("Cannot create partitioned state. The keyed state " +
 				"backend has not been set. This indicates that the operator is not " +
 				"partitioned/keyed.");
 		}
@@ -335,15 +294,16 @@ public abstract class AbstractStreamOperator<OUT>
 
 	@SuppressWarnings({"unchecked", "rawtypes"})
 	public void setKeyContext(Object key) {
-		if (stateBackend != null) {
+		if (keyedStateBackend != null) {
 			try {
-				stateBackend.setCurrentKey(key);
+				// need to work around type restrictions
+				@SuppressWarnings("unchecked,rawtypes")
+				KeyedStateBackend rawBackend = (KeyedStateBackend) keyedStateBackend;
+
+				rawBackend.setCurrentKey(key);
 			} catch (Exception e) {
 				throw new RuntimeException("Exception occurred while setting the current key context.", e);
 			}
-		} else {
-			throw new RuntimeException("Could not set the current key context, because the " +
-				"AbstractStateBackend has not been initialized.");
 		}
 	}
 
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/AbstractUdfStreamOperator.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/AbstractUdfStreamOperator.java
index b1bc531..6ac73e7 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/AbstractUdfStreamOperator.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/AbstractUdfStreamOperator.java
@@ -18,8 +18,6 @@
 
 package org.apache.flink.streaming.api.operators;
 
-import java.io.ObjectInputStream;
-import java.io.ObjectOutputStream;
 import java.io.Serializable;
 
 import org.apache.flink.annotation.PublicEvolving;
@@ -35,6 +33,7 @@ import org.apache.flink.streaming.api.checkpoint.Checkpointed;
 import org.apache.flink.streaming.api.graph.StreamConfig;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
 import org.apache.flink.streaming.runtime.tasks.StreamTask;
+import org.apache.flink.util.InstantiationUtil;
 
 import static java.util.Objects.requireNonNull;
 
@@ -49,7 +48,9 @@ import static java.util.Objects.requireNonNull;
  *            The type of the user function
  */
 @PublicEvolving
-public abstract class AbstractUdfStreamOperator<OUT, F extends Function> extends AbstractStreamOperator<OUT> implements OutputTypeConfigurable<OUT> {
+public abstract class AbstractUdfStreamOperator<OUT, F extends Function>
+		extends AbstractStreamOperator<OUT>
+		implements OutputTypeConfigurable<OUT> {
 
 	private static final long serialVersionUID = 1L;
 	
@@ -100,16 +101,11 @@ public abstract class AbstractUdfStreamOperator<OUT, F extends Function> extends
 	}
 
 	@Override
-	public void dispose() {
+	public void dispose() throws Exception {
 		super.dispose();
 		if (!functionsClosed) {
 			functionsClosed = true;
-			try {
-				FunctionUtils.closeFunction(userFunction);
-			}
-			catch (Throwable t) {
-				LOG.error("Exception while closing user function while failing or canceling task", t);
-			}
+			FunctionUtils.closeFunction(userFunction);
 		}
 	}
 
@@ -130,9 +126,7 @@ public abstract class AbstractUdfStreamOperator<OUT, F extends Function> extends
 				udfState = chkFunction.snapshotState(checkpointId, timestamp);
 				if (udfState != null) {
 					out.write(1);
-					ObjectOutputStream os = new ObjectOutputStream(out);
-					os.writeObject(udfState);
-					os.flush();
+					InstantiationUtil.serializeObject(out, udfState);
 				} else {
 					out.write(0);
 				}
@@ -153,8 +147,7 @@ public abstract class AbstractUdfStreamOperator<OUT, F extends Function> extends
 			int hasUdfState = in.read();
 
 			if (hasUdfState == 1) {
-				ObjectInputStream ois = new ObjectInputStream(in);
-				Serializable functionState = (Serializable) ois.readObject();
+				Serializable functionState = InstantiationUtil.deserializeObject(in, getUserCodeClassloader());
 				if (functionState != null) {
 					try {
 						chkFunction.restoreState(functionState);
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/StreamOperator.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/StreamOperator.java
index 3411a60..f1e8160 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/StreamOperator.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/StreamOperator.java
@@ -20,9 +20,9 @@ package org.apache.flink.streaming.api.operators;
 import java.io.Serializable;
 
 import org.apache.flink.annotation.PublicEvolving;
+import org.apache.flink.core.fs.FSDataOutputStream;
 import org.apache.flink.metrics.MetricGroup;
 import org.apache.flink.core.fs.FSDataInputStream;
-import org.apache.flink.core.fs.FSDataOutputStream;
 import org.apache.flink.streaming.api.graph.StreamConfig;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
 import org.apache.flink.streaming.runtime.tasks.StreamTask;
@@ -84,7 +84,7 @@ public interface StreamOperator<OUT> extends Serializable {
 	 * This method is expected to make a thorough effort to release all resources
 	 * that the operator has acquired.
 	 */
-	void dispose();
+	void dispose() throws Exception;
 
 	// ------------------------------------------------------------------------
 	//  state snapshots
@@ -92,8 +92,7 @@ public interface StreamOperator<OUT> extends Serializable {
 
 	/**
 	 * Called to draw a state snapshot from the operator. This method snapshots the operator state
-	 * (if the operator is stateful) and the key/value state (if it is being used and has been
-	 * initialized).
+	 * (if the operator is stateful).
 	 *
 	 * @param out The stream to which we have to write our state.
 	 * @param checkpointId The ID of the checkpoint.
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/GenericWriteAheadSink.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/GenericWriteAheadSink.java
index 8d074cc..35d1108 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/GenericWriteAheadSink.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/GenericWriteAheadSink.java
@@ -24,7 +24,7 @@ import org.apache.flink.core.fs.FSDataOutputStream;
 import org.apache.flink.core.memory.DataInputViewStreamWrapper;
 import org.apache.flink.core.memory.DataOutputViewStreamWrapper;
 import org.apache.flink.runtime.io.disk.InputViewIterator;
-import org.apache.flink.runtime.state.AbstractStateBackend;
+import org.apache.flink.runtime.state.CheckpointStreamFactory;
 import org.apache.flink.runtime.state.StreamStateHandle;
 import org.apache.flink.runtime.util.ReusingMutableToRegularIteratorWrapper;
 import org.apache.flink.streaming.api.operators.AbstractStreamOperator;
@@ -56,9 +56,10 @@ public abstract class GenericWriteAheadSink<IN> extends AbstractStreamOperator<I
 
 	protected static final Logger LOG = LoggerFactory.getLogger(GenericWriteAheadSink.class);
 	private final CheckpointCommitter committer;
-	private transient AbstractStateBackend.CheckpointStateOutputStream out;
+	private transient CheckpointStreamFactory.CheckpointStateOutputStream out;
 	protected final TypeSerializer<IN> serializer;
 	private final String id;
+	private transient CheckpointStreamFactory checkpointStreamFactory;
 
 	private ExactlyOnceState state = new ExactlyOnceState();
 
@@ -76,6 +77,8 @@ public abstract class GenericWriteAheadSink<IN> extends AbstractStreamOperator<I
 		committer.setOperatorSubtaskId(getRuntimeContext().getIndexOfThisSubtask());
 		committer.open();
 		cleanState();
+		checkpointStreamFactory =
+				getContainingTask().createCheckpointStreamFactory(this);
 	}
 
 	public void close() throws Exception {
@@ -184,9 +187,9 @@ public abstract class GenericWriteAheadSink<IN> extends AbstractStreamOperator<I
 	@Override
 	public void processElement(StreamRecord<IN> element) throws Exception {
 		IN value = element.getValue();
-		//generate initial operator state
+		// generate initial operator state
 		if (out == null) {
-			out = getStateBackend().createCheckpointStateOutputStream(0, 0);
+			out = checkpointStreamFactory.createCheckpointStateOutputStream(0, 0);
 		}
 		serializer.serialize(value, new DataOutputViewStreamWrapper(out));
 	}
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/windowing/AbstractAlignedProcessingTimeWindowOperator.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/windowing/AbstractAlignedProcessingTimeWindowOperator.java
index 2c95099..e74dd87 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/windowing/AbstractAlignedProcessingTimeWindowOperator.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/windowing/AbstractAlignedProcessingTimeWindowOperator.java
@@ -125,7 +125,7 @@ public abstract class AbstractAlignedProcessingTimeWindowOperator<KEY, IN, OUT,
 		
 		// decide when to first compute the window and when to slide it
 		// the values should align with the start of time (that is, the UNIX epoch, not the big bang)
-		final long now = System.currentTimeMillis();
+		final long now = getRuntimeContext().getCurrentProcessingTime();
 		nextEvaluationTime = now + windowSlide - (now % windowSlide);
 		nextSlideTime = now + paneSize - (now % paneSize);
 
@@ -178,7 +178,7 @@ public abstract class AbstractAlignedProcessingTimeWindowOperator<KEY, IN, OUT,
 	}
 
 	@Override
-	public void dispose() {
+	public void dispose() throws Exception {
 		super.dispose();
 		
 		// acquire the lock during shutdown, to prevent trigger calls at the same time
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperator.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperator.java
index dbdd660..25ec519 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperator.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperator.java
@@ -277,7 +277,7 @@ public class WindowOperator<K, IN, ACC, OUT, W extends Window>
 	}
 
 	@Override
-	public void dispose() {
+	public void dispose() throws Exception {
 		super.dispose();
 		timestampedCollector = null;
 		watermarkTimers = null;
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java
index 074257c..02579aa 100644
--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java
@@ -19,6 +19,7 @@ package org.apache.flink.streaming.runtime.tasks;
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.api.common.accumulators.Accumulator;
+import org.apache.flink.api.common.state.KeyGroupAssigner;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.configuration.ConfigConstants;
 import org.apache.flink.configuration.Configuration;
@@ -30,7 +31,10 @@ import org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable;
 import org.apache.flink.runtime.jobgraph.tasks.StatefulTask;
 import org.apache.flink.runtime.state.AbstractStateBackend;
 import org.apache.flink.runtime.state.ChainedStateHandle;
+import org.apache.flink.runtime.state.CheckpointStreamFactory;
+import org.apache.flink.runtime.state.KeyGroupRange;
 import org.apache.flink.runtime.state.KeyGroupsStateHandle;
+import org.apache.flink.runtime.state.KeyedStateBackend;
 import org.apache.flink.runtime.state.StateBackendFactory;
 import org.apache.flink.runtime.state.StreamStateHandle;
 import org.apache.flink.runtime.state.filesystem.FsStateBackend;
@@ -40,7 +44,6 @@ import org.apache.flink.runtime.taskmanager.DispatcherThreadFactory;
 import org.apache.flink.runtime.util.event.EventListener;
 import org.apache.flink.streaming.api.TimeCharacteristic;
 import org.apache.flink.streaming.api.graph.StreamConfig;
-import org.apache.flink.streaming.api.operators.AbstractStreamOperator;
 import org.apache.flink.streaming.api.operators.Output;
 import org.apache.flink.streaming.api.operators.StreamOperator;
 import org.apache.flink.streaming.runtime.io.RecordWriterOutput;
@@ -50,6 +53,7 @@ import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import java.io.Closeable;
+import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
@@ -57,6 +61,9 @@ import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.RunnableFuture;
 import java.util.concurrent.ScheduledFuture;
 import java.util.concurrent.ScheduledThreadPoolExecutor;
 
@@ -130,6 +137,12 @@ public abstract class StreamTask<OUT, Operator extends StreamOperator<OUT>>
 	/** The class loader used to load dynamic classes of a job */
 	private ClassLoader userClassLoader;
 
+	/** Our state backend. We use this to create checkpoint streams and a keyed state backend. */
+	private AbstractStateBackend stateBackend;
+
+	/** Keyed state backend for the head operator, if it is keyed. There can only ever be one. */
+	private KeyedStateBackend<?> keyedStateBackend;
+
 	/**
 	 * The internal {@link TimeServiceProvider} used to define the current
 	 * processing time (default = {@code System.currentTimeMillis()}) and
@@ -152,7 +165,7 @@ public abstract class StreamTask<OUT, Operator extends StreamOperator<OUT>>
 	private volatile AsynchronousException asyncException;
 
 	/** The currently active background materialization threads */
-	private final Set<Closeable> cancelables = new HashSet<Closeable>();
+	private final Set<Closeable> cancelables = new HashSet<>();
 	
 	/** Flag to mark the task "in operation", in which case check
 	 * needs to be initialized to true, so that early cancel() before invoke() behaves correctly */
@@ -163,6 +176,8 @@ public abstract class StreamTask<OUT, Operator extends StreamOperator<OUT>>
 
 	private long lastCheckpointSize = 0;
 
+	private ExecutorService asyncOperationsThreadPool;
+
 	// ------------------------------------------------------------------------
 	//  Life cycle methods for specific implementations
 	// ------------------------------------------------------------------------
@@ -205,9 +220,14 @@ public abstract class StreamTask<OUT, Operator extends StreamOperator<OUT>>
 			// -------- Initialize ---------
 			LOG.debug("Initializing {}", getName());
 
+			asyncOperationsThreadPool = Executors.newCachedThreadPool();
+
 			userClassLoader = getUserCodeClassLoader();
 
 			configuration = new StreamConfig(getTaskConfiguration());
+
+			stateBackend = createStateBackend();
+
 			accumulatorMap = getEnvironment().getAccumulatorRegistry().getUserMap();
 
 			// if the clock is not already set, then assign a default TimeServiceProvider
@@ -252,8 +272,7 @@ public abstract class StreamTask<OUT, Operator extends StreamOperator<OUT>>
 			// first order of business is to give operators back their state
 			restoreState();
 			lazyRestoreChainedOperatorState = null; // GC friendliness
-			lazyRestoreKeyGroupStates = null; // GC friendliness
-			
+
 			// we need to make sure that any triggers scheduled in open() cannot be
 			// executed before all operators are opened
 			synchronized (lock) {
@@ -292,6 +311,9 @@ public abstract class StreamTask<OUT, Operator extends StreamOperator<OUT>>
 			// still let the computation fail
 			tryDisposeAllOperators();
 			disposed = true;
+
+			// Don't forget to check and throw exceptions that happened in async thread one last time
+			checkTimerException();
 		}
 		finally {
 			// clean up everything we initialized
@@ -307,16 +329,17 @@ public abstract class StreamTask<OUT, Operator extends StreamOperator<OUT>>
 					LOG.error("Could not shut down timer service", t);
 				}
 			}
-			
+
 			// stop all asynchronous checkpoint threads
 			try {
 				closeAllClosables();
+				shutdownAsyncThreads();
 			}
 			catch (Throwable t) {
 				// catch and log the exception to not replace the original exception
 				LOG.error("Could not shut down async checkpoint threads", t);
 			}
-			
+
 			// release the output resources. this method should never fail.
 			if (operatorChain != null) {
 				operatorChain.releaseOutputs();
@@ -330,7 +353,7 @@ public abstract class StreamTask<OUT, Operator extends StreamOperator<OUT>>
 				// catch and log the exception to not replace the original exception
 				LOG.error("Error during cleanup of stream task", t);
 			}
-			
+
 			// if the operators were not disposed before, do a hard dispose
 			if (!disposed) {
 				disposeAllOperators();
@@ -414,6 +437,12 @@ public abstract class StreamTask<OUT, Operator extends StreamOperator<OUT>>
 		}
 	}
 
+	private void shutdownAsyncThreads() throws Exception {
+		if (!asyncOperationsThreadPool.isShutdown()) {
+			asyncOperationsThreadPool.shutdownNow();
+		}
+	}
+
 	/**
 	 * Execute the operator-specific {@link StreamOperator#dispose()} method in each
 	 * of the operators in the chain of this {@link StreamTask}. </b> Disposing happens
@@ -558,69 +587,6 @@ public abstract class StreamTask<OUT, Operator extends StreamOperator<OUT>>
 				cancelables.remove(lazyRestoreChainedOperatorState);
 			}
 		}
-//		if (lazyRestoreState != null || lazyRestoreKeyGroupStates != null) {
-//
-//			LOG.info("Restoring checkpointed state to task {}", getName());
-//
-//			final StreamOperator<?>[] allOperators = operatorChain.getAllOperators();
-//			StreamOperatorNonPartitionedState[] nonPartitionedStates;
-//
-//			final List<Map<Integer, PartitionedStateSnapshot>> keyGroupStates = new ArrayList<Map<Integer, PartitionedStateSnapshot>>(allOperators.length);
-//
-//			for (int i = 0; i < allOperators.length; i++) {
-//				keyGroupStates.add(new HashMap<Integer, PartitionedStateSnapshot>());
-//			}
-//
-//			if (lazyRestoreState != null) {
-//				try {
-//					nonPartitionedStates = lazyRestoreState.get(getUserCodeClassLoader());
-//
-//					// be GC friendly
-//					lazyRestoreState = null;
-//				} catch (Exception e) {
-//					throw new Exception("Could not restore checkpointed non-partitioned state.", e);
-//				}
-//			} else {
-//				nonPartitionedStates = new StreamOperatorNonPartitionedState[allOperators.length];
-//			}
-//
-//			if (lazyRestoreKeyGroupStates != null) {
-//				try {
-//					// construct key groups state for operators
-//					for (Map.Entry<Integer, ChainedStateHandle> lazyRestoreKeyGroupState : lazyRestoreKeyGroupStates.entrySet()) {
-//						int keyGroupId = lazyRestoreKeyGroupState.getKey();
-//
-//						Map<Integer, PartitionedStateSnapshot> chainedKeyGroupStates = lazyRestoreKeyGroupState.getValue().get(getUserCodeClassLoader());
-//
-//						for (Map.Entry<Integer, PartitionedStateSnapshot> chainedKeyGroupState : chainedKeyGroupStates.entrySet()) {
-//							int chainedIndex = chainedKeyGroupState.getKey();
-//
-//							Map<Integer, PartitionedStateSnapshot> keyGroupState;
-//
-//							keyGroupState = keyGroupStates.get(chainedIndex);
-//							keyGroupState.put(keyGroupId, chainedKeyGroupState.getValue());
-//						}
-//					}
-//
-//					lazyRestoreKeyGroupStates = null;
-//
-//				} catch (Exception e) {
-//					throw new Exception("Could not restore checkpointed partitioned state.", e);
-//				}
-//			}
-//
-//			for (int i = 0; i < nonPartitionedStates.length; i++) {
-//				StreamOperatorNonPartitionedState nonPartitionedState = nonPartitionedStates[i];
-//				StreamOperator<?> operator = allOperators[i];
-//				KeyGroupsStateHandle partitionedState = new KeyGroupsStateHandle(keyGroupStates.get(i));
-//				StreamOperatorState operatorState = new StreamOperatorState(partitionedState, nonPartitionedState);
-//
-//				if (operator != null) {
-//					LOG.debug("Restore state of task {} in chain ({}).", i, getName());
-//					operator.restoreState(operatorState, recoveryTimestamp);
-//				}
-//			}
-//		}
 	}
 
 	@Override
@@ -658,8 +624,15 @@ public abstract class StreamTask<OUT, Operator extends StreamOperator<OUT>>
 					StreamOperator<?> operator = allOperators[i];
 
 					if (operator != null) {
-						AbstractStateBackend.CheckpointStateOutputStream outStream =
-								((AbstractStreamOperator) operator).getStateBackend().createCheckpointStateOutputStream(checkpointId, timestamp);
+						CheckpointStreamFactory streamFactory =
+								stateBackend.createStreamFactory(
+										getEnvironment().getJobID(),
+										createOperatorIdentifier(
+												operator,
+												configuration.getVertexID()));
+
+						CheckpointStreamFactory.CheckpointStateOutputStream outStream =
+								streamFactory.createCheckpointStateOutputStream(checkpointId, timestamp);
 
 						operator.snapshotState(outStream, checkpointId, timestamp);
 
@@ -667,22 +640,37 @@ public abstract class StreamTask<OUT, Operator extends StreamOperator<OUT>>
 					}
 				}
 
-				if (!isRunning) {
-					// Rethrow the cancel exception because some state backends could swallow
-					// exceptions and seem to exit cleanly.
-					throw new CancelTaskException();
+				RunnableFuture<KeyGroupsStateHandle> keyGroupsStateHandleFuture = null;
+
+				if (keyedStateBackend != null) {
+					CheckpointStreamFactory streamFactory = stateBackend.createStreamFactory(
+							getEnvironment().getJobID(),
+							createOperatorIdentifier(
+									headOperator,
+									configuration.getVertexID()));
+					keyGroupsStateHandleFuture = keyedStateBackend.snapshot(
+							checkpointId,
+							timestamp,
+							streamFactory);
 				}
 
-				ChainedStateHandle<StreamStateHandle> states = new ChainedStateHandle<>(nonPartitionedStates);
-				List<KeyGroupsStateHandle> keyedStates = Collections.<KeyGroupsStateHandle>emptyList();
+				ChainedStateHandle<StreamStateHandle> chainedStateHandles = new ChainedStateHandle<>(nonPartitionedStates);
 
+				LOG.debug("Finished synchronous checkpoints for checkpoint {} on task {}", checkpointId, getName());
 
-				if (states.isEmpty() && keyedStates.isEmpty()) {
-					getEnvironment().acknowledgeCheckpoint(checkpointId);
-				} else  {
-					this.lastCheckpointSize = states.getStateSize();
-					getEnvironment().acknowledgeCheckpoint(checkpointId, states, keyedStates);
+				AsyncCheckpointRunnable asyncCheckpointRunnable = new AsyncCheckpointRunnable(
+						"checkpoint-" + checkpointId + "-" + timestamp,
+						this,
+						cancelables,
+						chainedStateHandles,
+						keyGroupsStateHandleFuture,
+						checkpointId);
+
+				synchronized (cancelables) {
+					cancelables.add(asyncCheckpointRunnable);
 				}
+
+				asyncOperationsThreadPool.submit(asyncCheckpointRunnable);
 				return true;
 			} else {
 				return false;
@@ -712,7 +700,7 @@ public abstract class StreamTask<OUT, Operator extends StreamOperator<OUT>>
 	//  State backend
 	// ------------------------------------------------------------------------
 
-	public AbstractStateBackend createStateBackend(String operatorIdentifier, TypeSerializer<?> keySerializer) throws Exception {
+	private AbstractStateBackend createStateBackend() throws Exception {
 		AbstractStateBackend stateBackend = configuration.getStateBackend(getUserCodeClassLoader());
 
 		if (stateBackend != null) {
@@ -732,7 +720,7 @@ public abstract class StreamTask<OUT, Operator extends StreamOperator<OUT>>
 			switch (backendName) {
 				case "jobmanager":
 					LOG.info("State backend is set to heap memory (checkpoint to jobmanager)");
-					stateBackend = MemoryStateBackend.create();
+					stateBackend = new MemoryStateBackend();
 					break;
 
 				case "filesystem":
@@ -760,10 +748,69 @@ public abstract class StreamTask<OUT, Operator extends StreamOperator<OUT>>
 					}
 			}
 		}
-		stateBackend.initializeForJob(getEnvironment(), operatorIdentifier, keySerializer);
 		return stateBackend;
 	}
 
+	public <K> KeyedStateBackend<K> createKeyedStateBackend(
+			TypeSerializer<K> keySerializer,
+			KeyGroupAssigner<K> keyGroupAssigner,
+			KeyGroupRange keyGroupRange) throws Exception {
+
+		if (keyedStateBackend != null) {
+			throw new RuntimeException("The keyed state backend can only be created once.");
+		}
+
+		String operatorIdentifier = createOperatorIdentifier(
+				headOperator,
+				configuration.getVertexID());
+
+		if (lazyRestoreKeyGroupStates != null) {
+			keyedStateBackend = stateBackend.restoreKeyedStateBackend(
+					getEnvironment(),
+					getEnvironment().getJobID(),
+					operatorIdentifier,
+					keySerializer,
+					keyGroupAssigner,
+					keyGroupRange,
+					lazyRestoreKeyGroupStates,
+					getEnvironment().getTaskKvStateRegistry());
+
+			lazyRestoreKeyGroupStates = null; // GC friendliness
+		} else {
+			keyedStateBackend = stateBackend.createKeyedStateBackend(
+					getEnvironment(),
+					getEnvironment().getJobID(),
+					operatorIdentifier,
+					keySerializer,
+					keyGroupAssigner,
+					keyGroupRange,
+					getEnvironment().getTaskKvStateRegistry());
+		}
+
+		return (KeyedStateBackend<K>) keyedStateBackend;
+	}
+
+	/**
+	 * This is only visible because
+	 * {@link org.apache.flink.streaming.runtime.operators.GenericWriteAheadSink} uses the
+	 * checkpoint stream factory to write write-ahead logs. <b>This should not be used for
+	 * anything else.</b>
+	 */
+	public CheckpointStreamFactory createCheckpointStreamFactory(StreamOperator operator) throws IOException {
+		return stateBackend.createStreamFactory(
+				getEnvironment().getJobID(),
+				createOperatorIdentifier(
+						operator,
+						configuration.getVertexID()));
+
+	}
+
+	private String createOperatorIdentifier(StreamOperator operator, int vertexId) {
+		return operator.getClass().getSimpleName() +
+				"_" + vertexId +
+				"_" + getEnvironment().getTaskInfo().getIndexOfThisSubtask();
+	}
+
 	/**
 	 * Registers a timer.
 	 */
@@ -852,77 +899,83 @@ public abstract class StreamTask<OUT, Operator extends StreamOperator<OUT>>
 
 	// ------------------------------------------------------------------------
 	
-//	private static class AsyncCheckpointThread extends Thread implements Closeable {
-//
-//		private final StreamTask<?, ?> owner;
-//
-//		private final Set<Closeable> cancelables;
-//
-//		private final StreamTaskState[] states;
-//
-//		private final long checkpointId;
-//
-//		AsyncCheckpointThread(String name, StreamTask<?, ?> owner, Set<Closeable> cancelables,
-//				StreamTaskState[] states, long checkpointId) {
-//			super(name);
-//			setDaemon(true);
-//
-//			this.owner = owner;
-//			this.cancelables = cancelables;
-//			this.states = states;
-//			this.checkpointId = checkpointId;
-//		}
-//
-//		@Override
-//		public void run() {
-//			try {
-//				for (StreamTaskState state : states) {
-//					if (state != null) {
-//						if (state.getFunctionState() instanceof AsynchronousStateHandle) {
-//							AsynchronousStateHandle<Serializable> asyncState = (AsynchronousStateHandle<Serializable>) state.getFunctionState();
-//							state.setFunctionState(asyncState.materialize());
-//						}
-//						if (state.getOperatorState() instanceof AsynchronousStateHandle) {
-//							AsynchronousStateHandle<?> asyncState = (AsynchronousStateHandle<?>) state.getOperatorState();
-//							state.setOperatorState(asyncState.materialize());
-//						}
-//						if (state.getKvStates() != null) {
-//							Set<String> keys = state.getKvStates().keySet();
-//							HashMap<String, KvStateSnapshot<?, ?, ?, ?, ?>> kvStates = state.getKvStates();
-//							for (String key: keys) {
-//								if (kvStates.get(key) instanceof AsynchronousKvStateSnapshot) {
-//									AsynchronousKvStateSnapshot<?, ?, ?, ?, ?> asyncHandle = (AsynchronousKvStateSnapshot<?, ?, ?, ?, ?>) kvStates.get(key);
-//									kvStates.put(key, asyncHandle.materialize());
-//								}
-//							}
-//						}
-//
-//					}
-//				}
-//				StreamTaskStateList allStates = new StreamTaskStateList(states);
-//				owner.lastCheckpointSize = allStates.getStateSize();
-//				owner.getEnvironment().acknowledgeCheckpoint(checkpointId, allStates);
-//
-//				LOG.debug("Finished asynchronous checkpoints for checkpoint {} on task {}", checkpointId, getName());
-//			}
-//			catch (Exception e) {
-//				if (owner.isRunning()) {
-//					LOG.error("Caught exception while materializing asynchronous checkpoints.", e);
-//				}
-//				if (owner.asyncException == null) {
-//					owner.asyncException = new AsynchronousException(e);
-//				}
-//			}
-//			finally {
-//				synchronized (cancelables) {
-//					cancelables.remove(this);
-//				}
-//			}
-//		}
-//
-//		@Override
-//		public void close() {
-//			interrupt();
-//		}
-//	}
+	private static class AsyncCheckpointRunnable implements Runnable, Closeable {
+
+		private final StreamTask<?, ?> owner;
+
+		private final Set<Closeable> cancelables;
+
+		private final ChainedStateHandle<StreamStateHandle> chainedStateHandles;
+
+		private final RunnableFuture<KeyGroupsStateHandle> keyGroupsStateHandleFuture;
+
+		private final long checkpointId;
+
+		private final String name;
+
+		AsyncCheckpointRunnable(
+				String name,
+				StreamTask<?, ?> owner,
+				Set<Closeable> cancelables,
+				ChainedStateHandle<StreamStateHandle> chainedStateHandles,
+				RunnableFuture<KeyGroupsStateHandle> keyGroupsStateHandleFuture,
+				long checkpointId) {
+
+			this.name = name;
+			this.owner = owner;
+			this.cancelables = cancelables;
+			this.chainedStateHandles = chainedStateHandles;
+			this.keyGroupsStateHandleFuture = keyGroupsStateHandleFuture;
+			this.checkpointId = checkpointId;
+		}
+
+		@Override
+		public void run() {
+			try {
+
+				List<KeyGroupsStateHandle> keyedStates = Collections.emptyList();
+
+				if (keyGroupsStateHandleFuture != null) {
+
+					if (!keyGroupsStateHandleFuture.isDone()) {
+						//TODO this currently works because we only have one RunnableFuture
+						keyGroupsStateHandleFuture.run();
+					}
+
+					KeyGroupsStateHandle keyGroupsStateHandle = this.keyGroupsStateHandleFuture.get();
+					if (keyGroupsStateHandle != null) {
+						keyedStates = Arrays.asList(keyGroupsStateHandle);
+					}
+				}
+
+				if (chainedStateHandles.isEmpty() && keyedStates.isEmpty()) {
+					owner.getEnvironment().acknowledgeCheckpoint(checkpointId);
+				} else  {
+					owner. getEnvironment().acknowledgeCheckpoint(checkpointId, chainedStateHandles, keyedStates);
+				}
+
+				LOG.debug("Finished asynchronous checkpoints for checkpoint {} on task {}", checkpointId, name);
+			}
+			catch (Exception e) {
+				if (owner.isRunning()) {
+					LOG.error("Caught exception while materializing asynchronous checkpoints.", e);
+				}
+				if (owner.asyncException == null) {
+					owner.asyncException = new AsynchronousException(e);
+				}
+			}
+			finally {
+				synchronized (cancelables) {
+					cancelables.remove(this);
+				}
+			}
+		}
+
+		@Override
+		public void close() {
+			if (keyGroupsStateHandleFuture != null) {
+				keyGroupsStateHandleFuture.cancel(true);
+			}
+		}
+	}
 }
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/StreamGroupedFoldTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/StreamGroupedFoldTest.java
index f6e7e6b..68a2bb2 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/StreamGroupedFoldTest.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/StreamGroupedFoldTest.java
@@ -27,6 +27,7 @@ import org.apache.flink.api.java.functions.KeySelector;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.streaming.api.watermark.Watermark;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
+import org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness;
 import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;
 import org.apache.flink.streaming.util.TestHarnessUtil;
 
@@ -69,8 +70,8 @@ public class StreamGroupedFoldTest {
 		StreamGroupedFold<Integer, String, String> operator = new StreamGroupedFold<>(new MyFolder(), "100");
 		operator.setOutputType(BasicTypeInfo.STRING_TYPE_INFO, new ExecutionConfig());
 
-		OneInputStreamOperatorTestHarness<Integer, String> testHarness = new OneInputStreamOperatorTestHarness<>(operator);
-		testHarness.configureForKeyedStream(keySelector, BasicTypeInfo.STRING_TYPE_INFO);
+		OneInputStreamOperatorTestHarness<Integer, String> testHarness =
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, keySelector, BasicTypeInfo.STRING_TYPE_INFO);
 
 		long initialTime = 0L;
 		ConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();
@@ -107,10 +108,9 @@ public class StreamGroupedFoldTest {
 				new TestOpenCloseFoldFunction(), "init");
 		operator.setOutputType(BasicTypeInfo.STRING_TYPE_INFO, new ExecutionConfig());
 
-		OneInputStreamOperatorTestHarness<Integer, String> testHarness = new OneInputStreamOperatorTestHarness<>(operator);
-		testHarness.configureForKeyedStream(keySelector, BasicTypeInfo.INT_TYPE_INFO);
-		
-		
+		OneInputStreamOperatorTestHarness<Integer, String> testHarness =
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, keySelector, BasicTypeInfo.INT_TYPE_INFO);
+
 		long initialTime = 0L;
 
 		testHarness.open();
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/StreamGroupedReduceTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/StreamGroupedReduceTest.java
index 6cb46c9..0f304a0 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/StreamGroupedReduceTest.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/StreamGroupedReduceTest.java
@@ -28,6 +28,7 @@ import org.apache.flink.api.java.functions.KeySelector;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.streaming.api.watermark.Watermark;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
+import org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness;
 import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;
 import org.apache.flink.streaming.util.TestHarnessUtil;
 import org.junit.Assert;
@@ -52,8 +53,8 @@ public class StreamGroupedReduceTest {
 		
 		StreamGroupedReduce<Integer> operator = new StreamGroupedReduce<>(new MyReducer(), IntSerializer.INSTANCE);
 
-		OneInputStreamOperatorTestHarness<Integer, Integer> testHarness = new OneInputStreamOperatorTestHarness<>(operator);
-		testHarness.configureForKeyedStream(keySelector, BasicTypeInfo.INT_TYPE_INFO);
+		OneInputStreamOperatorTestHarness<Integer, Integer> testHarness =
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, keySelector, BasicTypeInfo.INT_TYPE_INFO);
 
 		long initialTime = 0L;
 		ConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();
@@ -84,8 +85,8 @@ public class StreamGroupedReduceTest {
 		
 		StreamGroupedReduce<Integer> operator =
 				new StreamGroupedReduce<>(new TestOpenCloseReduceFunction(), IntSerializer.INSTANCE);
-		OneInputStreamOperatorTestHarness<Integer, Integer> testHarness = new OneInputStreamOperatorTestHarness<>(operator);
-		testHarness.configureForKeyedStream(keySelector, BasicTypeInfo.INT_TYPE_INFO);
+		OneInputStreamOperatorTestHarness<Integer, Integer> testHarness =
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, keySelector, BasicTypeInfo.INT_TYPE_INFO);
 
 		long initialTime = 0L;
 
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/StreamingRuntimeContextTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/StreamingRuntimeContextTest.java
index 3a88d94..d3b7ff9 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/StreamingRuntimeContextTest.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/StreamingRuntimeContextTest.java
@@ -19,6 +19,7 @@
 package org.apache.flink.streaming.api.operators;
 
 import org.apache.flink.api.common.ExecutionConfig;
+import org.apache.flink.api.common.JobID;
 import org.apache.flink.api.common.TaskInfo;
 import org.apache.flink.api.common.accumulators.Accumulator;
 import org.apache.flink.api.common.functions.ReduceFunction;
@@ -28,13 +29,21 @@ import org.apache.flink.api.common.state.ReducingStateDescriptor;
 import org.apache.flink.api.common.state.StateDescriptor;
 import org.apache.flink.api.common.state.ValueStateDescriptor;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.api.common.typeutils.base.IntSerializer;
 import org.apache.flink.api.common.typeutils.base.StringSerializer;
 import org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer;
 import org.apache.flink.core.fs.Path;
 import org.apache.flink.runtime.execution.Environment;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+import org.apache.flink.runtime.operators.testutils.DummyEnvironment;
+import org.apache.flink.runtime.query.KvStateRegistry;
+import org.apache.flink.runtime.state.HashKeyGroupAssigner;
+import org.apache.flink.runtime.state.KeyGroupRange;
+import org.apache.flink.runtime.state.KeyedStateBackend;
 import org.apache.flink.runtime.state.VoidNamespace;
 import org.apache.flink.runtime.state.VoidNamespaceSerializer;
-import org.apache.flink.runtime.state.memory.MemListState;
+import org.apache.flink.runtime.state.heap.HeapListState;
+import org.apache.flink.runtime.state.memory.MemoryStateBackend;
 import org.junit.Test;
 import org.mockito.invocation.InvocationOnMock;
 import org.mockito.stubbing.Answer;
@@ -181,11 +190,18 @@ public class StreamingRuntimeContextTest {
 					@Override
 					public ListState<String> answer(InvocationOnMock invocationOnMock) throws Throwable {
 						ListStateDescriptor<String> descr =
-							(ListStateDescriptor<String>) invocationOnMock.getArguments()[0];
-						MemListState<String, VoidNamespace, String> listState = new MemListState<>(
-								StringSerializer.INSTANCE, VoidNamespaceSerializer.INSTANCE, descr);
-						listState.setCurrentNamespace(VoidNamespace.INSTANCE);
-						return listState;
+								(ListStateDescriptor<String>) invocationOnMock.getArguments()[0];
+						KeyedStateBackend<Integer> backend = new MemoryStateBackend().createKeyedStateBackend(
+								new DummyEnvironment("test_task", 1, 0),
+								new JobID(),
+								"test_op",
+								IntSerializer.INSTANCE,
+								new HashKeyGroupAssigner<Integer>(1),
+								new KeyGroupRange(0, 0),
+								new KvStateRegistry().createTaskRegistry(new JobID(),
+										new JobVertexID()));
+						backend.setCurrentKey(0);
+						return backend.getPartitionedState(VoidNamespace.INSTANCE, VoidNamespaceSerializer.INSTANCE, descr);
 					}
 				});
 
@@ -196,7 +212,7 @@ public class StreamingRuntimeContextTest {
 		Environment env = mock(Environment.class);
 		when(env.getUserClassLoader()).thenReturn(StreamingRuntimeContextTest.class.getClassLoader());
 		when(env.getDistributedCacheEntries()).thenReturn(Collections.<String, Future<Path>>emptyMap());
-		when(env.getTaskInfo()).thenReturn(new TaskInfo("test task", 0, 1, 1));
+		when(env.getTaskInfo()).thenReturn(new TaskInfo("test task", 1, 0, 1, 1));
 		return env;
 	}
 }
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/StreamOperatorChainingTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/StreamOperatorChainingTest.java
index 3b201dc..f4ac5b2 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/StreamOperatorChainingTest.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/StreamOperatorChainingTest.java
@@ -333,21 +333,6 @@ public class StreamOperatorChainingTest {
 		when(mockTask.getEnvironment()).thenReturn(env);
 		when(mockTask.getExecutionConfig()).thenReturn(new ExecutionConfig().enableObjectReuse());
 
-		try {
-			doAnswer(new Answer<AbstractStateBackend>() {
-				@Override
-				public AbstractStateBackend answer(InvocationOnMock invocationOnMock) throws Throwable {
-					final String operatorIdentifier = (String) invocationOnMock.getArguments()[0];
-					final TypeSerializer<?> keySerializer = (TypeSerializer<?>) invocationOnMock.getArguments()[1];
-					MemoryStateBackend backend = MemoryStateBackend.create();
-					backend.initializeForJob(env, operatorIdentifier, keySerializer);
-					return backend;
-				}
-			}).when(mockTask).createStateBackend(any(String.class), any(TypeSerializer.class));
-		} catch (Exception e) {
-			throw new RuntimeException(e.getMessage(), e);
-		}
-
 		return mockTask;
 	}
 
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/AccumulatingAlignedProcessingTimeWindowOperatorTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/AccumulatingAlignedProcessingTimeWindowOperatorTest.java
index 2cb1809..40a6c79 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/AccumulatingAlignedProcessingTimeWindowOperatorTest.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/AccumulatingAlignedProcessingTimeWindowOperatorTest.java
@@ -21,10 +21,9 @@ package org.apache.flink.streaming.runtime.operators.windowing;
 import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.TaskInfo;
 import org.apache.flink.api.common.accumulators.Accumulator;
-import org.apache.flink.api.common.state.KeyGroupAssigner;
 import org.apache.flink.api.common.state.ValueState;
 import org.apache.flink.api.common.state.ValueStateDescriptor;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;
 import org.apache.flink.api.common.typeutils.base.IntSerializer;
 import org.apache.flink.api.common.typeutils.base.StringSerializer;
 import org.apache.flink.api.java.ClosureCleaner;
@@ -33,9 +32,6 @@ import org.apache.flink.configuration.ConfigConstants;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.runtime.execution.Environment;
 import org.apache.flink.runtime.operators.testutils.UnregisteredTaskMetricsGroup;
-import org.apache.flink.runtime.state.AbstractStateBackend;
-import org.apache.flink.runtime.state.memory.MemoryStateBackend;
-import org.apache.flink.runtime.state.HashKeyGroupAssigner;
 import org.apache.flink.runtime.state.StreamStateHandle;
 import org.apache.flink.runtime.taskmanager.TaskManagerRuntimeInfo;
 import org.apache.flink.streaming.api.functions.windowing.RichWindowFunction;
@@ -46,11 +42,12 @@ import org.apache.flink.streaming.api.windowing.windows.TimeWindow;
 import org.apache.flink.streaming.runtime.operators.Triggerable;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
 import org.apache.flink.streaming.runtime.tasks.StreamTask;
+import org.apache.flink.streaming.runtime.tasks.TestTimeServiceProvider;
+import org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness;
 import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;
 import org.apache.flink.util.Collector;
 
 import org.junit.After;
-import org.junit.Ignore;
 import org.junit.Test;
 
 import org.mockito.invocation.InvocationOnMock;
@@ -79,7 +76,6 @@ import static org.mockito.Mockito.mock;
 import static org.mockito.Mockito.when;
 
 @SuppressWarnings({"serial", "SynchronizationOnLocalVariableOrMethodParameter"})
-@Ignore
 public class AccumulatingAlignedProcessingTimeWindowOperatorTest {
 
 	@SuppressWarnings("unchecked")
@@ -203,7 +199,7 @@ public class AccumulatingAlignedProcessingTimeWindowOperatorTest {
 
 			op = new AccumulatingProcessingTimeWindowOperator<>(mockFunction, mockKeySelector,
 					StringSerializer.INSTANCE, StringSerializer.INSTANCE, 5000, 1000);
-			op.setup(mockTask, createTaskConfig(identitySelector, IntSerializer.INSTANCE, new HashKeyGroupAssigner<Object>(10)), mockOut);
+			op.setup(mockTask, new StreamConfig(new Configuration()), mockOut);
 			op.open();
 			assertTrue(op.getNextSlideTime() % 1000 == 0);
 			assertTrue(op.getNextEvaluationTime() % 1000 == 0);
@@ -255,7 +251,7 @@ public class AccumulatingAlignedProcessingTimeWindowOperatorTest {
 							IntSerializer.INSTANCE, IntSerializer.INSTANCE,
 							windowSize, windowSize);
 
-			op.setup(mockTask, createTaskConfig(identitySelector, IntSerializer.INSTANCE, new HashKeyGroupAssigner<Object>(10)), out);
+			op.setup(mockTask, new StreamConfig(new Configuration()), out);
 			op.open();
 
 			final int numElements = 1000;
@@ -306,7 +302,7 @@ public class AccumulatingAlignedProcessingTimeWindowOperatorTest {
 							validatingIdentityFunction, identitySelector,
 							IntSerializer.INSTANCE, IntSerializer.INSTANCE, 150, 50);
 
-			op.setup(mockTask, createTaskConfig(identitySelector, IntSerializer.INSTANCE, new HashKeyGroupAssigner<Object>(10)), out);
+			op.setup(mockTask, new StreamConfig(new Configuration()), out);
 			op.open();
 
 			final int numElements = 1000;
@@ -367,7 +363,7 @@ public class AccumulatingAlignedProcessingTimeWindowOperatorTest {
 							validatingIdentityFunction, identitySelector,
 							IntSerializer.INSTANCE, IntSerializer.INSTANCE, 50, 50);
 
-			op.setup(mockTask, createTaskConfig(identitySelector, IntSerializer.INSTANCE, new HashKeyGroupAssigner<Object>(10)), out);
+			op.setup(mockTask, new StreamConfig(new Configuration()), out);
 			op.open();
 
 			synchronized (lock) {
@@ -423,7 +419,7 @@ public class AccumulatingAlignedProcessingTimeWindowOperatorTest {
 							validatingIdentityFunction, identitySelector,
 							IntSerializer.INSTANCE, IntSerializer.INSTANCE, 150, 50);
 
-			op.setup(mockTask, createTaskConfig(identitySelector, IntSerializer.INSTANCE, new HashKeyGroupAssigner<Object>(10)), out);
+			op.setup(mockTask, new StreamConfig(new Configuration()), out);
 			op.open();
 
 			synchronized (lock) {
@@ -457,67 +453,58 @@ public class AccumulatingAlignedProcessingTimeWindowOperatorTest {
 
 	@Test
 	public void checkpointRestoreWithPendingWindowTumbling() {
-		final ScheduledExecutorService timerService = Executors.newSingleThreadScheduledExecutor();
 		try {
 			final int windowSize = 200;
-			final CollectingOutput<Integer> out = new CollectingOutput<>(windowSize);
-			final Object lock = new Object();
-			final StreamTask<?, ?> mockTask = createMockTaskWithTimer(timerService, lock);
 
-			// tumbling window that triggers every 50 milliseconds
+			// tumbling window that triggers every 200 milliseconds
 			AccumulatingProcessingTimeWindowOperator<Integer, Integer, Integer> op =
 					new AccumulatingProcessingTimeWindowOperator<>(
 							validatingIdentityFunction, identitySelector,
 							IntSerializer.INSTANCE, IntSerializer.INSTANCE,
 							windowSize, windowSize);
-			OneInputStreamOperatorTestHarness<Integer, Integer> testHarness =
 
-					new OneInputStreamOperatorTestHarness<>(op);
+			TestTimeServiceProvider timerService = new TestTimeServiceProvider();
+
+			OneInputStreamOperatorTestHarness<Integer, Integer> testHarness =
+					new OneInputStreamOperatorTestHarness<>(op, new ExecutionConfig(), timerService);
 
 			testHarness.setup();
 			testHarness.open();
 
+			timerService.setCurrentTime(0);
+
 			// inject some elements
 			final int numElementsFirst = 700;
 			final int numElements = 1000;
 			for (int i = 0; i < numElementsFirst; i++) {
-				synchronized (lock) {
-					op.processElement(new StreamRecord<Integer>(i));
-				}
-				Thread.sleep(1);
+				testHarness.processElement(new StreamRecord<>(i));
 			}
 
 			// draw a snapshot and dispose the window
-			StreamStateHandle state;
-			List<Integer> resultAtSnapshot;
-			synchronized (lock) {
-				int beforeSnapShot = out.getElements().size();
-				state = testHarness.snapshot(1L, System.currentTimeMillis());
-				resultAtSnapshot = new ArrayList<>(out.getElements());
-				int afterSnapShot = out.getElements().size();
-				assertEquals("operator performed computation during snapshot", beforeSnapShot, afterSnapShot);
-				assertTrue(afterSnapShot <= numElementsFirst);
-			}
+			System.out.println("GOT: " + testHarness.getOutput());
+			int beforeSnapShot = testHarness.getOutput().size();
+			StreamStateHandle state = testHarness.snapshot(1L, System.currentTimeMillis());
+			List<Integer> resultAtSnapshot = extractFromStreamRecords(testHarness.getOutput());
+			int afterSnapShot = testHarness.getOutput().size();
+			assertEquals("operator performed computation during snapshot", beforeSnapShot, afterSnapShot);
+			assertTrue(afterSnapShot <= numElementsFirst);
 
 			// inject some random elements, which should not show up in the state
 			for (int i = 0; i < 300; i++) {
-				synchronized (lock) {
-					op.processElement(new StreamRecord<Integer>(i + numElementsFirst));
-				}
-				Thread.sleep(1);
+				testHarness.processElement(new StreamRecord<>(i + numElementsFirst));
 			}
 			
 			op.dispose();
 			
 			// re-create the operator and restore the state
-			final CollectingOutput<Integer> out2 = new CollectingOutput<>(windowSize);
 			op = new AccumulatingProcessingTimeWindowOperator<>(
 							validatingIdentityFunction, identitySelector,
 							IntSerializer.INSTANCE, IntSerializer.INSTANCE,
 							windowSize, windowSize);
 
 			testHarness =
-					new OneInputStreamOperatorTestHarness<>(op);
+					new OneInputStreamOperatorTestHarness<>(op, new ExecutionConfig(), timerService);
+
 
 			testHarness.setup();
 			testHarness.restore(state);
@@ -525,18 +512,16 @@ public class AccumulatingAlignedProcessingTimeWindowOperatorTest {
 
 			// inject some more elements
 			for (int i = numElementsFirst; i < numElements; i++) {
-				synchronized (lock) {
-					op.processElement(new StreamRecord<Integer>(i));
-				}
-				Thread.sleep(1);
+				testHarness.processElement(new StreamRecord<>(i));
 			}
 
-
-			out2.waitForNElements(numElements - resultAtSnapshot.size(), 60_000);
+			timerService.setCurrentTime(400);
 
 			// get and verify the result
-			List<Integer> finalResult = new ArrayList<>(resultAtSnapshot);
-			finalResult.addAll(out2.getElements());
+			List<Integer> finalResult = new ArrayList<>();
+			finalResult.addAll(resultAtSnapshot);
+			List<Integer> finalPartialResult = extractFromStreamRecords(testHarness.getOutput());
+			finalResult.addAll(finalPartialResult);
 			assertEquals(numElements, finalResult.size());
 
 			Collections.sort(finalResult);
@@ -548,22 +533,16 @@ public class AccumulatingAlignedProcessingTimeWindowOperatorTest {
 			e.printStackTrace();
 			fail(e.getMessage());
 		}
-		finally {
-			timerService.shutdown();
-		}
 	}
 
 	@Test
 	public void checkpointRestoreWithPendingWindowSliding() {
-		final ScheduledExecutorService timerService = Executors.newSingleThreadScheduledExecutor();
 		try {
 			final int factor = 4;
 			final int windowSlide = 50;
 			final int windowSize = factor * windowSlide;
-			
-			final CollectingOutput<Integer> out = new CollectingOutput<>(windowSlide);
-			final Object lock = new Object();
-			final StreamTask<?, ?> mockTask = createMockTaskWithTimer(timerService, lock);
+
+			TestTimeServiceProvider timerService = new TestTimeServiceProvider();
 
 			// sliding window (200 msecs) every 50 msecs
 			AccumulatingProcessingTimeWindowOperator<Integer, Integer, Integer> op =
@@ -573,7 +552,9 @@ public class AccumulatingAlignedProcessingTimeWindowOperatorTest {
 							windowSize, windowSlide);
 
 			OneInputStreamOperatorTestHarness<Integer, Integer> testHarness =
-					new OneInputStreamOperatorTestHarness<>(op);
+					new OneInputStreamOperatorTestHarness<>(op, new ExecutionConfig(), timerService);
+
+			timerService.setCurrentTime(0);
 
 			testHarness.setup();
 			testHarness.open();
@@ -583,44 +564,32 @@ public class AccumulatingAlignedProcessingTimeWindowOperatorTest {
 			final int numElementsFirst = 700;
 			
 			for (int i = 0; i < numElementsFirst; i++) {
-				synchronized (lock) {
-					op.processElement(new StreamRecord<Integer>(i));
-				}
-				Thread.sleep(1);
+				testHarness.processElement(new StreamRecord<>(i));
 			}
 
 			// draw a snapshot
-			StreamStateHandle state;
-			List<Integer> resultAtSnapshot;
-			synchronized (lock) {
-				int beforeSnapShot = out.getElements().size();
-				state = testHarness.snapshot(1L, System.currentTimeMillis());
-				resultAtSnapshot = new ArrayList<>(out.getElements());
-				int afterSnapShot = out.getElements().size();
-				assertEquals("operator performed computation during snapshot", beforeSnapShot, afterSnapShot);
-			}
-			
+			List<Integer> resultAtSnapshot = extractFromStreamRecords(testHarness.getOutput());
+			int beforeSnapShot = testHarness.getOutput().size();
+			StreamStateHandle state = testHarness.snapshot(1L, System.currentTimeMillis());
+			int afterSnapShot = testHarness.getOutput().size();
+			assertEquals("operator performed computation during snapshot", beforeSnapShot, afterSnapShot);
+
 			assertTrue(resultAtSnapshot.size() <= factor * numElementsFirst);
 
 			// inject the remaining elements - these should not influence the snapshot
 			for (int i = numElementsFirst; i < numElements; i++) {
-				synchronized (lock) {
-					op.processElement(new StreamRecord<Integer>(i));
-				}
-				Thread.sleep(1);
+				testHarness.processElement(new StreamRecord<>(i));
 			}
 			
 			op.dispose();
 			
 			// re-create the operator and restore the state
-			final CollectingOutput<Integer> out2 = new CollectingOutput<>(windowSlide);
 			op = new AccumulatingProcessingTimeWindowOperator<>(
 					validatingIdentityFunction, identitySelector,
 					IntSerializer.INSTANCE, IntSerializer.INSTANCE,
 					windowSize, windowSlide);
 
-			testHarness =
-					new OneInputStreamOperatorTestHarness<>(op);
+			testHarness = new OneInputStreamOperatorTestHarness<>(op, new ExecutionConfig(), timerService);
 
 			testHarness.setup();
 			testHarness.restore(state);
@@ -629,29 +598,24 @@ public class AccumulatingAlignedProcessingTimeWindowOperatorTest {
 
 			// inject again the remaining elements
 			for (int i = numElementsFirst; i < numElements; i++) {
-				synchronized (lock) {
-					op.processElement(new StreamRecord<Integer>(i));
-				}
-				Thread.sleep(1);
+				testHarness.processElement(new StreamRecord<>(i));
 			}
 
-			// for a deterministic result, we need to wait until all pending triggers
-			// have fired and emitted their results
-			long deadline = System.currentTimeMillis() + 120000;
-			do {
-				Thread.sleep(20);
-			}
-			while (resultAtSnapshot.size() + out2.getElements().size() < factor * numElements
-					&& System.currentTimeMillis() < deadline);
+			timerService.setCurrentTime(50);
+			timerService.setCurrentTime(100);
+			timerService.setCurrentTime(150);
+			timerService.setCurrentTime(200);
+			timerService.setCurrentTime(250);
+			timerService.setCurrentTime(300);
+			timerService.setCurrentTime(350);
 
-			synchronized (lock) {
-				op.close();
-			}
+			testHarness.close();
 			op.dispose();
 
 			// get and verify the result
 			List<Integer> finalResult = new ArrayList<>(resultAtSnapshot);
-			finalResult.addAll(out2.getElements());
+			List<Integer> finalPartialResult = extractFromStreamRecords(testHarness.getOutput());
+			finalResult.addAll(finalPartialResult);
 			assertEquals(factor * numElements, finalResult.size());
 
 			Collections.sort(finalResult);
@@ -663,19 +627,12 @@ public class AccumulatingAlignedProcessingTimeWindowOperatorTest {
 			e.printStackTrace();
 			fail(e.getMessage());
 		}
-		finally {
-			timerService.shutdown();
-		}
 	}
 	
 	@Test
 	public void testKeyValueStateInWindowFunction() {
-		final ScheduledExecutorService timerService = Executors.newSingleThreadScheduledExecutor();
 		try {
-			final CollectingOutput<Integer> out = new CollectingOutput<>(50);
-			final Object lock = new Object();
-			final StreamTask<?, ?> mockTask = createMockTaskWithTimer(timerService, lock);
-			
+
 			StatefulFunction.globalCounts.clear();
 			
 			// tumbling window that triggers every 20 milliseconds
@@ -684,26 +641,28 @@ public class AccumulatingAlignedProcessingTimeWindowOperatorTest {
 							new StatefulFunction(), identitySelector,
 							IntSerializer.INSTANCE, IntSerializer.INSTANCE, 50, 50);
 
-			op.setup(mockTask, createTaskConfig(identitySelector, IntSerializer.INSTANCE, new HashKeyGroupAssigner<Object>(10)), out);
-			op.open();
+			TestTimeServiceProvider timerService = new TestTimeServiceProvider();
 
-			synchronized (lock) {
-				op.processElement(new StreamRecord<Integer>(1));
-				op.processElement(new StreamRecord<Integer>(2));
-			}
-			out.waitForNElements(2, 60000);
+			OneInputStreamOperatorTestHarness<Integer, Integer> testHarness =
+					new KeyedOneInputStreamOperatorTestHarness<>(op, new ExecutionConfig(), timerService, identitySelector, BasicTypeInfo.INT_TYPE_INFO);
 
-			synchronized (lock) {
-				op.processElement(new StreamRecord<Integer>(1));
-				op.processElement(new StreamRecord<Integer>(2));
-				op.processElement(new StreamRecord<Integer>(1));
-				op.processElement(new StreamRecord<Integer>(1));
-				op.processElement(new StreamRecord<Integer>(2));
-				op.processElement(new StreamRecord<Integer>(2));
-			}
-			out.waitForNElements(8, 60000);
+			testHarness.open();
 
-			List<Integer> result = out.getElements();
+			timerService.setCurrentTime(0);
+
+			testHarness.processElement(new StreamRecord<>(1));
+			testHarness.processElement(new StreamRecord<>(2));
+
+			op.processElement(new StreamRecord<>(1));
+			op.processElement(new StreamRecord<>(2));
+			op.processElement(new StreamRecord<>(1));
+			op.processElement(new StreamRecord<>(1));
+			op.processElement(new StreamRecord<>(2));
+			op.processElement(new StreamRecord<>(2));
+
+			timerService.setCurrentTime(1000);
+
+			List<Integer> result = extractFromStreamRecords(testHarness.getOutput());
 			assertEquals(8, result.size());
 
 			Collections.sort(result);
@@ -712,18 +671,13 @@ public class AccumulatingAlignedProcessingTimeWindowOperatorTest {
 			assertEquals(4, StatefulFunction.globalCounts.get(1).intValue());
 			assertEquals(4, StatefulFunction.globalCounts.get(2).intValue());
 			
-			synchronized (lock) {
-				op.close();
-			}
+			testHarness.close();
 			op.dispose();
 		}
 		catch (Exception e) {
 			e.printStackTrace();
 			fail(e.getMessage());
 		}
-		finally {
-			timerService.shutdown();
-		}
 	}
 	
 	// ------------------------------------------------------------------------
@@ -793,27 +747,11 @@ public class AccumulatingAlignedProcessingTimeWindowOperatorTest {
 		when(mockTaskManagerRuntimeInfo.getConfiguration()).thenReturn(configuration);
 
 		final Environment env = mock(Environment.class);
-		when(env.getTaskInfo()).thenReturn(new TaskInfo("Test task name", 0, 1, 0));
+		when(env.getTaskInfo()).thenReturn(new TaskInfo("Test task name", 1, 0, 1, 0));
 		when(env.getUserClassLoader()).thenReturn(AggregatingAlignedProcessingTimeWindowOperatorTest.class.getClassLoader());
 		when(env.getMetricGroup()).thenReturn(new UnregisteredTaskMetricsGroup());
 
 		when(task.getEnvironment()).thenReturn(env);
-
-		try {
-			doAnswer(new Answer<AbstractStateBackend>() {
-				@Override
-				public AbstractStateBackend answer(InvocationOnMock invocationOnMock) throws Throwable {
-					final String operatorIdentifier = (String) invocationOnMock.getArguments()[0];
-					final TypeSerializer<?> keySerializer = (TypeSerializer<?>) invocationOnMock.getArguments()[1];
-					MemoryStateBackend backend = MemoryStateBackend.create();
-					backend.initializeForJob(env, operatorIdentifier, keySerializer);
-					return backend;
-				}
-			}).when(task).createStateBackend(any(String.class), any(TypeSerializer.class));
-		} catch (Exception e) {
-			e.printStackTrace();
-		}
-
 		return task;
 	}
 
@@ -846,11 +784,14 @@ public class AccumulatingAlignedProcessingTimeWindowOperatorTest {
 		return mockTask;
 	}
 
-	private static StreamConfig createTaskConfig(KeySelector<?, ?> partitioner, TypeSerializer<?> keySerializer, KeyGroupAssigner<?> keyGroupAssigner) {
-		StreamConfig cfg = new StreamConfig(new Configuration());
-		cfg.setStatePartitioner(0, partitioner);
-		cfg.setStateKeySerializer(keySerializer);
-		cfg.setKeyGroupAssigner(keyGroupAssigner);
-		return cfg;
+	@SuppressWarnings({"unchecked", "rawtypes"})
+	private <T> List<T> extractFromStreamRecords(Iterable<Object> input) {
+		List<T> result = new ArrayList<>();
+		for (Object in : input) {
+			if (in instanceof StreamRecord) {
+				result.add((T) ((StreamRecord) in).getValue());
+			}
+		}
+		return result;
 	}
 }
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/AggregatingAlignedProcessingTimeWindowOperatorTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/AggregatingAlignedProcessingTimeWindowOperatorTest.java
index 472dccb..4e7e4d0 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/AggregatingAlignedProcessingTimeWindowOperatorTest.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/AggregatingAlignedProcessingTimeWindowOperatorTest.java
@@ -48,6 +48,8 @@ import org.apache.flink.streaming.runtime.operators.Triggerable;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
 import org.apache.flink.streaming.runtime.tasks.StreamTask;
 
+import org.apache.flink.streaming.runtime.tasks.TestTimeServiceProvider;
+import org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness;
 import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;
 import org.junit.After;
 import org.junit.Ignore;
@@ -81,7 +83,6 @@ import static org.mockito.Mockito.mock;
 import static org.mockito.Mockito.when;
 
 @SuppressWarnings({"serial", "SynchronizationOnLocalVariableOrMethodParameter"})
-@Ignore
 public class AggregatingAlignedProcessingTimeWindowOperatorTest {
 
 	@SuppressWarnings("unchecked")
@@ -553,12 +554,10 @@ public class AggregatingAlignedProcessingTimeWindowOperatorTest {
 
 	@Test
 	public void checkpointRestoreWithPendingWindowTumbling() {
-		final ScheduledExecutorService timerService = Executors.newSingleThreadScheduledExecutor();
 		try {
 			final int windowSize = 200;
-			final CollectingOutput<Tuple2<Integer, Integer>> out = new CollectingOutput<>(windowSize);
-			final Object lock = new Object();
-			final StreamTask<?, ?> mockTask = createMockTaskWithTimer(timerService, lock);
+
+			TestTimeServiceProvider timerService = new TestTimeServiceProvider();
 
 			// tumbling window that triggers every 50 milliseconds
 			AggregatingProcessingTimeWindowOperator<Integer, Tuple2<Integer, Integer>> op =
@@ -568,7 +567,9 @@ public class AggregatingAlignedProcessingTimeWindowOperatorTest {
 							windowSize, windowSize);
 
 			OneInputStreamOperatorTestHarness<Tuple2<Integer, Integer>, Tuple2<Integer, Integer>> testHarness =
-					new OneInputStreamOperatorTestHarness<>(op);
+					new OneInputStreamOperatorTestHarness<>(op, new ExecutionConfig(), timerService);
+
+			timerService.setCurrentTime(0);
 
 			testHarness.setup();
 			testHarness.open();
@@ -578,48 +579,34 @@ public class AggregatingAlignedProcessingTimeWindowOperatorTest {
 			final int numElements = 1000;
 			
 			for (int i = 0; i < numElementsFirst; i++) {
-				synchronized (lock) {
-					StreamRecord<Tuple2<Integer, Integer>> next = new StreamRecord<>(new Tuple2<>(i, i));
-					op.setKeyContextElement1(next);
-					op.processElement(next);
-				}
-				Thread.sleep(1);
+				StreamRecord<Tuple2<Integer, Integer>> next = new StreamRecord<>(new Tuple2<>(i, i));
+				testHarness.processElement(next);
 			}
 
-			// draw a snapshot and dispose the window
-			StreamStateHandle state;
-			List<Tuple2<Integer, Integer>> resultAtSnapshot;
-			synchronized (lock) {
-				int beforeSnapShot = out.getElements().size();
-				state = testHarness.snapshot(1L, System.currentTimeMillis());
-				resultAtSnapshot = new ArrayList<>(out.getElements());
-				int afterSnapShot = out.getElements().size();
-				assertEquals("operator performed computation during snapshot", beforeSnapShot, afterSnapShot);
-			}
-			
+			// draw a snapshot
+			List<Tuple2<Integer, Integer>> resultAtSnapshot = extractFromStreamRecords(testHarness.getOutput());
+			int beforeSnapShot = resultAtSnapshot.size();
+			StreamStateHandle state = testHarness.snapshot(1L, System.currentTimeMillis());
+			int afterSnapShot = testHarness.getOutput().size();
+			assertEquals("operator performed computation during snapshot", beforeSnapShot, afterSnapShot);
+
 			assertTrue(resultAtSnapshot.size() <= numElementsFirst);
 
 			// inject some random elements, which should not show up in the state
 			for (int i = numElementsFirst; i < numElements; i++) {
-				synchronized (lock) {
-					StreamRecord<Tuple2<Integer, Integer>> next = new StreamRecord<>(new Tuple2<>(i, i));
-					op.setKeyContextElement1(next);
-					op.processElement(next);
-				}
-				Thread.sleep(1);
+				StreamRecord<Tuple2<Integer, Integer>> next = new StreamRecord<>(new Tuple2<>(i, i));
+				testHarness.processElement(next);
 			}
 
 			op.dispose();
 
 			// re-create the operator and restore the state
-			final CollectingOutput<Tuple2<Integer, Integer>> out2 = new CollectingOutput<>(windowSize);
 			op = new AggregatingProcessingTimeWindowOperator<>(
 					sumFunction, fieldOneSelector,
 					IntSerializer.INSTANCE, tupleSerializer,
 					windowSize, windowSize);
 
-			testHarness =
-					new OneInputStreamOperatorTestHarness<>(op);
+			testHarness = new OneInputStreamOperatorTestHarness<>(op, new ExecutionConfig(), timerService);
 
 			testHarness.setup();
 			testHarness.restore(state);
@@ -627,24 +614,19 @@ public class AggregatingAlignedProcessingTimeWindowOperatorTest {
 
 			// inject the remaining elements
 			for (int i = numElementsFirst; i < numElements; i++) {
-				synchronized (lock) {
-					StreamRecord<Tuple2<Integer, Integer>> next = new StreamRecord<>(new Tuple2<>(i, i));
-					op.setKeyContextElement1(next);
-					op.processElement(next);
-				}
-				Thread.sleep(1);
+				StreamRecord<Tuple2<Integer, Integer>> next = new StreamRecord<>(new Tuple2<>(i, i));
+				testHarness.processElement(next);
 			}
 
-			out2.waitForNElements(numElements - resultAtSnapshot.size(), 60_000);
+			timerService.setCurrentTime(200);
 
 			// get and verify the result
 			List<Tuple2<Integer, Integer>> finalResult = new ArrayList<>(resultAtSnapshot);
-			finalResult.addAll(out2.getElements());
+			List<Tuple2<Integer, Integer>> partialFinalResult = extractFromStreamRecords(testHarness.getOutput());
+			finalResult.addAll(partialFinalResult);
 			assertEquals(numElements, finalResult.size());
 
-			synchronized (lock) {
-				op.close();
-			}
+			testHarness.close();
 			op.dispose();
 
 			Collections.sort(finalResult, tupleComparator);
@@ -657,22 +639,16 @@ public class AggregatingAlignedProcessingTimeWindowOperatorTest {
 			e.printStackTrace();
 			fail(e.getMessage());
 		}
-		finally {
-			timerService.shutdown();
-		}
 	}
 
 	@Test
 	public void checkpointRestoreWithPendingWindowSliding() {
-		final ScheduledExecutorService timerService = Executors.newSingleThreadScheduledExecutor();
 		try {
 			final int factor = 4;
 			final int windowSlide = 50;
 			final int windowSize = factor * windowSlide;
 
-			final CollectingOutput<Tuple2<Integer, Integer>> out = new CollectingOutput<>(windowSlide);
-			final Object lock = new Object();
-			final StreamTask<?, ?> mockTask = createMockTaskWithTimer(timerService, lock);
+			TestTimeServiceProvider timerService = new TestTimeServiceProvider();
 
 			// sliding window (200 msecs) every 50 msecs
 			AggregatingProcessingTimeWindowOperator<Integer, Tuple2<Integer, Integer>> op =
@@ -681,8 +657,10 @@ public class AggregatingAlignedProcessingTimeWindowOperatorTest {
 							IntSerializer.INSTANCE, tupleSerializer,
 							windowSize, windowSlide);
 
+			timerService.setCurrentTime(0);
+
 			OneInputStreamOperatorTestHarness<Tuple2<Integer, Integer>, Tuple2<Integer, Integer>> testHarness =
-					new OneInputStreamOperatorTestHarness<>(op);
+					new OneInputStreamOperatorTestHarness<>(op, new ExecutionConfig(), timerService);
 
 			testHarness.setup();
 			testHarness.open();
@@ -692,48 +670,34 @@ public class AggregatingAlignedProcessingTimeWindowOperatorTest {
 			final int numElementsFirst = 700;
 
 			for (int i = 0; i < numElementsFirst; i++) {
-				synchronized (lock) {
-					StreamRecord<Tuple2<Integer, Integer>> next = new StreamRecord<>(new Tuple2<>(i, i));
-					op.setKeyContextElement1(next);
-					op.processElement(next);
-				}
-				Thread.sleep(1);
+				StreamRecord<Tuple2<Integer, Integer>> next = new StreamRecord<>(new Tuple2<>(i, i));
+				testHarness.processElement(next);
 			}
 
 			// draw a snapshot
-			StreamStateHandle state;
-			List<Tuple2<Integer, Integer>> resultAtSnapshot;
-			synchronized (lock) {
-				int beforeSnapShot = out.getElements().size();
-				state = testHarness.snapshot(1L, System.currentTimeMillis());
-				resultAtSnapshot = new ArrayList<>(out.getElements());
-				int afterSnapShot = out.getElements().size();
-				assertEquals("operator performed computation during snapshot", beforeSnapShot, afterSnapShot);
-			}
+			List<Tuple2<Integer, Integer>> resultAtSnapshot = extractFromStreamRecords(testHarness.getOutput());
+			int beforeSnapShot = resultAtSnapshot.size();
+			StreamStateHandle state = testHarness.snapshot(1L, System.currentTimeMillis());
+			int afterSnapShot = testHarness.getOutput().size();
+			assertEquals("operator performed computation during snapshot", beforeSnapShot, afterSnapShot);
 
 			assertTrue(resultAtSnapshot.size() <= factor * numElementsFirst);
 
 			// inject the remaining elements - these should not influence the snapshot
 			for (int i = numElementsFirst; i < numElements; i++) {
-				synchronized (lock) {
-					StreamRecord<Tuple2<Integer, Integer>> next = new StreamRecord<>(new Tuple2<>(i, i));
-					op.setKeyContextElement1(next);
-					op.processElement(next);
-				}
-				Thread.sleep(1);
+				StreamRecord<Tuple2<Integer, Integer>> next = new StreamRecord<>(new Tuple2<>(i, i));
+				testHarness.processElement(next);
 			}
 
 			op.dispose();
 
 			// re-create the operator and restore the state
-			final CollectingOutput<Tuple2<Integer, Integer>> out2 = new CollectingOutput<>(windowSlide);
 			op = new AggregatingProcessingTimeWindowOperator<>(
 					sumFunction, fieldOneSelector,
 					IntSerializer.INSTANCE, tupleSerializer,
 					windowSize, windowSlide);
 
-			testHarness =
-					new OneInputStreamOperatorTestHarness<>(op);
+			testHarness = new OneInputStreamOperatorTestHarness<>(op, new ExecutionConfig(), timerService);
 
 			testHarness.setup();
 			testHarness.restore(state);
@@ -741,32 +705,27 @@ public class AggregatingAlignedProcessingTimeWindowOperatorTest {
 
 			// inject again the remaining elements
 			for (int i = numElementsFirst; i < numElements; i++) {
-				synchronized (lock) {
-					StreamRecord<Tuple2<Integer, Integer>> next = new StreamRecord<>(new Tuple2<>(i, i));
-					op.setKeyContextElement1(next);
-					op.processElement(next);
-				}
-				Thread.sleep(1);
+				StreamRecord<Tuple2<Integer, Integer>> next = new StreamRecord<>(new Tuple2<>(i, i));
+				testHarness.processElement(next);
 			}
 
-			// for a deterministic result, we need to wait until all pending triggers
-			// have fired and emitted their results
-			long deadline = System.currentTimeMillis() + 120000;
-			do {
-				Thread.sleep(20);
-			}
-			while (resultAtSnapshot.size() + out2.getElements().size() < factor * numElements
-					&& System.currentTimeMillis() < deadline);
+			timerService.setCurrentTime(50);
+			timerService.setCurrentTime(100);
+			timerService.setCurrentTime(150);
+			timerService.setCurrentTime(200);
+			timerService.setCurrentTime(250);
+			timerService.setCurrentTime(300);
+			timerService.setCurrentTime(350);
+			timerService.setCurrentTime(400);
 
-			synchronized (lock) {
-				op.close();
-			}
+			testHarness.close();
 			op.dispose();
 
 			// get and verify the result
 			List<Tuple2<Integer, Integer>> finalResult = new ArrayList<>(resultAtSnapshot);
-			finalResult.addAll(out2.getElements());
-			assertEquals(factor * numElements, finalResult.size());
+			List<Tuple2<Integer, Integer>> partialFinalResult = extractFromStreamRecords(testHarness.getOutput());
+			finalResult.addAll(partialFinalResult);
+			assertEquals(numElements * factor, finalResult.size());
 
 			Collections.sort(finalResult, tupleComparator);
 			for (int i = 0; i < factor * numElements; i++) {
@@ -778,20 +737,14 @@ public class AggregatingAlignedProcessingTimeWindowOperatorTest {
 			e.printStackTrace();
 			fail(e.getMessage());
 		}
-		finally {
-			timerService.shutdown();
-		}
 	}
 
 	@Test
 	public void testKeyValueStateInWindowFunctionTumbling() {
-		final ScheduledExecutorService timerService = Executors.newSingleThreadScheduledExecutor();
 		try {
 			final long twoSeconds = 2000;
 			
-			final CollectingOutput<Tuple2<Integer, Integer>> out = new CollectingOutput<>();
-			final Object lock = new Object();
-			final StreamTask<?, ?> mockTask = createMockTaskWithTimer(timerService, lock);
+			TestTimeServiceProvider timerService = new TestTimeServiceProvider();
 
 			StatefulFunction.globalCounts.clear();
 			
@@ -800,53 +753,52 @@ public class AggregatingAlignedProcessingTimeWindowOperatorTest {
 							new StatefulFunction(), fieldOneSelector,
 							IntSerializer.INSTANCE, tupleSerializer, twoSeconds, twoSeconds);
 
-			op.setup(mockTask, createTaskConfig(fieldOneSelector, IntSerializer.INSTANCE, new HashKeyGroupAssigner<Object>(10)), out);
-			op.open();
+			KeyedOneInputStreamOperatorTestHarness<Integer, Tuple2<Integer, Integer>, Tuple2<Integer, Integer>> testHarness = new KeyedOneInputStreamOperatorTestHarness<>(
+					op,
+					new ExecutionConfig(),
+					timerService,
+					fieldOneSelector,
+					BasicTypeInfo.INT_TYPE_INFO);
+
+			timerService.setCurrentTime(0);
+			testHarness.open();
 
 			// because the window interval is so large, everything should be in one window
 			// and aggregate into one value per key
 			
-			synchronized (lock) {
-				for (int i = 0; i < 10; i++) {
-					StreamRecord<Tuple2<Integer, Integer>> next1 = new StreamRecord<>(new Tuple2<>(1, i));
-					op.setKeyContextElement1(next1);
-					op.processElement(next1);
-	
-					StreamRecord<Tuple2<Integer, Integer>> next2 = new StreamRecord<>(new Tuple2<>(2, i));
-					op.setKeyContextElement1(next2);
-					op.processElement(next2);
-				}
-			}
+			for (int i = 0; i < 10; i++) {
+				StreamRecord<Tuple2<Integer, Integer>> next1 = new StreamRecord<>(new Tuple2<>(1, i));
+				testHarness.processElement(next1);
 
-			while (StatefulFunction.globalCounts.get(1) < 10 ||
-					StatefulFunction.globalCounts.get(2) < 10)
-			{
-				Thread.sleep(50);
+				StreamRecord<Tuple2<Integer, Integer>> next2 = new StreamRecord<>(new Tuple2<>(2, i));
+				testHarness.processElement(next2);
 			}
-			
-			op.close();
+
+			timerService.setCurrentTime(1000);
+
+			int count1 = StatefulFunction.globalCounts.get(1);
+			int count2 = StatefulFunction.globalCounts.get(2);
+
+			assertTrue(count1 >= 2 && count1 <= 2 * 10);
+			assertEquals(count1, count2);
+
+			testHarness.close();
 			op.dispose();
 		}
 		catch (Exception e) {
 			e.printStackTrace();
 			fail(e.getMessage());
 		}
-		finally {
-			timerService.shutdown();
-		}
 	}
 
 	@Test
 	public void testKeyValueStateInWindowFunctionSliding() {
-		final ScheduledExecutorService timerService = Executors.newSingleThreadScheduledExecutor();
 		try {
 			final int factor = 2;
 			final int windowSlide = 50;
 			final int windowSize = factor * windowSlide;
-			
-			final CollectingOutput<Tuple2<Integer, Integer>> out = new CollectingOutput<>();
-			final Object lock = new Object();
-			final StreamTask<?, ?> mockTask = createMockTaskWithTimer(timerService, lock);
+
+			TestTimeServiceProvider timerService = new TestTimeServiceProvider();
 
 			StatefulFunction.globalCounts.clear();
 			
@@ -855,8 +807,15 @@ public class AggregatingAlignedProcessingTimeWindowOperatorTest {
 							new StatefulFunction(), fieldOneSelector,
 							IntSerializer.INSTANCE, tupleSerializer, windowSize, windowSlide);
 
-			op.setup(mockTask, createTaskConfig(fieldOneSelector, IntSerializer.INSTANCE, new HashKeyGroupAssigner<Object>(10)), out);
-			op.open();
+			timerService.setCurrentTime(0);
+			KeyedOneInputStreamOperatorTestHarness<Integer, Tuple2<Integer, Integer>, Tuple2<Integer, Integer>> testHarness = new KeyedOneInputStreamOperatorTestHarness<>(
+					op,
+					new ExecutionConfig(),
+					timerService,
+					fieldOneSelector,
+					BasicTypeInfo.INT_TYPE_INFO);
+
+			testHarness.open();
 
 			// because the window interval is so large, everything should be in one window
 			// and aggregate into one value per key
@@ -870,25 +829,19 @@ public class AggregatingAlignedProcessingTimeWindowOperatorTest {
 				StreamRecord<Tuple2<Integer, Integer>> next3 = new StreamRecord<>(new Tuple2<>(1, i));
 				StreamRecord<Tuple2<Integer, Integer>> next4 = new StreamRecord<>(new Tuple2<>(2, i));
 
-				// because we do not release the lock between elements, they end up in the same windows
-				synchronized (lock) {
-					op.setKeyContextElement1(next1);
-					op.processElement(next1);
-					op.setKeyContextElement1(next2);
-					op.processElement(next2);
-					op.setKeyContextElement1(next3);
-					op.processElement(next3);
-					op.setKeyContextElement1(next4);
-					op.processElement(next4);
-				}
-
-				Thread.sleep(1);
-			}
-			
-			synchronized (lock) {
-				op.close();
+				testHarness.processElement(next1);
+				testHarness.processElement(next2);
+				testHarness.processElement(next3);
+				testHarness.processElement(next4);
 			}
 
+			timerService.setCurrentTime(50);
+			timerService.setCurrentTime(100);
+			timerService.setCurrentTime(150);
+			timerService.setCurrentTime(200);
+
+			testHarness.close();
+
 			int count1 = StatefulFunction.globalCounts.get(1);
 			int count2 = StatefulFunction.globalCounts.get(2);
 			
@@ -901,9 +854,6 @@ public class AggregatingAlignedProcessingTimeWindowOperatorTest {
 			e.printStackTrace();
 			fail(e.getMessage());
 		}
-		finally {
-			timerService.shutdown();
-		}
 	}
 	
 	// ------------------------------------------------------------------------
@@ -991,21 +941,6 @@ public class AggregatingAlignedProcessingTimeWindowOperatorTest {
 		final Environment env = new DummyEnvironment("Test task name", 1, 0);
 		when(task.getEnvironment()).thenReturn(env);
 
-		try {
-			doAnswer(new Answer<AbstractStateBackend>() {
-				@Override
-				public AbstractStateBackend answer(InvocationOnMock invocationOnMock) throws Throwable {
-					final String operatorIdentifier = (String) invocationOnMock.getArguments()[0];
-					final TypeSerializer<?> keySerializer = (TypeSerializer<?>) invocationOnMock.getArguments()[1];
-					MemoryStateBackend backend = MemoryStateBackend.create();
-					backend.initializeForJob(env, operatorIdentifier, keySerializer);
-					return backend;
-				}
-			}).when(task).createStateBackend(any(String.class), any(TypeSerializer.class));
-		} catch (Exception e) {
-			e.printStackTrace();
-		}
-
 		return task;
 	}
 
@@ -1040,9 +975,17 @@ public class AggregatingAlignedProcessingTimeWindowOperatorTest {
 
 	private static StreamConfig createTaskConfig(KeySelector<?, ?> partitioner, TypeSerializer<?> keySerializer, KeyGroupAssigner<?> keyGroupAssigner) {
 		StreamConfig cfg = new StreamConfig(new Configuration());
-		cfg.setStatePartitioner(0, partitioner);
-		cfg.setStateKeySerializer(keySerializer);
-		cfg.setKeyGroupAssigner(keyGroupAssigner);
 		return cfg;
 	}
+
+	@SuppressWarnings({"unchecked", "rawtypes"})
+	private <T> List<T> extractFromStreamRecords(Iterable<Object> input) {
+		List<T> result = new ArrayList<>();
+		for (Object in : input) {
+			if (in instanceof StreamRecord) {
+				result.add((T) ((StreamRecord) in).getValue());
+			}
+		}
+		return result;
+	}
 }
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/EvictingWindowOperatorTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/EvictingWindowOperatorTest.java
index dc71440..681a334 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/EvictingWindowOperatorTest.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/EvictingWindowOperatorTest.java
@@ -41,6 +41,7 @@ import org.apache.flink.streaming.api.windowing.windows.Window;
 import org.apache.flink.streaming.runtime.operators.windowing.functions.InternalIterableWindowFunction;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecordSerializer;
+import org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness;
 import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;
 import org.apache.flink.streaming.util.TestHarnessUtil;
 import org.apache.flink.util.Collector;
@@ -85,9 +86,7 @@ public class EvictingWindowOperatorTest {
 
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-				new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
 
 		long initialTime = 0L;
 		ConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();
@@ -157,9 +156,7 @@ public class EvictingWindowOperatorTest {
 
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-			new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
 
 		long initialTime = 0L;
 		ConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();
@@ -227,9 +224,7 @@ public class EvictingWindowOperatorTest {
 		operator.setInputType(inputType, new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-			new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
 
 		long initialTime = 0L;
 
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperatorTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperatorTest.java
index cda6e1e..67a6f55 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperatorTest.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperatorTest.java
@@ -64,6 +64,7 @@ import org.apache.flink.streaming.runtime.operators.windowing.functions.Internal
 import org.apache.flink.streaming.runtime.operators.windowing.functions.InternalSingleValueWindowFunction;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
 import org.apache.flink.streaming.runtime.tasks.TestTimeServiceProvider;
+import org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness;
 import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;
 import org.apache.flink.streaming.util.TestHarnessUtil;
 import org.apache.flink.streaming.util.WindowingTestHarness;
@@ -182,9 +183,7 @@ public class WindowOperatorTest extends TestLogger {
 		operator.setInputType(inputType, new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-				new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
 
 		testHarness.setup();
 		testHarness.open();
@@ -220,9 +219,7 @@ public class WindowOperatorTest extends TestLogger {
 		operator.setInputType(inputType, new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-				new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
 
 		testHarness.open();
 
@@ -323,9 +320,7 @@ public class WindowOperatorTest extends TestLogger {
 		operator.setInputType(TypeInfoParser.<Tuple2<String, Integer>>parse("Tuple2<String, Integer>"), new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-				new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
 
 		testHarness.open();
 
@@ -359,9 +354,7 @@ public class WindowOperatorTest extends TestLogger {
 		operator.setInputType(TypeInfoParser.<Tuple2<String, Integer>>parse("Tuple2<String, Integer>"), new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-				new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
 
 		testHarness.open();
 
@@ -398,9 +391,7 @@ public class WindowOperatorTest extends TestLogger {
 		operator.setInputType(TypeInfoParser.<Tuple2<String, Integer>>parse("Tuple2<String, Integer>"), new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple3<String, Long, Long>> testHarness =
-				new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
 
 		ConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();
 
@@ -474,9 +465,7 @@ public class WindowOperatorTest extends TestLogger {
 		operator.setInputType(TypeInfoParser.<Tuple2<String, Integer>>parse("Tuple2<String, Integer>"), new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple3<String, Long, Long>> testHarness =
-				new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
 
 		ConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();
 
@@ -552,9 +541,7 @@ public class WindowOperatorTest extends TestLogger {
 		operator.setInputType(TypeInfoParser.<Tuple2<String, Integer>>parse("Tuple2<String, Integer>"), new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple3<String, Long, Long>> testHarness =
-				new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
 		
 		ConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();
 
@@ -659,9 +646,7 @@ public class WindowOperatorTest extends TestLogger {
 		operator.setInputType(TypeInfoParser.<Tuple2<String, Integer>>parse("Tuple2<String, Integer>"), new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple3<String, Long, Long>> testHarness =
-				new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
 		
 		ConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();
 
@@ -721,10 +706,8 @@ public class WindowOperatorTest extends TestLogger {
 		operator.setInputType(TypeInfoParser.<Tuple2<String, Integer>>parse("Tuple2<String, Integer>"), new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-				new OneInputStreamOperatorTestHarness<>(operator);
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
-		
 		ConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();
 
 		testHarness.open();
@@ -814,10 +797,8 @@ public class WindowOperatorTest extends TestLogger {
 				"Tuple2<String, Integer>"), new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-				new OneInputStreamOperatorTestHarness<>(operator);
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
-		
 		ConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();
 
 		testHarness.open();
@@ -837,17 +818,39 @@ public class WindowOperatorTest extends TestLogger {
 
 		// do a snapshot, close and restore again
 		StreamStateHandle snapshot = testHarness.snapshot(0L, 0L);
+
 		testHarness.close();
+
+		ConcurrentLinkedQueue<Object> outputBeforeClose = testHarness.getOutput();
+
+		stateDesc = new ReducingStateDescriptor<>("window-contents",
+				new SumReducer(),
+				inputType.createSerializer(new ExecutionConfig()));
+
+		operator = new WindowOperator<>(
+				GlobalWindows.create(),
+				new GlobalWindow.Serializer(),
+				new TupleKeySelector(),
+				BasicTypeInfo.STRING_TYPE_INFO.createSerializer(new ExecutionConfig()),
+				stateDesc,
+				new InternalSingleValueWindowFunction<>(new PassThroughWindowFunction<String, GlobalWindow, Tuple2<String, Integer>>()),
+				PurgingTrigger.of(CountTrigger.of(WINDOW_SIZE)),
+				0);
+
+		testHarness = new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+
+		operator.setInputType(TypeInfoParser.<Tuple2<String, Integer>>parse(
+				"Tuple2<String, Integer>"), new ExecutionConfig());
+
 		testHarness.setup();
 		testHarness.restore(snapshot);
 		testHarness.open();
 
 		testHarness.processElement(new StreamRecord<>(new Tuple2<>("key2", 1), 1000));
 
-
 		expectedOutput.add(new StreamRecord<>(new Tuple2<>("key2", 4), Long.MAX_VALUE));
 
-		TestHarnessUtil.assertOutputEqualsSorted("Output was not correct.", expectedOutput, testHarness.getOutput(), new Tuple2ResultSortComparator());
+		TestHarnessUtil.assertOutputEqualsSorted("Output was not correct.", expectedOutput, Iterables.concat(outputBeforeClose, testHarness.getOutput()), new Tuple2ResultSortComparator());
 
 		testHarness.processElement(new StreamRecord<>(new Tuple2<>("key1", 1), 10999));
 
@@ -858,7 +861,10 @@ public class WindowOperatorTest extends TestLogger {
 		expectedOutput.add(new StreamRecord<>(new Tuple2<>("key1", 4), Long.MAX_VALUE));
 		expectedOutput.add(new StreamRecord<>(new Tuple2<>("key2", 4), Long.MAX_VALUE));
 
-		TestHarnessUtil.assertOutputEqualsSorted("Output was not correct.", expectedOutput, testHarness.getOutput(), new Tuple2ResultSortComparator());
+		System.out.println("BEFORE GOT: " + outputBeforeClose);
+		System.out.println("GOT: " + testHarness.getOutput());
+
+		TestHarnessUtil.assertOutputEqualsSorted("Output was not correct.", expectedOutput, Iterables.concat(outputBeforeClose, testHarness.getOutput()), new Tuple2ResultSortComparator());
 
 		testHarness.close();
 	}
@@ -887,9 +893,7 @@ public class WindowOperatorTest extends TestLogger {
 
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-				new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
 		operator.setInputType(inputType, new ExecutionConfig());
 		testHarness.open();
@@ -922,9 +926,8 @@ public class WindowOperatorTest extends TestLogger {
 				0);
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> otherTestHarness =
-				new OneInputStreamOperatorTestHarness<>(otherOperator);
+				new KeyedOneInputStreamOperatorTestHarness<>(otherOperator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
 
-		otherTestHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
 		otherOperator.setInputType(inputType, new ExecutionConfig());
 
 		otherTestHarness.setup();
@@ -959,9 +962,7 @@ public class WindowOperatorTest extends TestLogger {
 		operator.setInputType(TypeInfoParser.<Tuple2<String, Integer>>parse("Tuple2<String, Integer>"), new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-				new OneInputStreamOperatorTestHarness<>(operator, new ExecutionConfig(), testTimeProvider);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, new ExecutionConfig(), testTimeProvider, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
 
 		ConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();
 
@@ -1021,9 +1022,7 @@ public class WindowOperatorTest extends TestLogger {
 		operator.setInputType(TypeInfoParser.<Tuple2<String, Integer>>parse("Tuple2<String, Integer>"), new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-				new OneInputStreamOperatorTestHarness<>(operator, new ExecutionConfig(), testTimeProvider);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, new ExecutionConfig(), testTimeProvider, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
 
 		ConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();
 
@@ -1096,9 +1095,7 @@ public class WindowOperatorTest extends TestLogger {
 		operator.setInputType(TypeInfoParser.<Tuple2<String, Integer>>parse("Tuple2<String, Integer>"), new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-				new OneInputStreamOperatorTestHarness<>(operator, new ExecutionConfig(), testTimeProvider);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, new ExecutionConfig(), testTimeProvider, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
 
 		ConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();
 
@@ -1163,9 +1160,7 @@ public class WindowOperatorTest extends TestLogger {
 				LATENESS);
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-			new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+			new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
 		operator.setInputType(inputType, new ExecutionConfig());
 		testHarness.open();
@@ -1226,9 +1221,7 @@ public class WindowOperatorTest extends TestLogger {
 					LATENESS);
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-			new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+			new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
 		operator.setInputType(inputType, new ExecutionConfig());
 		testHarness.open();
@@ -1295,9 +1288,7 @@ public class WindowOperatorTest extends TestLogger {
 				LATENESS);
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-			new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+			new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
 		operator.setInputType(inputType, new ExecutionConfig());
 		testHarness.open();
@@ -1358,9 +1349,7 @@ public class WindowOperatorTest extends TestLogger {
 				LATENESS);
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-			new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+			new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
 		operator.setInputType(inputType, new ExecutionConfig());
 		testHarness.open();
@@ -1438,9 +1427,7 @@ public class WindowOperatorTest extends TestLogger {
 		operator.setInputType(TypeInfoParser.<Tuple2<String, Integer>>parse("Tuple2<String, Integer>"), new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple3<String, Long, Long>> testHarness =
-			new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+			new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
 		testHarness.open();
 		
@@ -1532,9 +1519,7 @@ public class WindowOperatorTest extends TestLogger {
 		operator.setInputType(TypeInfoParser.<Tuple2<String, Integer>>parse("Tuple2<String, Integer>"), new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple3<String, Long, Long>> testHarness =
-			new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+			new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
 		testHarness.open();
 		
@@ -1620,9 +1605,7 @@ public class WindowOperatorTest extends TestLogger {
 		operator.setInputType(TypeInfoParser.<Tuple2<String, Integer>>parse("Tuple2<String, Integer>"), new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple3<String, Long, Long>> testHarness =
-			new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+			new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
 		testHarness.open();
 
@@ -1708,9 +1691,7 @@ public class WindowOperatorTest extends TestLogger {
 		operator.setInputType(TypeInfoParser.<Tuple2<String, Integer>>parse("Tuple2<String, Integer>"), new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple3<String, Long, Long>> testHarness =
-			new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+			new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
 		testHarness.open();
 		
@@ -1805,9 +1786,7 @@ public class WindowOperatorTest extends TestLogger {
 		operator.setInputType(TypeInfoParser.<Tuple2<String, Integer>>parse("Tuple2<String, Integer>"), new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple3<String, Long, Long>> testHarness =
-			new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+			new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
 		testHarness.open();
 		
@@ -1894,9 +1873,7 @@ public class WindowOperatorTest extends TestLogger {
 		operator.setInputType(TypeInfoParser.<Tuple2<String, Integer>>parse("Tuple2<String, Integer>"), new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple3<String, Long, Long>> testHarness =
-			new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+			new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
 		testHarness.open();
 
@@ -1981,9 +1958,7 @@ public class WindowOperatorTest extends TestLogger {
 				LATENESS);
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, String> testHarness =
-			new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+			new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
 		operator.setInputType(inputType, new ExecutionConfig());
 		testHarness.open();
@@ -2038,9 +2013,7 @@ public class WindowOperatorTest extends TestLogger {
 				LATENESS);
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-			new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+			new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
 		operator.setInputType(inputType, new ExecutionConfig());
 		testHarness.open();
@@ -2087,9 +2060,7 @@ public class WindowOperatorTest extends TestLogger {
 				LATENESS);
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-			new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+			new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
 		operator.setInputType(inputType, new ExecutionConfig());
 		testHarness.open();
@@ -2144,9 +2115,7 @@ public class WindowOperatorTest extends TestLogger {
 				LATENESS);
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-			new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+			new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
 		operator.setInputType(inputType, new ExecutionConfig());
 		testHarness.open();
@@ -2192,9 +2161,7 @@ public class WindowOperatorTest extends TestLogger {
 				LATENESS);
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-			new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+			new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
 		operator.setInputType(inputType, new ExecutionConfig());
 		testHarness.open();
@@ -2241,9 +2208,7 @@ public class WindowOperatorTest extends TestLogger {
 		operator.setInputType(TypeInfoParser.<Tuple2<String, Integer>>parse("Tuple2<String, Integer>"), new ExecutionConfig());
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple3<String, Long, Long>> testHarness =
-			new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+			new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
 		testHarness.open();
 
@@ -2294,9 +2259,7 @@ public class WindowOperatorTest extends TestLogger {
 				LATENESS);
 
 		OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Tuple2<String, Integer>> testHarness =
-			new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO);
+			new KeyedOneInputStreamOperatorTestHarness<>(operator, new TupleKeySelector(), BasicTypeInfo.STRING_TYPE_INFO); ;
 
 		operator.setInputType(inputType, new ExecutionConfig());
 		testHarness.open();
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/InterruptSensitiveRestoreTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/InterruptSensitiveRestoreTest.java
index 1d07bdd..f8b4063 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/InterruptSensitiveRestoreTest.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/InterruptSensitiveRestoreTest.java
@@ -129,7 +129,7 @@ public class InterruptSensitiveRestoreTest {
 				new ExecutionAttemptID(),
 				new SerializedValue<>(new ExecutionConfig()),
 				"test task name",
-				0, 1, 0,
+				1, 0, 1, 0,
 				new Configuration(),
 				taskConfig,
 				SourceStreamTask.class.getName(),
@@ -170,7 +170,7 @@ public class InterruptSensitiveRestoreTest {
 	private static class InterruptLockingStateHandle extends AbstractCloseableHandle implements StreamStateHandle {
 
 		@Override
-		public FSDataInputStream openInputStream() throws Exception {
+		public FSDataInputStream openInputStream() throws IOException {
 			ensureNotClosed();
 			FSDataInputStream is = new FSDataInputStream() {
 
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/OneInputStreamTaskTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/OneInputStreamTaskTest.java
index f757943..7ef0080 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/OneInputStreamTaskTest.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/OneInputStreamTaskTest.java
@@ -26,6 +26,7 @@ import org.apache.flink.api.java.functions.KeySelector;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.core.fs.FSDataInputStream;
 import org.apache.flink.core.fs.FSDataOutputStream;
+import org.apache.flink.core.testutils.OneShotLatch;
 import org.apache.flink.runtime.io.network.api.CheckpointBarrier;
 import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;
 import org.apache.flink.runtime.operators.testutils.MockInputSplitProvider;
@@ -359,14 +360,16 @@ public class OneInputStreamTaskTest extends TestLogger {
 
 		streamTask.triggerCheckpoint(checkpointId, checkpointTimestamp);
 
-		testHarness.endInput();
-		testHarness.waitForTaskCompletion(deadline.timeLeft().toMillis());
-
 		// since no state was set, there shouldn't be restore calls
 		assertEquals(0, TestingStreamOperator.numberRestoreCalls);
 
+		env.getCheckpointLatch().await();
+
 		assertEquals(checkpointId, env.getCheckpointId());
 
+		testHarness.endInput();
+		testHarness.waitForTaskCompletion(deadline.timeLeft().toMillis());
+
 		final OneInputStreamTask<String, String> restoredTask = new OneInputStreamTask<String, String>();
 		restoredTask.setInitialState(env.getState(), env.getKeyGroupStates());
 
@@ -459,9 +462,11 @@ public class OneInputStreamTaskTest extends TestLogger {
 	}
 
 	private static class AcknowledgeStreamMockEnvironment extends StreamMockEnvironment {
-		private long checkpointId;
-		private ChainedStateHandle<StreamStateHandle> state;
-		private List<KeyGroupsStateHandle> keyGroupStates;
+		private volatile long checkpointId;
+		private volatile ChainedStateHandle<StreamStateHandle> state;
+		private volatile List<KeyGroupsStateHandle> keyGroupStates;
+
+		private final OneShotLatch checkpointLatch = new OneShotLatch();
 
 		public long getCheckpointId() {
 			return checkpointId;
@@ -494,6 +499,11 @@ public class OneInputStreamTaskTest extends TestLogger {
 			this.checkpointId = checkpointId;
 			this.state = state;
 			this.keyGroupStates = keyGroupStates;
+			checkpointLatch.trigger();
+		}
+
+		public OneShotLatch getCheckpointLatch() {
+			return checkpointLatch;
 		}
 	}
 
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/StreamMockEnvironment.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/StreamMockEnvironment.java
index 5e82569..0901b32 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/StreamMockEnvironment.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/StreamMockEnvironment.java
@@ -103,7 +103,7 @@ public class StreamMockEnvironment implements Environment {
 
 	public StreamMockEnvironment(Configuration jobConfig, Configuration taskConfig, ExecutionConfig executionConfig,
 									long memorySize, MockInputSplitProvider inputSplitProvider, int bufferSize) {
-		this.taskInfo = new TaskInfo("", 0, 1, 0);
+		this.taskInfo = new TaskInfo("", 1, 0, 1, 0);
 		this.jobConfiguration = jobConfig;
 		this.taskConfiguration = taskConfig;
 		this.inputs = new LinkedList<InputGate>();
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/StreamTaskTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/StreamTaskTest.java
index 3d9d50f..408b5b1 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/StreamTaskTest.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/StreamTaskTest.java
@@ -237,7 +237,7 @@ public class StreamTaskTest {
 		TaskDeploymentDescriptor tdd = new TaskDeploymentDescriptor(
 				new JobID(), "Job Name", new JobVertexID(), new ExecutionAttemptID(),
 				new SerializedValue<>(new ExecutionConfig()),
-				"Test Task", 0, 1, 0,
+				"Test Task", 1, 0, 1, 0,
 				new Configuration(),
 				taskConfig.getConfiguration(),
 				invokable.getName(),
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/KeyedOneInputStreamOperatorTestHarness.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/KeyedOneInputStreamOperatorTestHarness.java
new file mode 100644
index 0000000..5594193
--- /dev/null
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/KeyedOneInputStreamOperatorTestHarness.java
@@ -0,0 +1,201 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.util;
+
+import org.apache.flink.api.common.ExecutionConfig;
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.api.common.state.KeyGroupAssigner;
+import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.api.java.ClosureCleaner;
+import org.apache.flink.api.java.functions.KeySelector;
+import org.apache.flink.core.fs.FSDataInputStream;
+import org.apache.flink.runtime.state.CheckpointStreamFactory;
+import org.apache.flink.runtime.state.HashKeyGroupAssigner;
+import org.apache.flink.runtime.state.KeyGroupRange;
+import org.apache.flink.runtime.state.KeyGroupsStateHandle;
+import org.apache.flink.runtime.state.KeyedStateBackend;
+import org.apache.flink.runtime.state.StreamStateHandle;
+import org.apache.flink.runtime.state.memory.MemoryStateBackend;
+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;
+import org.apache.flink.streaming.runtime.tasks.TimeServiceProvider;
+import org.mockito.invocation.InvocationOnMock;
+import org.mockito.stubbing.Answer;
+
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.util.Collections;
+import java.util.concurrent.RunnableFuture;
+
+import static org.mockito.Matchers.any;
+import static org.mockito.Mockito.*;
+
+/**
+ * Extension of {@link OneInputStreamOperatorTestHarness} that allows the operator to get
+ * a {@link KeyedStateBackend}.
+ *
+ */
+public class KeyedOneInputStreamOperatorTestHarness<K, IN, OUT>
+		extends OneInputStreamOperatorTestHarness<IN, OUT> {
+
+	// in case the operator creates one we store it here so that we
+	// can snapshot its state
+	private KeyedStateBackend<?> keyedStateBackend = null;
+
+	// when we restore we keep the state here so that we can call restore
+	// when the operator requests the keyed state backend
+	private KeyGroupsStateHandle restoredKeyedState = null;
+
+	public KeyedOneInputStreamOperatorTestHarness(
+			OneInputStreamOperator<IN, OUT> operator,
+			final KeySelector<IN, K> keySelector,
+			TypeInformation<K> keyType) {
+		super(operator);
+
+		ClosureCleaner.clean(keySelector, false);
+		config.setStatePartitioner(0, keySelector);
+		config.setStateKeySerializer(keyType.createSerializer(executionConfig));
+		config.setKeyGroupAssigner(new HashKeyGroupAssigner<K>(MAX_PARALLELISM));
+
+		setupMockTaskCreateKeyedBackend();
+	}
+
+	public KeyedOneInputStreamOperatorTestHarness(OneInputStreamOperator<IN, OUT> operator,
+			ExecutionConfig executionConfig,
+			KeySelector<IN, K> keySelector,
+			TypeInformation<K> keyType) {
+		super(operator, executionConfig);
+
+		ClosureCleaner.clean(keySelector, false);
+		config.setStatePartitioner(0, keySelector);
+		config.setStateKeySerializer(keyType.createSerializer(executionConfig));
+		config.setKeyGroupAssigner(new HashKeyGroupAssigner<K>(MAX_PARALLELISM));
+
+		setupMockTaskCreateKeyedBackend();
+	}
+
+	public KeyedOneInputStreamOperatorTestHarness(OneInputStreamOperator<IN, OUT> operator,
+			ExecutionConfig executionConfig,
+			TimeServiceProvider testTimeProvider,
+			KeySelector<IN, K> keySelector,
+			TypeInformation<K> keyType) {
+		super(operator, executionConfig, testTimeProvider);
+
+		ClosureCleaner.clean(keySelector, false);
+		config.setStatePartitioner(0, keySelector);
+		config.setStateKeySerializer(keyType.createSerializer(executionConfig));
+		config.setKeyGroupAssigner(new HashKeyGroupAssigner<K>(MAX_PARALLELISM));
+
+		setupMockTaskCreateKeyedBackend();
+	}
+
+	private void setupMockTaskCreateKeyedBackend() {
+
+		try {
+			doAnswer(new Answer<KeyedStateBackend>() {
+				@Override
+				public KeyedStateBackend answer(InvocationOnMock invocationOnMock) throws Throwable {
+
+					final TypeSerializer keySerializer = (TypeSerializer) invocationOnMock.getArguments()[0];
+					final KeyGroupAssigner keyGroupAssigner = (KeyGroupAssigner) invocationOnMock.getArguments()[1];
+					final KeyGroupRange keyGroupRange = (KeyGroupRange) invocationOnMock.getArguments()[2];
+
+					if (restoredKeyedState == null) {
+						keyedStateBackend = stateBackend.createKeyedStateBackend(
+								mockTask.getEnvironment(),
+								new JobID(),
+								"test_op",
+								keySerializer,
+								keyGroupAssigner,
+								keyGroupRange,
+								mockTask.getEnvironment().getTaskKvStateRegistry());
+						return keyedStateBackend;
+					} else {
+						keyedStateBackend = stateBackend.restoreKeyedStateBackend(
+								mockTask.getEnvironment(),
+								new JobID(),
+								"test_op",
+								keySerializer,
+								keyGroupAssigner,
+								keyGroupRange,
+								Collections.singletonList(restoredKeyedState),
+								mockTask.getEnvironment().getTaskKvStateRegistry());
+						restoredKeyedState = null;
+						return keyedStateBackend;
+					}
+				}
+			}).when(mockTask).createKeyedStateBackend(any(TypeSerializer.class), any(KeyGroupAssigner.class), any(KeyGroupRange.class));
+		} catch (Exception e) {
+			throw new RuntimeException(e.getMessage(), e);
+		}
+	}
+
+	/**
+	 * Calls {@link org.apache.flink.streaming.api.operators.StreamOperator#snapshotState(org.apache.flink.core.fs.FSDataOutputStream, long, long)} ()}
+	 */
+	@Override
+	public StreamStateHandle snapshot(long checkpointId, long timestamp) throws Exception {
+		// simply use an in-memory handle
+		MemoryStateBackend backend = new MemoryStateBackend();
+
+		CheckpointStreamFactory streamFactory = backend.createStreamFactory(new JobID(), "test_op");
+		CheckpointStreamFactory.CheckpointStateOutputStream outStream =
+				streamFactory.createCheckpointStateOutputStream(checkpointId, timestamp);
+
+		operator.snapshotState(outStream, checkpointId, timestamp);
+
+		if (keyedStateBackend != null) {
+			RunnableFuture<KeyGroupsStateHandle> keyedSnapshotRunnable = keyedStateBackend.snapshot(checkpointId,
+					timestamp,
+					streamFactory);
+			if(!keyedSnapshotRunnable.isDone()) {
+				Thread runner = new Thread(keyedSnapshotRunnable);
+				runner.start();
+			}
+			outStream.write(1);
+			ObjectOutputStream oos = new ObjectOutputStream(outStream);
+			oos.writeObject(keyedSnapshotRunnable.get());
+			oos.flush();
+		} else {
+			outStream.write(0);
+		}
+		return outStream.closeAndGetHandle();
+	}
+
+	/**
+	 * Calls {@link org.apache.flink.streaming.api.operators.StreamOperator#restoreState(org.apache.flink.core.fs.FSDataInputStream)} ()}
+	 */
+	@Override
+	public void restore(StreamStateHandle snapshot) throws Exception {
+		FSDataInputStream inStream = snapshot.openInputStream();
+		operator.restoreState(inStream);
+
+		byte keyedStatePresent = (byte) inStream.read();
+		if (keyedStatePresent == 1) {
+			ObjectInputStream ois = new ObjectInputStream(inStream);
+			this.restoredKeyedState = (KeyGroupsStateHandle) ois.readObject();
+		}
+	}
+
+	/**
+	 * Calls close and dispose on the operator.
+	 */
+	public void close() throws Exception {
+		super.close();
+	}
+}
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/MockContext.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/MockContext.java
index bc255ff..65ed43d 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/MockContext.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/MockContext.java
@@ -78,9 +78,7 @@ public class MockContext<IN, OUT> {
 				KeySelector<IN, KEY> keySelector, TypeInformation<KEY> keyType) throws Exception {
 
 		OneInputStreamOperatorTestHarness<IN, OUT> testHarness =
-				new OneInputStreamOperatorTestHarness<>(operator);
-
-		testHarness.configureForKeyedStream(keySelector, keyType);
+				new KeyedOneInputStreamOperatorTestHarness<>(operator, keySelector, keyType);
 
 		testHarness.setup();
 		testHarness.open();
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/OneInputStreamOperatorTestHarness.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/OneInputStreamOperatorTestHarness.java
index 6cb46d6..78e05b7 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/OneInputStreamOperatorTestHarness.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/OneInputStreamOperatorTestHarness.java
@@ -18,22 +18,25 @@
 package org.apache.flink.streaming.util;
 
 import org.apache.flink.api.common.ExecutionConfig;
-import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.api.common.state.KeyGroupAssigner;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.api.java.ClosureCleaner;
-import org.apache.flink.api.java.functions.KeySelector;
 import org.apache.flink.api.java.typeutils.TypeExtractor;
 import org.apache.flink.configuration.Configuration;
+import org.apache.flink.core.fs.FSDataOutputStream;
 import org.apache.flink.runtime.execution.Environment;
 import org.apache.flink.runtime.operators.testutils.MockEnvironment;
 import org.apache.flink.runtime.operators.testutils.MockInputSplitProvider;
 import org.apache.flink.runtime.state.AbstractStateBackend;
-import org.apache.flink.runtime.state.HashKeyGroupAssigner;
+import org.apache.flink.runtime.state.CheckpointStreamFactory;
+import org.apache.flink.runtime.state.KeyGroupRange;
+import org.apache.flink.runtime.state.KeyedStateBackend;
 import org.apache.flink.runtime.state.StreamStateHandle;
 import org.apache.flink.runtime.state.memory.MemoryStateBackend;
 import org.apache.flink.streaming.api.graph.StreamConfig;
 import org.apache.flink.streaming.api.operators.OneInputStreamOperator;
 import org.apache.flink.streaming.api.operators.Output;
+import org.apache.flink.streaming.api.operators.StreamOperator;
 import org.apache.flink.streaming.api.watermark.Watermark;
 import org.apache.flink.streaming.runtime.operators.Triggerable;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
@@ -44,6 +47,7 @@ import org.mockito.invocation.InvocationOnMock;
 import org.mockito.stubbing.Answer;
 
 import java.util.Collection;
+import java.util.Collections;
 import java.util.concurrent.ConcurrentLinkedQueue;
 import java.util.concurrent.Executors;
 
@@ -63,6 +67,8 @@ import static org.mockito.Mockito.when;
  */
 public class OneInputStreamOperatorTestHarness<IN, OUT> {
 
+	protected static final int MAX_PARALLELISM = 10;
+
 	final OneInputStreamOperator<IN, OUT> operator;
 
 	final ConcurrentLinkedQueue<Object> outputList;
@@ -78,7 +84,7 @@ public class OneInputStreamOperatorTestHarness<IN, OUT> {
 	StreamTask<?, ?> mockTask;
 
 	// use this as default for tests
-	private AbstractStateBackend stateBackend = new MemoryStateBackend();
+	AbstractStateBackend stateBackend = new MemoryStateBackend();
 
 	/**
 	 * Whether setup() was called on the operator. This is reset when calling close().
@@ -108,7 +114,7 @@ public class OneInputStreamOperatorTestHarness<IN, OUT> {
 		this.executionConfig = executionConfig;
 		this.checkpointLock = new Object();
 
-		final Environment env = new MockEnvironment("MockTwoInputTask", 3 * 1024 * 1024, new MockInputSplitProvider(), 1024, underlyingConfig);
+		final Environment env = new MockEnvironment("MockTwoInputTask", 3 * 1024 * 1024, new MockInputSplitProvider(), 1024, underlyingConfig, executionConfig, MAX_PARALLELISM, 1, 0);
 		mockTask = mock(StreamTask.class);
 		timeServiceProvider = testTimeProvider;
 
@@ -120,21 +126,6 @@ public class OneInputStreamOperatorTestHarness<IN, OUT> {
 		when(mockTask.getExecutionConfig()).thenReturn(executionConfig);
 		when(mockTask.getUserCodeClassLoader()).thenReturn(this.getClass().getClassLoader());
 
-		try {
-			doAnswer(new Answer<AbstractStateBackend>() {
-				@Override
-				public AbstractStateBackend answer(InvocationOnMock invocationOnMock) throws Throwable {
-					final String operatorIdentifier = (String) invocationOnMock.getArguments()[0];
-					final TypeSerializer<?> keySerializer = (TypeSerializer<?>) invocationOnMock.getArguments()[1];
-					OneInputStreamOperatorTestHarness.this.stateBackend.disposeAllStateForCurrentJob();
-					OneInputStreamOperatorTestHarness.this.stateBackend.initializeForJob(env, operatorIdentifier, keySerializer);
-					return OneInputStreamOperatorTestHarness.this.stateBackend;
-				}
-			}).when(mockTask).createStateBackend(any(String.class), any(TypeSerializer.class));
-		} catch (Exception e) {
-			throw new RuntimeException(e.getMessage(), e);
-		}
-		
 		doAnswer(new Answer<Void>() {
 			@Override
 			public Void answer(InvocationOnMock invocation) throws Throwable {
@@ -153,6 +144,20 @@ public class OneInputStreamOperatorTestHarness<IN, OUT> {
 				return timeServiceProvider.getCurrentProcessingTime();
 			}
 		}).when(mockTask).getCurrentProcessingTime();
+
+		try {
+			doAnswer(new Answer<CheckpointStreamFactory>() {
+				@Override
+				public CheckpointStreamFactory answer(InvocationOnMock invocationOnMock) throws Throwable {
+
+					final StreamOperator operator = (StreamOperator) invocationOnMock.getArguments()[0];
+					return stateBackend.createStreamFactory(new JobID(), operator.getClass().getSimpleName());
+				}
+			}).when(mockTask).createCheckpointStreamFactory(any(StreamOperator.class));
+		} catch (Exception e) {
+			throw new RuntimeException(e.getMessage(), e);
+		}
+
 	}
 
 	public void setStateBackend(AbstractStateBackend stateBackend) {
@@ -167,13 +172,6 @@ public class OneInputStreamOperatorTestHarness<IN, OUT> {
 		return this.mockTask.getEnvironment();
 	}
 
-	public <K> void configureForKeyedStream(KeySelector<IN, K> keySelector, TypeInformation<K> keyType) {
-		ClosureCleaner.clean(keySelector, false);
-		config.setStatePartitioner(0, keySelector);
-		config.setStateKeySerializer(keyType.createSerializer(executionConfig));
-		config.setKeyGroupAssigner(new HashKeyGroupAssigner<K>(10));
-	}
-
 	/**
 	 * Get all the output from the task. This contains StreamRecords and Events interleaved. Use
 	 * {@link org.apache.flink.streaming.util.TestHarnessUtil#getStreamRecordsFromOutput(java.util.List)}
@@ -205,13 +203,12 @@ public class OneInputStreamOperatorTestHarness<IN, OUT> {
 	}
 
 	/**
-	 * Calls {@link org.apache.flink.streaming.api.operators.StreamOperator#snapshotState(org.apache.flink.core.fs.FSDataOutputStream, long, long)} ()}
+	 * Calls {@link org.apache.flink.streaming.api.operators.StreamOperator#snapshotState(FSDataOutputStream, long, long)} ()}
 	 */
 	public StreamStateHandle snapshot(long checkpointId, long timestamp) throws Exception {
-		// simply use an in-memory handle
-		MemoryStateBackend backend = new MemoryStateBackend();
-		AbstractStateBackend.CheckpointStateOutputStream outStream =
-				backend.createCheckpointStateOutputStream(checkpointId, timestamp);
+		CheckpointStreamFactory.CheckpointStateOutputStream outStream = stateBackend.createStreamFactory(
+				new JobID(),
+				"test_op").createCheckpointStateOutputStream(checkpointId, timestamp);
 		operator.snapshotState(outStream, checkpointId, timestamp);
 		return outStream.closeAndGetHandle();
 	}
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/TestHarnessUtil.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/TestHarnessUtil.java
index bc4074f..58e8c6b 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/TestHarnessUtil.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/TestHarnessUtil.java
@@ -17,6 +17,7 @@
  */
 package org.apache.flink.streaming.util;
 
+import com.google.common.collect.Iterables;
 import org.apache.flink.streaming.api.watermark.Watermark;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
 import org.junit.Assert;
@@ -76,8 +77,8 @@ public class TestHarnessUtil {
 	/**
 	 * Compare the two queues containing operator/task output by converting them to an array first.
 	 */
-	public static void assertOutputEqualsSorted(String message, Queue<Object> expected, Queue<Object> actual, Comparator<Object> comparator) {
-		assertEquals(expected.size(), actual.size());
+	public static void assertOutputEqualsSorted(String message, Iterable<Object> expected, Iterable<Object> actual, Comparator<Object> comparator) {
+		assertEquals(Iterables.size(expected), Iterables.size(actual));
 
 		// first, compare only watermarks, their position should be deterministic
 		Iterator<Object> exIt = expected.iterator();
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/WindowingTestHarness.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/WindowingTestHarness.java
index 779436a..af1f3fa 100644
--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/WindowingTestHarness.java
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/WindowingTestHarness.java
@@ -85,8 +85,7 @@ public class WindowingTestHarness<K, IN, W extends Window> {
 		operator.setInputType(inputType, executionConfig);
 
 		timeServiceProvider = new TestTimeServiceProvider();
-		testHarness = new OneInputStreamOperatorTestHarness<>(operator, executionConfig, timeServiceProvider);
-		testHarness.configureForKeyedStream(keySelector, keyType);
+		testHarness = new KeyedOneInputStreamOperatorTestHarness<>(operator, executionConfig, timeServiceProvider, keySelector, keyType);
 	}
 
 	/**
diff --git a/flink-tests/src/test/java/org/apache/flink/test/checkpointing/EventTimeWindowCheckpointingITCase.java b/flink-tests/src/test/java/org/apache/flink/test/checkpointing/EventTimeWindowCheckpointingITCase.java
index 97c8339..9f8ab90 100644
--- a/flink-tests/src/test/java/org/apache/flink/test/checkpointing/EventTimeWindowCheckpointingITCase.java
+++ b/flink-tests/src/test/java/org/apache/flink/test/checkpointing/EventTimeWindowCheckpointingITCase.java
@@ -73,7 +73,6 @@ import static org.junit.Assert.*;
 @RunWith(Parameterized.class)
 public class EventTimeWindowCheckpointingITCase extends TestLogger {
 
-
 	private static final int MAX_MEM_STATE_SIZE = 10 * 1024 * 1024;
 	private static final int PARALLELISM = 4;
 
@@ -118,20 +117,11 @@ public class EventTimeWindowCheckpointingITCase extends TestLogger {
 				this.stateBackend = new FsStateBackend("file://" + backups);
 				break;
 			}
-			case ROCKSDB: {
-				String rocksDb = tempFolder.newFolder().getAbsolutePath();
-				String rocksDbBackups = tempFolder.newFolder().toURI().toString();
-				RocksDBStateBackend rdb = new RocksDBStateBackend(rocksDbBackups, new MemoryStateBackend(MAX_MEM_STATE_SIZE));
-				rdb.setDbStoragePath(rocksDb);
-				this.stateBackend = rdb;
-				break;
-			}
 			case ROCKSDB_FULLY_ASYNC: {
 				String rocksDb = tempFolder.newFolder().getAbsolutePath();
 				String rocksDbBackups = tempFolder.newFolder().toURI().toString();
 				RocksDBStateBackend rdb = new RocksDBStateBackend(rocksDbBackups, new MemoryStateBackend(MAX_MEM_STATE_SIZE));
 				rdb.setDbStoragePath(rocksDb);
-				rdb.enableFullyAsyncSnapshots();
 				this.stateBackend = rdb;
 				break;
 			}
@@ -774,14 +764,13 @@ public class EventTimeWindowCheckpointingITCase extends TestLogger {
 		return Arrays.asList(new Object[][] {
 				{StateBackendEnum.MEM},
 				{StateBackendEnum.FILE},
-				{StateBackendEnum.ROCKSDB},
 				{StateBackendEnum.ROCKSDB_FULLY_ASYNC}
 			}
 		);
 	}
 
 	private enum StateBackendEnum {
-		MEM, FILE, ROCKSDB, ROCKSDB_FULLY_ASYNC
+		MEM, FILE, ROCKSDB_FULLY_ASYNC
 	}
 
 
diff --git a/flink-tests/src/test/java/org/apache/flink/test/classloading/ClassLoaderITCase.java b/flink-tests/src/test/java/org/apache/flink/test/classloading/ClassLoaderITCase.java
index 3bc3cf5..8b56d3d 100644
--- a/flink-tests/src/test/java/org/apache/flink/test/classloading/ClassLoaderITCase.java
+++ b/flink-tests/src/test/java/org/apache/flink/test/classloading/ClassLoaderITCase.java
@@ -310,7 +310,7 @@ public class ClassLoaderITCase extends TestLogger {
 			// Success :-)
 			LOG.info("Disposed savepoint at " + savepointPath);
 		} else if (disposeResponse instanceof DisposeSavepointFailure) {
-			throw new IllegalStateException("Failed to dispose savepoint");
+			throw new IllegalStateException("Failed to dispose savepoint " + disposeResponse);
 		} else {
 			throw new IllegalStateException("Unexpected response to DisposeSavepoint");
 		}
diff --git a/flink-tests/src/test/java/org/apache/flink/test/classloading/jar/CustomKvStateProgram.java b/flink-tests/src/test/java/org/apache/flink/test/classloading/jar/CustomKvStateProgram.java
index c6a4c7f..8de4797 100644
--- a/flink-tests/src/test/java/org/apache/flink/test/classloading/jar/CustomKvStateProgram.java
+++ b/flink-tests/src/test/java/org/apache/flink/test/classloading/jar/CustomKvStateProgram.java
@@ -18,11 +18,13 @@
 
 package org.apache.flink.test.classloading.jar;
 
+import org.apache.flink.api.common.functions.MapFunction;
 import org.apache.flink.api.common.functions.ReduceFunction;
 import org.apache.flink.api.common.functions.RichFlatMapFunction;
 import org.apache.flink.api.common.state.ReducingState;
 import org.apache.flink.api.common.state.ReducingStateDescriptor;
 import org.apache.flink.api.java.functions.KeySelector;
+import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.runtime.state.filesystem.FsStateBackend;
 import org.apache.flink.streaming.api.datastream.DataStream;
@@ -56,14 +58,23 @@ public class CustomKvStateProgram {
 		env.setStateBackend(new FsStateBackend(checkpointPath));
 
 		DataStream<Integer> source = env.addSource(new InfiniteIntegerSource());
-		source.keyBy(new KeySelector<Integer, Integer>() {
-			private static final long serialVersionUID = -9044152404048903826L;
-
-			@Override
-			public Integer getKey(Integer value) throws Exception {
-				return ThreadLocalRandom.current().nextInt(parallelism);
-			}
-		}).flatMap(new ReducingStateFlatMap()).writeAsText(outputPath);
+		source
+				.map(new MapFunction<Integer, Tuple2<Integer, Integer>>() {
+					private static final long serialVersionUID = 1L;
+
+					@Override
+					public Tuple2<Integer, Integer> map(Integer value) throws Exception {
+						return new Tuple2<>(ThreadLocalRandom.current().nextInt(parallelism), value);
+					}
+				})
+				.keyBy(new KeySelector<Tuple2<Integer,Integer>, Integer>() {
+					private static final long serialVersionUID = 1L;
+
+					@Override
+					public Integer getKey(Tuple2<Integer, Integer> value) throws Exception {
+						return value.f0;
+					}
+				}).flatMap(new ReducingStateFlatMap()).writeAsText(outputPath);
 
 		env.execute();
 	}
@@ -88,10 +99,10 @@ public class CustomKvStateProgram {
 		}
 	}
 
-	private static class ReducingStateFlatMap extends RichFlatMapFunction<Integer, Integer> {
+	private static class ReducingStateFlatMap extends RichFlatMapFunction<Tuple2<Integer, Integer>, Integer> {
 
 		private static final long serialVersionUID = -5939722892793950253L;
-		private ReducingState<Integer> kvState;
+		private transient ReducingState<Integer> kvState;
 
 		@Override
 		public void open(Configuration parameters) throws Exception {
@@ -106,11 +117,13 @@ public class CustomKvStateProgram {
 
 
 		@Override
-		public void flatMap(Integer value, Collector<Integer> out) throws Exception {
-			kvState.add(value);
+		public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {
+			kvState.add(value.f1);
 		}
 
 		private static class ReduceSum implements ReduceFunction<Integer> {
+			private static final long serialVersionUID = 1L;
+
 			@Override
 			public Integer reduce(Integer value1, Integer value2) throws Exception {
 				return value1 + value2;
diff --git a/flink-tests/src/test/java/org/apache/flink/test/state/StateHandleSerializationTest.java b/flink-tests/src/test/java/org/apache/flink/test/state/StateHandleSerializationTest.java
index b3e2137..77a9b2e 100644
--- a/flink-tests/src/test/java/org/apache/flink/test/state/StateHandleSerializationTest.java
+++ b/flink-tests/src/test/java/org/apache/flink/test/state/StateHandleSerializationTest.java
@@ -18,8 +18,6 @@
 
 package org.apache.flink.test.state;
 
-import org.apache.flink.runtime.state.KvStateSnapshot;
-
 import org.apache.flink.runtime.state.StateObject;
 import org.junit.Test;
 
@@ -51,18 +49,6 @@ public class StateHandleSerializationTest {
 			for (Class<?> clazz : stateHandleImplementations) {
 				validataSerialVersionUID(clazz);
 			}
-
-			// check all key/value snapshots
-
-			@SuppressWarnings("unchecked")
-			Set<Class<?>> kvStateSnapshotImplementations = (Set<Class<?>>) (Set<?>)
-					reflections.getSubTypesOf(KvStateSnapshot.class);
-
-			System.out.println(kvStateSnapshotImplementations);
-			
-			for (Class<?> clazz : kvStateSnapshotImplementations) {
-				validataSerialVersionUID(clazz);
-			}
 		}
 		catch (Exception e) {
 			e.printStackTrace();
diff --git a/flink-tests/src/test/java/org/apache/flink/test/streaming/runtime/StateBackendITCase.java b/flink-tests/src/test/java/org/apache/flink/test/streaming/runtime/StateBackendITCase.java
index 41455cf..d4dd475 100644
--- a/flink-tests/src/test/java/org/apache/flink/test/streaming/runtime/StateBackendITCase.java
+++ b/flink-tests/src/test/java/org/apache/flink/test/streaming/runtime/StateBackendITCase.java
@@ -18,26 +18,28 @@
 
 package org.apache.flink.test.streaming.runtime;
 
+import org.apache.flink.api.common.JobID;
 import org.apache.flink.api.common.functions.RichMapFunction;
 import org.apache.flink.api.common.restartstrategy.RestartStrategies;
-import org.apache.flink.api.common.state.FoldingState;
-import org.apache.flink.api.common.state.FoldingStateDescriptor;
-import org.apache.flink.api.common.state.ListState;
-import org.apache.flink.api.common.state.ListStateDescriptor;
-import org.apache.flink.api.common.state.ReducingState;
-import org.apache.flink.api.common.state.ReducingStateDescriptor;
-import org.apache.flink.api.common.state.ValueState;
-import org.apache.flink.api.common.state.ValueStateDescriptor;
+import org.apache.flink.api.common.state.KeyGroupAssigner;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.runtime.client.JobExecutionException;
 import org.apache.flink.runtime.execution.Environment;
+import org.apache.flink.runtime.query.TaskKvStateRegistry;
 import org.apache.flink.runtime.state.AbstractStateBackend;
+import org.apache.flink.runtime.state.CheckpointStreamFactory;
+import org.apache.flink.runtime.state.KeyGroupRange;
+import org.apache.flink.runtime.state.KeyGroupsStateHandle;
+import org.apache.flink.runtime.state.KeyedStateBackend;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.streaming.util.StreamingMultipleProgramsTestBase;
 import org.junit.Test;
 
+import java.io.IOException;
+import java.util.List;
+
 import static org.junit.Assert.fail;
 
 public class StateBackendITCase extends StreamingMultipleProgramsTestBase {
@@ -74,7 +76,7 @@ public class StateBackendITCase extends StreamingMultipleProgramsTestBase {
 				}
 			})
 			.print();
-		
+
 		try {
 			see.execute();
 			fail();
@@ -89,49 +91,39 @@ public class StateBackendITCase extends StreamingMultipleProgramsTestBase {
 
 
 	public static class FailingStateBackend extends AbstractStateBackend {
-		
 		private static final long serialVersionUID = 1L;
 
 		@Override
-		public void initializeForJob(Environment env, String operatorIdentifier, TypeSerializer<?> keySerializer) throws Exception {
+		public CheckpointStreamFactory createStreamFactory(JobID jobId,
+				String operatorIdentifier) throws IOException {
 			throw new SuccessException();
 		}
 
 		@Override
-		public void disposeAllStateForCurrentJob() throws Exception {}
-
-		@Override
-		public void close() throws Exception {}
-
-		@Override
-		protected <N, T> ValueState<T> createValueState(TypeSerializer<N> namespaceSerializer, ValueStateDescriptor<T> stateDesc) throws Exception {
-			return null;
-		}
-
-		@Override
-		protected <N, T> ListState<T> createListState(TypeSerializer<N> namespaceSerializer, ListStateDescriptor<T> stateDesc) throws Exception {
-			return null;
-		}
-
-		@Override
-		protected <N, T> ReducingState<T> createReducingState(TypeSerializer<N> namespaceSerializer, ReducingStateDescriptor<T> stateDesc) throws Exception {
-			return null;
-		}
-
-		@Override
-		protected <N, T, ACC> FoldingState<T, ACC> createFoldingState(TypeSerializer<N> namespaceSerializer,
-			FoldingStateDescriptor<T, ACC> stateDesc) throws Exception {
-			return null;
+		public <K> KeyedStateBackend<K> createKeyedStateBackend(Environment env,
+				JobID jobID,
+				String operatorIdentifier,
+				TypeSerializer<K> keySerializer,
+				KeyGroupAssigner<K> keyGroupAssigner,
+				KeyGroupRange keyGroupRange,
+				TaskKvStateRegistry kvStateRegistry) throws Exception {
+			throw new SuccessException();
 		}
 
 		@Override
-		public CheckpointStateOutputStream createCheckpointStateOutputStream(long checkpointID,
-			long timestamp) throws Exception {
-			return null;
+		public <K> KeyedStateBackend<K> restoreKeyedStateBackend(Environment env,
+				JobID jobID,
+				String operatorIdentifier,
+				TypeSerializer<K> keySerializer,
+				KeyGroupAssigner<K> keyGroupAssigner,
+				KeyGroupRange keyGroupRange,
+				List<KeyGroupsStateHandle> restoredState,
+				TaskKvStateRegistry kvStateRegistry) throws Exception {
+			throw new SuccessException();
 		}
 	}
 
-	static final class SuccessException extends Exception {
+	static final class SuccessException extends IOException {
 		private static final long serialVersionUID = -9218191172606739598L;
 	}
 
